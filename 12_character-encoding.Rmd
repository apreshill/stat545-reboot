# Character encoding {#character-encoding}

```{r include = FALSE}
source("common.R")
```

<!--TODO: *under development ... not clear where this is going*-->

<!--Original content: https://stat545.com/block032_character-encoding.html-->

## Resources {#encoding-resources}

* [Strings subsection of data import chapter][r4ds-readr-strings] in R for Data Science [@wickham2016].
* Screeds on the Minimum Everyone Needs to Know about encoding:
  - [The Absolute Minimum Every Software Developer Absolutely, Positively Must Know About Unicode and Character Sets (No Excuses!)][unicode-no-excuses]
  - [What Every Programmer Absolutely, Positively Needs To Know About Encodings And Character Sets To Work With Text][programmers-encoding]
* Debugging charts:
  - [Windows-1252 Characters to UTF-8 Bytes to Latin-1 Characters][utf8-debug]
* Character inspection:
  - <https://apps.timwhitlock.info/unicode/inspect>

## Translating two blog posts from Ruby to R

For now, this page walks through these two mini-tutorials (written for Ruby), but translated to R:

* [Guide to fixing encoding problems in Ruby][encoding-probs-ruby]
* [How to Get From Theyâ€™re to They’re][theyre-to-theyre]
  
Don't expect much creativity from me here. My goal is faithful translation.

## What is an encoding?

Look at the string `"hello!"` in bytes. *Ruby*
 
```ruby
irb(main):001:0> "hello!".bytes
=> [104, 101, 108, 108, 111, 33]
```
 
The base function `charToRaw()` "converts a length-one character string to raw bytes. It does so without taking into account any declared encoding". It displays bytes in hexadecimal. Use `as.integer()` to convert to decimal, which is more intuitive and allows us to compare against the Ruby results.

```{r start_char_enc}
charToRaw("hello!")
as.integer(charToRaw("hello!"))
```

Use a character less common in English: *Ruby*

```
irb(main):002:0> "hellṏ!".bytes
=> [104, 101, 108, 108, 225, 185, 143, 33]
```

```{r}
charToRaw("hellṏ!")
as.integer(charToRaw("hellṏ!"))
```

Now we see that it takes more than one byte to represent `"ṏ"`. Three in fact: `r paste0("[", paste(as.integer(charToRaw("ṏ")), collapse = ", "), "]")`. The encoding of a string defines this relationship: encoding is a map between one or more bytes and a displayable character.

Take a look at what a single set of bytes looks like when you try different encodings.

Here's, a string encoded as ISO-8859-1 (also known as "Latin1") with a special character. *Ruby*

```
irb(main):003:0> str = "hellÔ!".encode("ISO-8859-1"); str.encode("UTF-8")
=> "hellÔ!"

irb(main):004:0> str.bytes
=> [104, 101, 108, 108, 212, 33]
```

```{r}
string_latin <- iconv("hellÔ!", from = "UTF-8", to = "Latin1")
string_latin
charToRaw(string_latin)
as.integer(charToRaw(string_latin))
```

We've confirmed that we have the correct bytes (meaning the same as the Ruby example). What would that string look like interpreted as ISO-8859-5 instead? *Ruby*

```
irb(main):005:0> str.force_encoding("ISO-8859-5"); str.encode("UTF-8")
=> "hellд!"
```

```{r}
iconv(string_latin, from = "ISO-8859-5", to = "UTF-8")
```

It's garbled, which is your first tip-off to an encoding problem.

Also not all strings can be represented in all encodings: *Ruby*

```
irb(main):006:0> "hi∑".encode("Windows-1252")
Encoding::UndefinedConversionError: U+2211 to WINDOWS-1252 in conversion from UTF-8 to WINDOWS-1252
 from (irb):61:in `encode'
 from (irb):61
 from /usr/local/bin/irb:11:in `<main>'
```

```{r}
(string <- "hi∑")
Encoding(string)
as.integer(charToRaw(string))
(string_windows <- iconv(string, from = "UTF-8", to = "Windows-1252"))
```

In Ruby, apparently that is an error. In R, we just get `NA`. Alternatively, and somewhat like Ruby, you can specify a substitution for non-convertible bytes.

```{r}
(string_windows <- iconv(string, from = "UTF-8", to = "Windows-1252", sub = "?"))
```

In the Ruby post, we've seen 3 string functions so far. Review and note which R function was used in the translation.

* `encode` translates a string to another encoding. We've used `iconv(x, from = "UTF-8", to = <DIFFERENT_ENCODING>)` here.
* `bytes` shows the bytes that make up a string. We've used `charToRaw()`, which returns hexadecimal in R. For the sake of comparison to the Ruby post, I've converted to decimal with `as.integer()`.
* `force_encoding` shows what the input bytes would look like if interpreted by a different encoding. We've used `iconv(x, from = <DIFFERENT_ENCODING>, to = "UTF-8")`.

## A three-step process for fixing encoding bugs

### Discover which encoding your string is actually in.

Shhh. Secret: this is encoded as Windows-1252. `\x99` should be the trademark symbol ™. Ruby can guess at the encoding. *Ruby*

```ruby
irb(main):078:0> "hi\x99!".encoding
=> #<Encoding:UTF-8>
```

Ruby's guess is bad. This is not encoded as UTF-8. R admits it doesn't know and `stringi`'s guess is not good.

```{r}
string <- "hi\x99!"
string
Encoding(string)
stringi::stri_enc_detect(string)
```

Advice given in post is to sleuth it out based on where the data came from. With larger amounts of text, each language's guessing facilities presumably do better than they do here. In real life, all of this advice can prove to be ... overly optimistic?

I find it helpful to scrutinize debugging charts and look for the weird stuff showing up in my text. Here's [one that shows what UTF-8 bytes look like][utf8-debug] when erroneously interpreted under Windows-1252 encoding. This phenomenon is known as [*mojibake*][wiki-mojibake], which is a delightful word for a super-annoying phenomenon. If it helps, know that the most common encodings are UTF-8, ISO-8859-1 (or Latin1), and Windows-1252, so that really narrows things down.

### Decide which encoding you want the string to be

That's easy. UTF-8. Done.

### Re-encode your string

```
irb(main):088:0> "hi\x99!".encode("UTF-8", "Windows-1252")
=> "hi™!"
```

```{r}
string_windows <- "hi\x99!"
string_utf8 <- iconv(string_windows, from = "Windows-1252", to = "UTF-8")
Encoding(string_utf8)
string_utf8
```

## How to Get From Theyâ€™re to They’re

Moving on to the second blog post now.

### Multi-byte characters

Since we need to represent more than 256 characters, not all can be represented by a single byte. Let's look at the curly single quote. *Ruby*

```
irb(main):001:0> "they’re".bytes
=> [116, 104, 101, 121, 226, 128, 153, 114, 101]
```

```{r}
string_curly <- "they’re"
charToRaw(string_curly)
as.integer(charToRaw(string_curly))
length(as.integer(charToRaw(string_curly)))
nchar(string_curly)
```

The string has 7 characters, but 9 bytes, because we're using 3 bytes to represent the curly single quote. Let's focus just on that. *Ruby*

```
irb(main):002:0> "’".bytes
=> [226, 128, 153]
```

```{r}
charToRaw("’")
as.integer(charToRaw("’"))
length(as.integer(charToRaw("’")))
```

One of the most common encoding fiascos you'll see is this: theyâ€™re. Note that the curly single quote has been turned into a 3 character monstrosity. This is no coincidence. Remember those 3 bytes?

This is what happens when you interpret bytes that represent text in the UTF-8 encoding as if it's encoded as Windows-1252. Learn to recognize it. *Ruby*

```
irb(main):003:0> "they’re".force_encoding("Windows-1252").encode("UTF-8")
=> "theyâ€™re"
```

```{r}
(string_mis_encoded <- iconv(string_curly, to = "UTF-8", from = "windows-1252"))
```

Let's assume this little gem is buried in some large file and you don't immediately notice. So this string is interpreted with the wrong encoding, i.e. stored as the wrong bytes, either in an R object or in a file on disk. Now what?

Let's review the original, correct bytes vs. the current, incorrect bytes and print the associated strings.

```{r}
as.integer(charToRaw(string_curly))
as.integer(charToRaw(string_mis_encoded))
string_curly
string_mis_encoded
```

### Encoding repair

How do you fix this? You have to reverse your steps. You have a UTF-8 encoded string. Convert it back to Windows-1252, to get the original bytes. Then re-encode that as UTF-8. *Ruby*

```
irb(main):006:0> "theyâ€™re".encode("Windows-1252").force_encoding("UTF-8")
=> "they’re"
```

```{r end_char_enc}
string_mis_encoded
backwards_one <- iconv(string_mis_encoded, from = "UTF-8", to = "Windows-1252")
backwards_one
Encoding(backwards_one)
as.integer(charToRaw(backwards_one))
as.integer(charToRaw(string_curly))
```


## Case study: useR! 2019 {#encoding-case-study}

<!-- Original content: https://stat545.com/block034_useR-encoding-case-study.html-->

### The useR! 2019 Speaker List

The last time I taught character encoding I vowed to develop a small case study the next time I encountered a suitable example in the wild. And, lo, the list of talks accepted for [useR! 2019](https://user2019.r-project.org/program_overview/) provides a great example of the real world encoding problems faced routinely by data analysts.

Let me be clear: I direct no criticism towards the organizers of useR! 2019. It's very easy for these sorts of problems to creep in whenever multiple humans fling strings around the internet, using diverse operating systems, locales, and software. It was very thoughtful of them to share this information publicly before the full program is published. And I thank them for a great expository example.

Here's how I downloaded the original csv file, which I store inside this repo to make this case study more future-proof:

```{r eval = FALSE}
curl::curl_download(
  "http://www.user2019.fr/static/uploads/accepted-submissions_2019-04-16.csv",
  destfile = "useR-2019-accepted-talks.csv"
)
```

### What is the encoding of the file?

Analysts are always admonished to specify the correct encoding whenever they import data.

**Reality Check #1**: How the heck are you supposed to know what the encoding is? In the real world, data providers rarely provide this precious information.

Mercifully, the useR! organizers are on the ball and actually do describe the file:

> The list of accepted talks can be downloaded here (CSV file, separated by semi-colon, ISO-8859-15 encoded).
This is fantastic. And incredibly rare. So how would you determine this otherwise?

If you're on some flavor of *nix, you can probably use the `file` command to get some info (I executed this in a bash shell on macOS):

```{bash eval = FALSE}
file -I useR-2019-accepted-talks.csv
#> useR-2019-accepted-talks.csv: text/plain; charset=iso-8859-1
```

The putative encoding returned by `file -I` is ISO-8859-1, which is wrong but close and probably close enough. In these situations, you are thankful for any shred of information.

There are many other ways to do this sort of detective work. For example, within R, you might use the `stri_enc_detect()` function from the [stringi package](http://www.gagolewski.com/software/stringi/):

```{r}
x <- rawToChar(readBin("useR-2019-accepted-talks.csv", "raw", 100000))
stringi::stri_enc_detect(x)
```

The readr package exposes `stringi::stri_enc_detect()` in a more user-friendly form that takes a file path.

```{r}
readr::guess_encoding("useR-2019-accepted-talks.csv")
```

Again, we get the implication that this file is encoded as ISO-8859-1, which is a good guess, but wrong. Again, this is a realistic tutorial.

Let's now assume you know the encoding. Or, well ... you think you do.

### Import the data

Use your favorite method of importing a delimited file. I use `readr::read_csv2()`, where the "2" signals that the semicolon `;` is the field delimiter. We also specify the ISO-8859-15 encoding we were advised to use. I load the tidyverse meta-package, because this exposition makes use of readr, dplyr, purrr, and the pipe `%>%` operator.

```{r message = FALSE}
library(tidyverse)
```

```{r}
user <- read_csv2(
  "useR-2019-accepted-talks.csv",
  locale = locale(encoding = "ISO-8859-15")
)
```

### Tricky rows

Having inspected this file closely, allow me to draw your attention to a few specific names (format is "Lastname Firstname"):

```{r include = FALSE, eval = FALSE}
which(Encoding(user$CREATEUSERID) != "unknown")
```

```{r}
user[c(34, 43, 61, 107, 212, 336), "CREATEUSERID"]
```

We've got two examples where we've clearly had some sort of encoding mishap (entries 3 and 4), mixed in with other strings with non-ASCII characters that look just fine. Hmm, that's peculiar.

#### How did I know to focus on these names?

Usually, you first discover encoding problems downstream, when you stumble across a garbled string. I first imported this file without specifying an encoding and saw problems. Then I remembered that the organizers specified an encoding, so I imported again with the correct encoding. And I *still* saw problems, just different ones. Then I buckled up to write this case study.

Once I knew I had problems, I went from my anecdata to a more comprehensive search for names likely to be affected. I found these non-ASCII strings by looking for the elements of `user$CREATEUSERID` where the declared encoding, reported by `Encoding()`, was not "unknown". The exact behaviour of the encoding marks returned by `Encoding()` is very complicated and beyond our scope. I suspect that the functions `stringi::stri_enc_mark()` and `stringi::stri_enc_isascii()` would provide a less frustrating foundation for a more formal workflow.

### Try another encoding

Again, with my head start, allow me to show you something else. What if I import this file with UTF-8 encoding?

```{r}
please_work <- read_csv2(
  "useR-2019-accepted-talks.csv",
  locale = locale(encoding = "UTF-8")
)
please_work[c(34, 43, 61, 107, 212, 336), "CREATEUSERID"]
```

Sad. We've correctly imported the problematic names, at the cost of garbling the other four.

**Reality Check #2**: In an ideal world there is One True Encoding for any given file. Yes, that is how it is supposed to be. And yet it is not how it is.

### Unraveling a mixed encoding

So, what happened to this file? Its declared encoding is ISO-8859-15 but it's got some strings that need to be ingested as UTF-8.

The good news is you get to learn a delightful word for an unsavory phenomenon: **mojibake**:

> Mojibake is the garbled text that is the result of text being decoded using an unintended character encoding. The result is a systematic replacement of symbols with completely unrelated ones, often from a different writing system. -- [Wikipedia][wiki-mojibake]

Used in a sentence:

> We have an ISO-8859-15 encoded, semicolon-delimited CSV, with a touch of mojibake.
At this point, I can't tell you how to catch this systematically. I can only say that with experience you get pretty good at knowing mojibake when you see it, forming a hypothesis about what went wrong, and fixing it. Remember the [Resources](#encoding-resources) section of this chapter links to external resources, such as debugging tables that juxtapose intended and actual characters for the most common encoding fiascos.

### Diagnosis

The fact that we've got strings that import correctly when interpreted as UTF-8 (contradicting the nominal encoding) is the critical clue.

At some point, strings made it into this database that were UTF-8 encoded, although the intended encoding is ISO-8859-15. Then, when `readr::read_csv2()` ingests these allegedly ISO-8859-15 bytes and re-encodes them as UTF-8, we get the dreaded mojibake.

*Remember: I'm assuming you understand this problem space at the level presented in the previous [Character encoding](#character-encoding) sections, i.e., you basically understand that characters are represented as bytes and different encodings are different systems for mapping between Unicode code points and 1 or more bytes.*

Let's look at the (correct) bytes that represent our target first names in UTF-8, which is the default for my OS (macOS) and what readr always returns. These are the bytes we *should* be seeing for these names.

```{r}
correct <- c("Bénédicte", "Cécile")
iconv(correct, from = "UTF-8", toRaw = TRUE)
```

This is hard to parse as a human, so I'm going to present richer output I got with the aid of a GitHub-only package, [ThinkR-open/utf8splain](https://github.com/ThinkR-open/utf8splain#readme).

```{r results = "hide"}
library(utf8splain)
runes("Bénédicte")
```

```{r echo = FALSE}
x <- runes("Bénédicte")
knitr::kable(
  x[c("id", "description", "rune", "utf8_bytes")],
  format = "markdown"
)
```

You'll notice the `utf8_bytes` reported here for "Bénédicte" match those returned by the `iconv()` call above, but the other columns help orient you to what else is going on:

* `id` basically corresponds to what we perceive as "which character?" within the string
* `description` is self-explanatory
* `rune` identifies the associated Unicode code point
* `utf8_bytes` are the literal bytes used to represent this code point in the UTF-8 encoding

Recall my claim that these specific strings were represented by UTF-8 bytes, in a file that is nominally ISO-8859-15, and were then garbled at ingest. We can reproduce the problem exactly.

```{r}
## 1. take UTF-8 encoded strings
c("Bénédicte", "Cécile") %>%
  ## 2. tell your converter to treat them as ISO-8859-15,
  ##    i.e. convert from ISO-8858-15 to UTF-8 and give you the bytes
  iconv(from = "ISO-8859-15", to = "UTF-8", toRaw = TRUE) -> my_bytes
my_bytes
## 3. convert these raw (allegedly UTF-8) bytes back to strings
map_chr(my_bytes, rawToChar)
```

And that explains the mojibake we saw after the initial import:

```{r}
user[c(61, 107), "CREATEUSERID"]
```

### Treatment

How do we fix this? For the affected strings, we:

* Assert they are UTF-8 encoded.
* Ask that these bytes be converted to ISO-8859-15.
* Insert these new byte representations into the existing (implicitly UTF-8) character vector.
  
#### Gory byte details

First, I reproduce the problem and prototype a solution with just the character "é".
This shows the gory details at the byte level and can be skipped upon first reading. Just move on the next section.

This chunk reproduces the problem:

* UTF-8 encoded strings sneak into the file at the data **s**ource
* **r**eadr dutifully interprets the bytes as ISO-8859-15 and re-encodes as UTF-8
  
```{r}
good <- "é"
# s: string encoded as UTF-8 in the data **s**source
iconv(good, from = "UTF-8", toRaw = TRUE)[[1]]
# r: UTF-8 bytes mis-interpreted as ISO-8859-15 by **r**eadr and 
#    re-encoded as UTF-8 bytes
(bytes <- iconv(good, from = "ISO-8859-15", to = "UTF-8", toRaw = TRUE)[[1]])
(bad <- rawToChar(bytes))
```

In this sketch, the problem is created as we travel down. It explains how "é" on someone's computer screen got turned into "Ã©" on mine.


```
                  é <- good
       
s                 "Latin Small Letter E with Acute"
s                 U+00E9
s      
s--+       UTF-8  C3                                     A9                ^
   r                                                                       ^
   r  ISO-8859-15 "Latin Capital Letter A with Tilde"   "Copyright Sign"   ^
   r              U+00C3                                U+00A9             ^
   r                                                                       ^
   r        UTF-8 C3 83                                 C2 A9              ^
       
                  Ã© <- bad                                                
```

The solution is therefore to invert this process, i.e. to travel upwards.

```{r}
(fixed_bytes <- iconv(
  bad, from = "UTF-8", to = "ISO-8859-15", toRaw = TRUE)[[1]]
)
(fixed <- rawToChar(fixed_bytes))
```

#### Repair the mis-encoded strings

Now we apply the repair formula to the affected names in our useR! data frame:
  
```{r}
fixme <- c(61, 107)
iconv(user$CREATEUSERID[fixme], from = "UTF-8", to = "ISO-8859-15")
user$CREATEUSERID[fixme] <- iconv(
  user$CREATEUSERID[fixme],
  from = "UTF-8", to = "ISO-8859-15"
)
```

We revisit the original 6 specimens to confirm that we've corrected the strings that needed repair, without disturbing those that did not.

```{r}
user[c(34, 43, 61, 107, 212, 336), c("CREATEUSERID", "TITLE")]
```

Success!

You will notice that this was a highly manual process and I'm afraid that is quite realistic, when something that should never happen -- mixed encoding -- actually happens. In my experience, it is typical to stumble across the problem, hope you can systematically identify the affected data, and then apply a targeted fix.

```{r links, child="links.md"}
```