"CREATEUSERID";"TITLE";"ABSTRACT";"TYPDOC"
"Kosmidis Ioannis";"trackeRapp: An integrated shiny workflow for the analysis of running, cycling and swimming data";"Recent technological advances have allowed the frequent and simultaneous measurement of multiple aspects of athletic activities, like running, cycling and swimming in high frequency, at a small cost and through a wealth of devices. The resulting data are increasingly being used by not only elite sports teams but also recreational athletes to quantify and track aspects of their activity and performance through time. In this talk, we will present the trackeRapp R package, which is the first shiny interface for the analysis and tracking of athletic activity data in R. trackeRapp builds on the extensive infrastructure provided by the trackeR R package and offers a user-friendly, web-interface that implements an integrated workflow for the analysis of sports data from GPS-enabled tracking devices through flexible and extensive interactive visualisations and data-analytic tools. trackeRapp offers functionality to import data from raw activity files of popular formats (tcx, gpx, json and db3), clean them, organised them and finally export images of processed activity data that can be used for further analysis not only within the interface but also for more advanced modelling in R. We will introduce the interface and its capabilities through a live demonstration, and we will present the challenges we faced whilst developing it and our future plans.";"oral presentation"
"Scheffer Annette";"Navigating spatial data management and analysis in Sustainable Fisheries using a combined R ? Python approach";" Geospatial data is of increasing importance in resource management, impact assessment and decision-making. However, the variety of requirements and user experience when working with spatial data - from developing data visualisations through graphical interfaces to undertaking analyses with customized coding - necessitates tailored solutions to maximize accessibility of data and analysis tools for different user groups. The Marine Stewardship Council (MSC) is a non-profit organization providing an internationally recognized sustainable seafood ecolabel and fishery and traceability certification programs. The nature of such an organization means that multiple departments have different requirements and coding proficiency, requiring that the MSC's spatial data management and analysis strategy accommodates a variety of access and analysis tools. Here, we discuss the implementation of spatial data storage, organisation and analysis at the MSC in light of maximizing accessibility, workflow efficiency and reproducibility of results. Our implementation combines open-source analysis tools such as R, Python and qGIS together with PostgreSQL for spatial data management. R and Python scripts combined under the rPython package allow connecting to the spatial database via R or qGIS and automating repeated processes such as specific queries and analyses.";"lightning talk"
"Arabzadeh Rezgar";"Application of WRSS in Water and Energy Analysis; An object oriented R package for large-scale water resources operation";"Water Resources Simulator, an object-oriented open-source software package based on R language, is developed for simulation of water resources systems based on Standard Operation Policy. In spite of numerous commercially available software packages for modeling and simulation of water resources systems, only a limited number of free and open source tools are available in this regard. This acted as the initiative for the development of WRSS which enables water resources engineers to study and assess water resources projects by modeling and simulation. This package provides users a number of functions and methods to build a model, manipulate its components, simulate the scenarios and publish and visualize the results in a water resource system study. WRSS is capable to incorporate various components of a large and complex supply-demand system as well as hydropower analysis for reservoirs since they have not been available in any other R packages. In addition, a particular coding system, devised for WRSS, allows water resources component to interact together by transferring mass in terms of seepage, leakage, spillage, and return flow. In addition to its capabilities, the package was successfully applied to a case study of a water resources system composed of 5 reservoirs and 11 demand sites and the results proved the role of WRSS in the enhancement of the dam operation under the large-scale model.";"oral presentation"
"Laier Gunnar";"Fast calculation of bessel functions with applications to statistical models in finance and geology";"We illustrate and compare different strategies for calculation of Bessel functions with applications to heavy tailed distributions in finance and geology.
 Calculation strategies ranging from very classic old-fashion hard-coding, optimization of traditional R functions and application of Rcpp are illustrated as we assess performance and precision of different implementations of Bessel functions in both tables and graphics. Precision and speed of calculations are related to identifiability and optimization problems within the family of generalized hyperbolic distribution and recent developments.";"poster"
"Joo Rocio";"Navigating through the R packages for movement";"The advent of biologging devices has led to the collection of ever-increasing quantities of tracking data from animals and humans. In parallel, sophisticated tools to process, visualize and analyze tracking data have been developed in abundance. Within the R software, we listed 57 packages focused on these tasks, called here tracking packages.
Here we review and describe each package based on a workflow centered around tracking data, broken down in three stages: pre-processing, post-processing and analysis (data visualization, track description, path reconstruction, behavioral pattern identification, space use characterization, trajectory simulation and others). 
Links between packages are assessed through a network graph analysis and show that one third of the packages worked on isolation, reflecting a fragmentation in the R movement-ecology programming community. 
Finally, we provide recommendations for users to choose packages and for developers to maximize the usefulness of their contribution and strengthen the links between the programming community.";"oral presentation"
"Thorne Brent";"Transitioning between various RMarkdown packages for workflow optimization in academic research; a graduate student's perspective.";"Reproducible documentation and the use of RMarkdown is becoming more prevalent in the academic world. Packages built on RMarkdown such as posterdown, xaringan, pagedown, and rticles allow for most aspects of a typical research project to be produced; however, newcomers to RMarkdown can be put off or discouraged by the inconsistencies in which each type of document needs to be formatted in the '.Rmd' file. This talk will provide insight into the benefits and downfalls of fully customizable RMarkdown packages with emphasis on (1) the importance of a timely workflow for the short duration of an average graduate student program; (2) introducing students or supervisors who are new to RMarkdown, and (3) show the importance of reproducibility in every stage of a good research project. Concepts from this presentation intend to spark conversation and show the importance of reproducible document generation in all stages of a research project.  ";"lightning talk"
"Hong Seong-Yun";"Exploring the geographic distribution of the elderly population at risk in South Korea";"The rapid demographic transition to an aged society, which is defined as a country in which people aged 65 or over exceeds 14 percent of the total population, in South Korea has increased the demand for social welfare and public health programs. There is however always limited budget for welfare services and care resources, so it should be efficiently managed and utilised. To address these challenges, we attempt to identify neighbourhoods where elderly people in need are concentrated using spatial segregation measures in R. We assess whether low-income seniors or those who live alone are geographically isolated in certain areas using the spatial exposure index implemented in the seg package and then visualise the results. If the old people are isolated in a few administrative zones, the accessibility of medical services, community centres and grocery stores for those living in the areas are evaluated. This process will be implemented as a function in R so it can assist policy-makers by quickly suggesting areas that needs more investment.";"poster"
"Smira Martin";"Shiny for real-time monitoring in travel tech company";"In our travel tech company, we face a vast amount of data both as an input and also on the output side. Due to the data amount and fast development culture, we can face possible glitches that can cause misbehaviour of our product. For the purpose of real-time monitoring one part of the product behaviour, we have developed a dashboard in Shiny that helps identify some issues early on and allow for investigation of the problem. We have faced some challenges while building the app since the data input on 61 440 000 records per day and therefore we had to use several tricks to boost the performance. ";"poster"
"Seynaeve Daan";"rjenkins and rrundeck: Coordinating Continuous Integration and Delivery with R";"Continuous integration is a software development practice that advocates for members of a team to merge their work frequently in a shared repository. Each integration is verified through a process of automated building and testing. This process can be facilitated through the use of a build server. A popular choice is Jenkins: a free and open source automation server. Continuous delivery is an extension of the continuous integration principle that asks that every successfully built version of the software is put into a staging environment from where it can easily be deployed to a production setting. The deployment process is still manual but it can be simplified through the use of self-service operations. Rundeck is an open source management platform that allows to define such self-service operations. We propose two new R packages: rjenkins and rrundeck. These packages interact with the Web APIs offered by Jenkins and Rundeck to easily create, trigger and monitor jobs and operations. This provides R users with an intuitive interface to these tools which can be used in an interactive or scripted way.";"oral presentation"
"Zhao Allan";"prVis: a Novel Method for Visual Dimension Reduction";"Visualization is an invaluable tool for gaining insight into complex high-dimensional data. Here we introduce prVis: a polynomial-based 2-D visualization method. PrVis, or polynomial visualization, captures potential nonlinear relationships in datasets by performing polynomial expansion and applying PCA on the resulting expanded polynomials. Our method produces an arguably better outcome on the famous Swiss Roll test case than do t-sne, UMAP, and ordinary PCA, and generally does as well or better than those methods on other datasets. PrVis offers options to further investigate how the clusters of data relate to the original variables. The prVis software package includes several features to support easier interpretation of visualized data, including color coding for discrete and continuous variables, alpha blending, adding row numbers to data points, and removing outliers. PrVis also provides users with options for random sampling and memory-mapping to better work with large datasets.

";"oral presentation"
"Højsgaard Søren";"Ironing online data until they are smooth enough with the dataIrony package";"The dataIrony package grew out of the need for smoothing online data
acquired in an agricultural science setting. Typical for such data, in
addition to becomming available online, is that there is no underlying
model describing the trend of data and data are often measured at
non-equidistant time points. Methods available in the package includes
exponential smoothing and recursive least squares methods. We shall
illustrate the applications of the package in a setting related
monitoring live weight of dairy cattle.";"lightning talk"
"Sanchez-Pla Alex";"Lheuristic-A Shiny app to select genes potentially regulated by methylation";"Methylation is a key process in cancer. Usually it acts by inhibiting the expression of the gene but if methylation is low then any values of expression, high or low, can be found. This suggests that to select genes regulated by methylation one may look for patterns in the relation between gene expression and methylation showing either an L-shape or negative correlation between expression and methylation. We have developed a heuristic algorithm that mimics the process of visually selecting an ?L-shape?, that is genes that can show a wide range of expression values (low to high) when methylation is low, but only low expressions for intermediate or high methylation. We have compared the method with naïve correlation and, despite not being able to quantify its accuracy -because no dataset with ?TRUE? L-shaped genes is available- its performance seems to be very good especially due to its flexibility. The method has been implemented in an R package, ?Lheuristic? and a Shiny application, both available from GitHub (http://github.com/alexsanchezpla). Given two matrices -expression and methylation values - with the same row and column names the program offers the possibility to select genes based on either negative correlation, the heuristic algorithm or both methods at once. Once genes have been selected, results can be interactively reviewed, plotted or downloaded.";"poster"
"Alvarez Adolfo";"Building your machinery: an example of processes and solutions from marketing analytics";"In this poster we present how our data science team built an integrated set of tools for the full process of marketing analytics including data integration, transformation, modeling, reporting, simulation, and optimization. This poster focus on the process itself, giving an schematic view of the different tools we use, the scripts we have written, and how R was integrated with other software such as Bash, Bitbucket, H2O, or Docker among others.";"poster"
"Birr Stefan";"Pattern localization in electrical time series";"Using the rstudio interface to keras and tensorflow in R we constructed and trained a convolutional neural network to detect start and end time of a single device run from the consumption data of the complete household. This technique can be used in the area of non-intrusive load monitoring, to extract the consumption of single devices from the whole signal. Furthermore, it enables us to learn about the frequency and the typical time when a device is used in a household. While Deep Learning techniques require access to rather large datasets, the process of collecting this data is time and cost intensive. Also in the area of non-intrusive load monitoring the labelling is often not reliable enough. Therefore we created a large artificial dataset combining extracted pattern, created by collecting consumption pattern with smart plugs, and noise from multiple smartmeter readings. This enabled us to perform the training on a much bigger dataset than usually available. Another fascinating finding was that, although feature engineering is rather atypical when using deep learning, we were able to enhance our results significantly and were able to counter the information loss that comes with the necessary batch normalization. Additionally to our methods and models, I would also like to give some insights on the experimental iterations we went through before we arrived at our present result.  ";"poster"
"Michonneau Francois";"How a non-profit uses R for its daily operations?";"The Carpentries is a non-profit organization that organizes a global community of scientists that runs workshops where researchers are taught foundational computational and data science skills. Since 2012, The Carpentries has taught 2,000+ workshops in 46 countries reaching 38,000+ learners.    Here, I will present how The Carpentries uses R in its daily operations. From analyzing our survey results, to sending personalized certificates of workshop attendance to learners, from creating live-updating visualizations for our workshop instructors to understand their audience before the workshops, to our lesson templates, I will go through some examples of how R has become a central part of how we set up our systems to manage and automate our workflows within our organization.   The combination of literate programming to generate reports, web application development with shiny, and the availability of packages to interact with the web API for many of the online tools and services we use, has allowed us to develop custom workflows. We can iterate quickly and deploy using continuous integration services and Docker.   This talk will be of interest to organizations looking to automate their operations to demonstrate how R can successfully be used in production.";"oral presentation"
"Protacio Angeline";"Using R and the Tidyverse to Play Fantasy Baseball";"Of the existing R packages that help analyze baseball statistics, few are geared toward fantasy baseball, which benefits from projection data for drafting a team, and updated data for daily analysis to set game day rosters and maintain a fantasy team. This talk describes how to use the tidyverse package suite to bring in baseball projected and actualized data, analyze baseball statistics relevant to fantasy baseball, draft a competitive fantasy baseball team, and maintain this team throughout the course of a season. By applying the tidyverse package suite of data cleaning and visualization tools to baseball statistical analysis, R users already familiar with this popular suite of packages can learn how to play and succeed at fantasy baseball. Users not already familiar with fantasy sports can expect a quick primer on fantasy baseball and statistics, sourcing baseball data for analysis, and analytic approaches to apply to a league of their own. ";"lightning talk"
"Genaro Sucarrat";"General-to-Specific (GETS) Modelling with User-Specified Estimators and Models";"General-to-Specific (GETS) modelling provides a comprehensive, systematic and cumulative approach to statistical modelling ideally suited for hypothesis testing, forecasting and counterfactual analysis. The implementation of GETS-modelling, however, puts a large programming-burden on the user, and may require substantial computing power. We develop a framework for GETS-modelling with user-specified estimators and models, and provide flexible and computationally efficient functions via the R package gets (Pretis, Reade and Sucarrat (2018), J. of Statistical Software). In addition, the framework permits the user-specified estimation to be implemented in an external language (e.g.\ C++, Python, STATA, EViews, MATLAB, etc.).";"oral presentation"
"Hejblum Boris";"VICI: a Shiny app for accurate estimation of Vaccine Induced Cellular Immunogenicity with bivariate modeling";"In vaccine clinical development, it is important to assess the ability of a candidate vaccine to generate immune responses. Cellular immunogenicity is usually measured by Intra-Cellular Staining (ICS) after specific stimulation. The usual method for the analyzing such data is to i) first subtract the response observed in non-stimulated cells from each stimulated sample; ii) then perform an inter- or intra-arm comparison of the percentages of cells producing cytokine(s) of interest. Step i) aims at capturing the antigen-specific response, but the subtraction may induce biased estimates and compromise type 1 error and harm statistical power. We have proposed a bivariate regression model as a new method for an accurate estimation of the vaccine effect from ICS data in vaccine trials. This allows to model a linear relationship between the non-stimulated response and the antigenic responses, while taking into account both of their measurement errors, taking all available cellular response information into account. Our method displays good performances in terms of bias, type I error control and statistical power under various scenarios in numerical simulations. We present a user-friendly R Shiny application (relying on the nlme package) that we implemented for immunologists to be able to use our method directly, and we applied it to analyze data from two HIV vaccine trials.";"lightning talk"
"Coors Stefan";"mlr3ordinal";"Ordinal regression is a method to model targets of ordinal nature, e.g. preference values. We present a more efficient and general alternative compared to classical approaches like ordinal binary decomposition or traditional statistical thresholding models such as the cumulative link. These classical approaches are either computationally demanding - as they require fitting of multiple models - or are limited to a single, often linear model. Our model-agnostic approach casts the problem as a regression problem first, then assigns ordinal classes by optimizing multiple thresholds. This allows the use of arbitrary non-linear ML regression techniques, including ensembles or model stacks. Thresholds are determined by black-box optimizing them w.r.t. to any performance measure of choice and cross-validation to avoid overfitting. The package also includes common ordinal regression performance measures for model evaluation.  ";"poster"
"Grose Daniel";"Collective and Point Anomaly Detection in R";"Anomaly detection is a topic of considerable importance and has been subject to increasing attention in recent years. This is certainly true within the R community, as evidenced by the number of packages hosted by CRAN related to this area.

The majority of packages contain approaches aimed at detecting point anomalies i.e. single observations, which are anomalous with regards to their data context. We introduce the anomaly package, which contains novel methodology aimed at detecting not only point anomalies but also collective anomalies, i.e., anomalous data segments. Importantly, the main algorithm of the package is capable of simultaneously detecting and distinguishing between both anomaly types. The utility of this approach is demonstrated on both simulated data, and real astrophysical data from NASA's Kepler telescope, which is included within the package.";"oral presentation"
"Vaidyanathan Ramnath";"satin: Large Beautiful Shiny Dashboards Made Easy!";"Dashboards built using Shiny and shinydashboard have made their way into multiple industries ranging from finance to consulting. They provide a really powerful way for data scientists to turn their analyses and insights into an engaging experience that can be consumed by others. However, building such large dashboards in a collaborative manner has been a challenge. In this talk, I will introduce the R package, satin, that provides a highly opinionated way to organize and build large, multi-page shiny dashboards. Satin (http://github.com/ramnathv/satin) allows users to focus on building independent modules for each page of the dashboard following a minimal set of conventions, and relegates all the boilerplate code involding in laying them out in the dashboard to a simple YAML configuration file. We have used satin successfully at DataCamp to deploy a large dashboard app developed in a highly collaborative manner. In this talk, I will (1) introduce satin and demonstrate its capabilities, (2) build a dashboard using satin in real-time, and (3) present best practices on developing large shiny apps. This talk is aimed at those with some experience/interest in building dashboards with shiny.";"oral presentation"
"Gasparrini Antonio";"The R package mixmeta: an extended mixed-effects framework for meta-analysis";"The package mixmeta offers an extended mixed-effects framework for meta-analysis, where potentially complex patterns of e?ect sizes are modelled through a ?exible structure of ?xed and random terms. This allows the definition of standard univariate fixed and random-effects models, but also extensions separately proposed in the literature, such as multivariate, network, multilevel, longitudinal, and dose-response meta-analysis and meta-regression. The main function mixmeta provides a simple way to specify alternative models through a formula syntax consistent with standard linear mixed-effects packages. Likelihood-based estimation procedures are implemented using an efficient and robust hybrid approach consisting of a combination of iterative generalized least squares and Newton-type algorithms. Methods functions for standard regression tasks and specific meta-analytical summaries. The package is thoroughly documented and includes illustrative examples that replicate published analyses using a variety of meta-analytical models.";"oral presentation"
"Sasso-Schafer Katie";"Transitioning from Academia to Industry: Using R for CaRReeR Success";"The academic job market can be tough, with limited positions and little flexibility in where you live and work. Given this, many Ph.D. students are seeking ?industry? or business focused careers. In this talk, you will learn how to translate your R and statistics expertise from your academic research into ?business speak?. From physics and psychology to political science and statistics, learn how to make your PhD training work for you. Practical discussion of topics including: converting your CV to a resume using packages like page-down, translating your academic statistics training into sexy ?machine learning? skills and key packages to know (xgboost, Caret, GAMLSS) and other resources like Interpretable Machine Learning by Molnar and Feature Engineering and Selection: A Practical Approach for Predictive Models, by Kuhn & Johnson. Perhaps less intuitive but equally important skills will also be discussed including networking, business acronyms, and some considerations to help guide your decision. Practical handouts including a statistical terms cheat sheet and a CV to Resume helper will be provided.";"poster"
"Falip Joris";"Why do we even use that?! Learning how to teach R while teaching how to learn R";"After teaching a computer science class introducing economics students to programming using R, I want to share a post mortem of what went wrong, why, and how I tried to overcome these difficulty. It encompasses the ""teaching base R or tidyverse"" question, the ease of access of some more advanced features like RMarkdown (and even Git) and the pros and cons of R as a first programming experience. The whole argument is based on an extensive survey involving close to fifty students from different backgrounds, using it to showcase their difficulties and joys in learning to code, using their own words";"poster"
"Beatriz Milz Beatriz";"The evolution and importance of the R-Ladies São Paulo chapter in Brazil";"R-Ladies is an worldwide organization that promotes gender diversity in the R community. Brazil is a developing country in Latin America, that currently has 9 R-Ladies chapters, and the one in Sao Paulo was created in August 2018. By February 2019, it had almost 300 members, showing an impressive growth in such a small period of time. The chapter already held 7 events (four 3 hour meetups, one datathon and 2 whole day workshops), and other meetings are being planned. All of the content presented is made available on github, so anyone can access it. The group provides a safe environment for people interested in R, and all of the activities are free to attend. The gratuity of the activities is very relevant in the Brazilian context, since the few R courses available in Portuguese are often expensive, and there is no other active R group in Sao Paulo. The chapter also collaborates with other projects, through lectures and workshops, also always open to the community. The increasing popularity of the group shows us how important it is to support it, what can be the key to motivate the creation of other chapters in Brazil and increase the strength of the Brazilian R community.";"lightning talk"
"Millo Giovanni";"Common Correlated Effects Methods in R";"In the last decade, econometricians have devoted increasing attention first to the issues of nonstationarity and cointegration in pooled time series data, then to the related problem of crosssectional dependence induced by common factors. Among different approaches, Pesaran's Common Correlated Effects (CCE) augmentation has proved particularly promising both for consistent estimation and unit root testing under nonstationarity and cross-sectional strong dependence. An extension of package plm is described, comprising pooled and heterogeneous CCE estimators in both the mean groups and the generalized fixed effects (CCEP) styles. The same software framework allows for cross-sectional-correlation robust, so-called second generation unit root tests as cross-sectionally augmented Dickey-Fuller (CADF) regressions; and for common-factor robust pooling tests as well. Lastly, implementing CCE estimators as OLS on transformed (here: projected) data allows for easily extending the general framework for cluster-robust and for block-bootstrapped standard errors to this class of estimators.

";"oral presentation"
"Perrot-Dockes Marie";"Penalized regressions to study multivariate linear models : the VariSel package.";"We propose a new methodology to study multivariate linear models.
It consists in estimating the covariance matrix of the responses beforehand and plugging this estimator into a penalized regression model to perform variable selection.
Depending on the application field and the data at hand, different penalizations are preferred. For example, recoursing on a l1-norm penalty performs a ""simple"" selection in the regression coefficients. In certain situations, we need to select the same variable for all the responses or a group of variables all together. This can be done by means of a penalty similar to the sparse group Lasso (Yuan and Lin (2006)). Another example is the fused-Lasso penalty, where one want to influence variables to have the same coefficient (Hoefling (2010)). We study an example in genomic for study water-use efficiency (WE) of vine. The covariables are the number of replicates of the different alleles of different genes and the responses characterize the WE of the different vines. The aim is to select genes or alleles that have an effect on this WE. We consider several ways to study the problem: either by selecting an allele of a gene for a specific response or by influencing it to have the same coefficients for all the responses. It can also be interesting to select all the alleles of the same gene together. The presentation will show how to do all of this with the VariSel package.";"lightning talk"
"Preu Friedrike";"Authentication and authorization in plumber with the sealr package";"Application Programming Interfaces (APIs) have become the most common way how services ""talk"" to each other, e.g. in a typical frontend-backend design. The plumber package offers capabilities to implement an API in R, making it possible to use R for software development use cases that often require security best practices. In my talk, I will show how we can use plumber filters to secure plumber APIs. I then present the sealr package (github.com/jandix/sealr) which provides standardized strategies for authentication and authorization, namely JSON Web Tokens (JWT) (implemented), OAuth2 and general API tokens (under development at time of submission). sealr is inspired by the manifold passport.js package for Node.js. The main functionality of sealr is verifying tokens sent to plumber - how those tokens are issued by plumber is not covered (yet) as it is highly application-specific. However, we provide implementation examples for each method.
As authentication middleware specifically developed for the plumber framework, sealr differs from packages such as sodium, openssl, jose and bcrypt that implement specific encryption and/or hashing algorithms. secret, digest and keyring are used for securely storing (R) objects.
With sealr, we aim to make authentication and authorization as easy as possible to implement for R users so that plumber APIs can be used in security-sensitive environments.";"oral presentation"
"Alvarez Castro Ignacio";"Visualizing and modeling educational online platform data";"Plan Ceibal is an uruguayan initiative to implement the ""One lapto per child"" program, it was created in 2007 as a plan for inclusion and equal opportunities in acces to tecnology within uruguayan educational public system. The main concept of Plan Ceibal program consist in that every child who enters the public education system in any part of the country is given a computer for personal use with free Internet connection at school. Huge ammount of data is generated each year describing the use of many different digital plataforms with interesting patterns at many levels of study (child, class, school, regions, etc). There are two main goals in this paper. First, we develop interactive tools to summarize and vizualize how digital plataforms are actually being use by the primary school students. This would help authorities to asses and understand how tecnology is being used in the classroom. Shiny, ggplot2 and plotly packages are intensivelly used for this goal. Secondly, statistical models to understand the impact of the so-called blended learning programs in student performance are proposed. Machine learning algorithms are used, the analysis need to be scalable since the large amount of the data generated every day by the different digital plataforms. H2O package is used in order to be able to fit these models in reasonable time.    ";"poster"
"Nazarov Maxim";"Packaging shiny applications";"Shiny applications are ubiquitous nowadays and traditionally created as a collection of R script files (optionally with external resources, such as images, css files etc). When the applications grow larger it may become difficult to manage these files.

I will advocate the use of R packages for creation of shiny applications. This approach allows to leverage all the advantages of the R packaging ecosystem, including managing namespaces and dependencies, versioning, documentation and tests. These are especially important when deploying and supporting shiny applications in production. In particular I will talk about using functions to define both ""server"" and ""ui"" components, and separating logical application pieces into shiny modules for easier structuring and sharing.

The aim of this talk is to encourage people to try this approach by discussing the differences with the traditional (file-based) approach, advantages of having an R package and things to watch out for. Finally I will mention how a packaged shiny application can be deployed in an enterprise context using ShinyProxy.";"lightning talk"
"Schmid Roland";"compareWith - user-friendly diff viewing and VCS interaction";"Version control systems provide an important environment for controlled code and software development. In the case of R and RStudio however, the integration of version control tools is still significantly behind what is desirable. This has become a common typical concern for the whole community, especially with the uprise of Github and git for open-source R package development, where issues are being tackled through separate branches and contributors are more numerous and heterogenous.

Command-line git interaction can be an additional barrier and so individuals and organizations have sought different ways to deal with the shortcomings. In this talk, we propose a flexible light-weight combination with Meld (http://meldmerge.org/), an open-source visual diff and merge tool, and demonstrate ""compareWith"" (https://github.com/miraisolutions/compareWith), an R package that allows users to interact with Meld from within RStudio.

compareWith provides user-friendly addins that enable and improve tasks that are otherwise difficult or impossible without any custom extension.
Examples include: i) compare differences prior to commit, for single active files or the whole project; ii) resolve and merge conflicts via three-way comparison; iii) compare 2 distinct files with each other. Even simple tasks benefit from the improved diff viewing tool compared to what is built into RStudio.";"lightning talk"
"Wiedemann Kenia";"Mathematical Modeling with R: Embedding Computational Thinking into High School Math Classes in the United States";"Only 35% of high schools in the US offer computer science (CS). However, by 2020 50% of all new STEM jobs will require some knowledge in CS and coding. Developing computational thinking (CT) across all disciplines and educational levels have become a priority for scholars and agencies, urging to integrate CT across K-12 curricula. CT can be better learned when blended to primary subject areas. CodeR4MATH is an NSF-funded project working to provide a collection of learning and tutoring resources for math educators to collaborate in an R environment. Linking math to real-life problem solving, math modeling is an iterative process involving situation representation, math operations, interpretation, and validation. The modules are being developed to be used in high school math classes in Stats, Algebra I and II. We present here the results from the first module (named Lifehacking) implementation, showing students greatly benefit from investigating meaningful problems that appeal to their personal interests. It guides students to model in R to answer practical questions such as the costs of eating in college or the costs of owning a car. Tasks are delivered via R tutorials using learnr tools, with students using real data to create their models. They are then guided to move to RStudio exploring its capabilities. The 2nd and 3rd modules are under development, focusing on Earth Sciences and Engineering.";"oral presentation"
"Porreca Riccardo";"Shiny app deployment and integration into a custom website gallery";"R Shiny has become overwhelmingly popular and widespread in the R community. Shiny web applications provide companies and individuals with an excellent way of demoing and showcasing data analytics and visualization, and interactive R-based projects in general. Although products and services exist for exposing Shiny apps on the web (Shiny Server, RStudio Connect, Shinyapps.io, ShinyProxy), there is a desire for and a clear benefit of full integration with an existing website. This allows providers to retain full flexibility in terms of customization and the experience provided to users exploring their apps. In this talk, we cover the challenges and requirements for the seamless embedding and integration of Shiny apps into a custom gallery part of a larger website. We will show how we used Docker and Kubernetes as natural choices for deploying and running Shiny apps, and for customizing the way they are served and made accessible through the web. We explain how to set up an embeddable Shiny app, how to build, maintain and deploy the corresponding Docker image, and how to configure the relevant Kubernetes resources. Finally, we will demonstrate how all this can easily be combined into a GitHub Pages website based on Jekyll.";"oral presentation"
"Gallopin Mélina";"Appinetwork : Analysis of Protein-Protein Interaction NETwork";"We present the package appinetwork (Analysis of Protein-Protein Interaction NETwork), a tool that facilitates extraction and analysis of physical or genetic interactions between proteins in various organisms. The Protein-Protein Interactions (PPI) files come from personal ressources or different public repositories (IntAct, BioGRID, iRefIndex... ). The package enables to reformat the files coming from different ressources and provide an output file with the different identification numbers of the same protein across the repositories. Starting from a list of proteins provided by the user (proteins of a given biological process or subunits of a protein complex), appinetwork extracts from the formatted files the relevant interactions and constructs the PPI networks of first and second degrees of the input list. Subsequently, the package enables to look for assembly intermediaries using several computing methods and to search for all proteins involved in the biological process of interest. The user can easily choose the action to carry out (Files formatting / Building the network / Searching for proteins involved in a biological process / Modeling the assembly of a protein complex) and the parameters via a user-friendly interface. Results are provided in files that could be easily exported. The package is available on GitHub https://github.com/melinagallopin/appinetwork.";"poster"
"Miller Mikaela";"Becoming a mission-driven R-ganization: examples of R solutions in the non-profit sector";"Children International (CI) in a mission-driven organization based in Kansas City, MO, USA that works in ten countries to end poverty through local partnerships, and child and youth programming in the areas of health, education, empowerment, and employment. As with many non-profit organizations, CI has experienced resource constraints concerning information management and data analysis. In 2017, CI formed a new analytics department charged with providing data understandings and insights to various levels of the organization. Open-source and available to all constituents globally, R has been used for ad hoc analyses and provided a number of no-cost solutions in the form of functions, Shiny apps, and parameterized reports. An auto-modeler was built using xgboost to aid in predictive modeling for targeting of various marketing and fundraising campaigns. Through the deployment of an R Shiny app, the client care team was provided with a ?next-best action? and data capture tool to enhance and automate the existing spreadsheet-managed process. R Markdown parameterized reports and an R Shiny app have been created for the global programs team to assess nutritional status of children according to WHO standards. The development of these flexible and interactive tools through R has begun to facilitate a culture transition to wiser and more efficient usage of data.";"poster"
"Fabiola La Gamba";"Bayesian sequential integration within a preclinical PK/PD modeling framework using rstan package: Lessons learned";"Although Bayesian methods are expanding considerably, their applications in the field of pharmacokinetic/pharmacodynamic (PK/PD) modelling is still relatively limited. In this work, Bayesian techniques are used to estimate a novel PK/PD model developed to quantify the PD synergy between two compounds using historical in vivo data. The model is fitted using package rstan, the R interface to Stan. Stan is a recently developed software package which allows an efficient estimation using the No-U-Turn Sampler (NUTS). rstan facilitates the use of this powerful tool, allowing to run Stan models within R. Since the data consist of a series of trials performed sequentially, a Bayesian sequential integration is considered: the posteriors resulting from the analysis of one trial are used to specify the priors of the next trial. Challenges and opportunities of this technique will be discussed with respect to: (i) prior specification, (ii) random effect choice, (iii) experimental design. In addition, the results from an extensive simulation study assessing the performance of the Bayesian sequential integration for an increasing model complexity are evaluated. The results suggest that the Bayesian sequential integration is promising in certain settings. The specification of informative priors and a suitable experimental design are nevertheless advisable, especially in preclinical studies.";"lightning talk"
"Flores Siaca Ian";"La combinaison du Linux et la R: how to use them together for open science";"   The first version of Linux was released in 1991 and the first version of R was released in 1993. Since then, both ecosystems have developed into immense open source communities. Nevertheless, using Linux and R together still seems exclusive to people focusing in bioinformatics, High Performance Computing and some developers. In this lightning talk, I aim to explain why the use of Linux and R is beneficial in the long term for the R community. I will also add on to address how it helps bring people from diverse backgrounds by making computing resources more accessible. (1.5 min) After this, I will explain how people can get started in Linux and the steps to get up to there using as example the installation of the tidyverse. This will also include an explanation of how the installation process is different between Operating Systems. (2.5 min) To conclude the talk, I will proceed with an explanation of how Linux can be incorporated in the educational process of teaching R. (1 min) This talk would be aimed at R users who are interested in incorporating Linux to their R workflows.";"lightning talk"
"Varewyck Machteld";"Automated Surveys and Reports for Expert Elicitation with Shiny";"Experts are often consulted to quantify uncertainty on subjects when insufficient data is available, e.g. the temperature in Toulouse next week. The goal is to find a consensus by combining all experts' knowledge. We have built an R Shiny application to automate this procedure for an international governmental organization.  The administration module allows to create a survey, invite experts and control the elicitation phase. When elicitation is completed, a report can be built automatically. This helps to find a consensus by plotting e.g. the combined distribution of all experts' elicitations. The administrator can, at any time, download the available data for use outside the Shiny application. In the elicitation module, the assigned experts can define values for the requested quantiles in a table. The corresponding distribution is shown in an interactive barplot. By dragging the breakpoints in the barplot, the provided elicitation values in the table are automatically updated and vice-versa. 
 During this talk, we will share our experiences on deployment with ShinyProxy. This allowed us to easily restrict the administration module to authorized users only and to guarantee persistent data storage using a docker volume.";"oral presentation"
"Valencia Cárdenas Marisol";"Forecasting with Bayesian processes for pollution variables.";"Forecasting processes techniques have been improving with increasing attention to facilitate the efficiency for prediction an uncertain process, for example, to improve enviromental decision making. Models based on Bayesian Inference has currently an increasing attention, in special more accurate results have been demonstrated. This work presents a Bayesian Model that learns from statistical data components characteristics, that improves forecasting accuracy. The application is presented to a pollution variable about air contamination, measured in Medellin city, and of course, programmed in R. Bayesian Statistics uses probability distributions to model the random variables uncertainty for data forecasting and for parameters. A distribution for parameters of the model is considered, but also, a predictive distribution is used to do the forecasting process. Parameter estimation are crucial for prediction, because they should represent a similar behavior of the data that present a seasonal behavior and correlation with their past. The Bayesian Regression Model considers these dynamic characteristics, which in this case will perform a prediction about pollution, important to politics formulation about the environment.";"poster"
"Guyader Vincent";"Golem : A Framework for Building Robust & Production Ready Shiny Apps";"Shiny has established itself as an essential communication tool in the datascience field, and has gone beyond the scope of simple R users.
Its ease of use makes it possible to produce ""quick and dirty"" applications.
However, as soon as we try to go beyond the proof-of-concept and produce applications with professional vocation, some development and deployment aspects must be taken into account.
Designing a shiny application that is maintainable over time, scalable, tested, robust and deployable is not an easy task. And like everything R-related, there are more wrong ways than right ways to do so.

With the {golem} package, we suggest a development template that relies on a R package. It contains a series of tools that allows you to add functionalities to your application for both development and deployment. For example `golem::add_module()` allows to add a Shiny module, `golem::use_favicon()` allows to add a favicon to the application. In addition, functions such as `golem::add_rconnect_file()` or `golem::gen_dockerfile()` allows you to easily deploy your application on RSconnect or shinyproxy. 

{golem} is an opinionated template designed to help easy development, deployment and maintainance inside a documented R package.";"oral presentation"
"Burkhardt Johannes";"RAdwords - Utilizing the Google Ads API with R";"Advertising with Google Ads plays a significant role in e-commerce and digital marketing. A data-driven approach to online marketing is very important for online marketers to run successful marketing campaigns. This includes the evaluation of the campaign objectives, campaign data analysis and business intelligence reporting in a programmatic fashion. RAdwords is a project to make use of Google Ads campaign data in R. The project provides an R package that is an interface to the Google Ads API with the aim to load Google Ads data directly into R. RAdwords, as an R client library for the Google Ads API, provides three main features: First, the package implements an authentication process for R with the Google Ads API via OAUTH2. Second, the package offers an interface to apply the Adwords query language in R and query the Google Ads API. Third, Google Ads data are transformed into suitable data formats for further data processing and data analysis in R.";"poster"
"Mariosa Daniela";"MR studies in R: how to use genetic information for identifying modifiable risk factors";"Mendelian randomization (MR) is a powerful approach to study causality by using germline genetic variants associated with a risk factor (exposure) as instrumental variables for the risk factor itself. The growing availability of results from large genome-wide association studies, not only for clinical outcomes but also for lifestyle exposures, makes MR analyses based on summary genetic data relevant for many exposure-outcome relationships. Properly conducting MR studies in R requires several steps that involve packages for both estimation and data visualization. We will present how to best exploit R to perform and present two-sample MR analyses with an example of our work on the role of obesity-related factors in cancer risk. The approach include the harmonization of the genetic information for the exposure and outcome, the estimation of the causal effect of the exposure on the outcome using different estimators, and a number of complementary analyses for the evaluation of potential pleiotropy, heterogeneity, assumption violations, and bias. The wide range of techniques required to conduct a robust MR analysis is reflected in the use of both largely used and MR-specific packages.";"lightning talk"
"Robin Geneviève";"R package lori: A new multiple imputation method for count data with side information";"In ecology, and particularly in species monitoring, one encounters count tables with many missing values. In these settings, multiple imputation is often used to produce complete data sets which correctly reflect the uncertainty related to the missingness. Side information in the form of covariates describing the rows (environmental sites, etc.) and columns (times stamps, species, etc.) of the table are often also available. The simultaneous analysis of covariates with the count table is useful in order to (i) improve the quality of imputation if covariates are good predictors of the counts (ii) assess the relationship between the covariates and the counts to detect factors which may be favourable or adverse to species. We introduce an R package, lori, which implements a new multiple imputation method for count data with side information, and achieves these two goals simultaneously. To generate multiple imputed data set, we rely on a resampling procedure combining nonparametric and semiparametric bootstrap, which produces several (incomplete) new data sets. Then, each data set is imputed using a Poisson log-linear model which may incorporate main effects of rows, columns and/or covariates, and row-column interactions; the model is fit with a doubly penalized maximum likelihood procedure. The package also contains functions to aggregate and visualize the multiple imputation results.";"poster"
"Audigier Vincent";"micemd: a smart multiple imputation R package for missing multilevel data";"Multiple imputation is a common strategy to overcome the missing data issue. Several MI methods have been proposed in the literature to impute multilevel data with classical sporadically missing values only. However, methods for dealing with more complex missing data are needed. Indeed, the multilevel structure is often due to data merging (because of the heterogeneity between collected datasets), but variables often vary according to the dataset, leading to systematically missing variables. micemd is an addon for the mice package to perform multiple imputation using chained equations with two-level data. It includes imputation methods specifically handling sporadically and systematically missing values (Resche-Rigon et al. 2013, Audigier, V. et al, 2018). micemd offers a complete solution for the analysis: the choice of the imputation model for each variable can be automatically tuned according to the data structure (Audigier, V. et al, 2018), it gathers tools for checking model fitting (Blackwell, M. et al, 2015) and allows parallel calculation. The talk is motivated by a meta-analysis in cardiovascular disease consisting of 28 observational cohorts in which systematically missing and sporadically missing data (GREAT data). Then, based on a simulation study, advantages and drawbacks of each multiple imputation method are discussed. Finally, methods are compared on the GREAT data.";"oral presentation"
"Dietze Michael";"sandbox - an R package to create and analyse virtual sediment sections for geoscientific purpose";"Concepts and hypotheses in Earth sciences are usually formulated based on empiric data from the field or the laboratory (deduction). After translation into models they can be applied to case study scenarios (induction). However, the other way around ? expressing hypotheses explicitly by models and test these by empiric data ? is a rarely touched trail. While there are numerous models to investigate the processes that generate, mobilise and route sediment across a landscape, the final step ? sediment deposition ? is usually omitted. Essentially, there is no model that explicitly focuses on mapping out the characteristics of sedimentary deposits ? the material that is used by many disciplines..  The R-package sandbox is a model framework that allows creating and analysing virtual sediment sections for exploratory, explanatory, forecasting and inverse research questions. sandbox is a probabilistic and rule-based model framework for a wide range of possible applications. It has been advanced and linked to another model to allow the full work flow of modelling luminescence measurements. This contribution introduces news about recent developments and shows a set of applications.";"poster"
"Giner Goknur";"Pathway-VisualiseR: An Interactive Web Application for Visualising Gene Networks";"RNA-seq based expression profiling allows us to quantify gene expression changes in pertinent biology and to discover a set of biomarkers that are potentially the origin of the biological phenomenon of interest. Moreover, investigating the collective behaviour of the biomarker genes as a set, utilising databases such as Gene Ontology (GO), KEGG (Kyoto Encyclopedia of Genes and Genomes), STRINGdb (Search Tool for the Retrieval of Interacting proteins database), play an important role in gaining a comprehensive knowledge about the relevant biological inquiry. Here, we built a web-based interactive RShiny tool in order to gain further insights into the alterations within gene sets and the changes in the interactions between them. PathwayVisualiseR implements a fit object, which is the output from a linear model that was fit for each gene for a given series of samples, from the limma R package. PathwayVisualiseR, therefore, enables the user to incorporate the results from gene set tests, such as Camera, Roast and Fry. To summary, visualising gene set interactions with PathwayVisualiseR helps in speeding up the process of comprehending large or complex data sets and unfolds the relationship among the functional pathways and biological mechanisms.";"lightning talk"
"Carpentier Florence";"SILand: an R package for estimating the spatial influence of landscape";"In ecology or in epidemiology, understanding how landscape structures the spatial distributions of species and populations is crucial to determine management rules for sustainable systems in agriculture and conservation biology. However, identifying landscape effects remains difficult, especially since no easy-to-use tools are available for this type of analysis. Here we present 'SILand', an R package. It is the first user-friendly tool that permits a complete and complex analysis of landscape effects, using few functions based on a ?classic? syntax similar to well-known packages (such as stat and lme4). It can be used for the following: (i) quickly import GIS files in R; (ii) infer the spatial influence of landscape variables and the area of significant influence of each landscape variable and (iii) provides highly informative visualizations of the results (prediction maps).";"lightning talk"
"Tvedebrink Torben";"genogeographer - a tool for ancestry informative markers";"Ancestry informative markers (AIMs) are genetic markers that give information about the genogeographic ancestry of individuals. They are for example used to predict the genogeographic origin of individuals related to forensic crime and identification cases. A likelihood ratio test (LRT) approach is derived in order to prevent erroneous conclusions due to e.g. no relevant populations in a database of reference populations. The LRT is a measure of absolute concordance between a profile and a population, rather than a relative measure of the profile's likelihood in two populations by a likelihood ratio (LR). The LRT is similar to Fisher's exact test and constitutes an outlier test analogous to studentized residuals. Varying sample sizes of the reference populations in the database are explicitly included in the LRT with fewer assumptions than the LR. The LRT is extended to handle admixed profiles (parents of different genogeographic origin). The methodology has been implemented in the R package genogeographer with an optional shiny front-end, that enables forensic geneticists to make explorative analyses, produce various graphical outputs together with evidential weight computations.";"oral presentation"
"Baker Peter";"R gnumaker: easy Makefile construction for enhancing reproducible research";"Is there a crisis in reproducible research? Some studies such as Ioannidis et. el (2009) have estimated that over fifty percent of published papers in some fields of research are not reproducible. In statistical and data analysis projects, this appears to be due to lack of training and poor tools rather than scientific fraud.

In the R community, there is a move towards Don't Repeat Yourself (DRY) approaches to reproducible research and reporting. Employing computing tools like GNU Make to regenerate output when syntax or data files change, GNU git for version control, writing GNU R functions or packages for repetitive tasks and R Markdown for reporting can greatly improve reproducibility.

The gnumaker package is ideal for statisticians and data analysts with little experience of Make. By specifying the relationships between syntax and data files gnumaker makes it easy to create and plot GNU Makefiles as DAGs. Gnumaker employs Make pattern rules for R, Sweave, R Markdown, Stata, SAS, etc outlined in P Baker (2019) Using GNU Make to Manage the Workflow of Data Analysis Projects, Journal of Statistical Software (Accepted). See https://github.com/petebaker

";"oral presentation"
"Van Moerbeke Marijke";"Multi-data learning with M-ABC: extending your ABC's";"The paradigm of the current revolution in biomedical studies, and life-sciences in general, is to focus on a better understanding of biological processes. In the early stages of drug development, different types of information on the compounds are collected: the chemical structures of the molecules, the predicted targets (target predictions), various bioassays, the toxicity and more. An analysis of each data set could reveal interesting yet disjoint information. An important task is the integration of the data sets from the experiments to understand the working mechanism of the drug. Multi-source clustering methods aim to discover groups of objects that are consistently similar across data sets. We introduce a new multi-source clustering method, M-ABC, which applies a consensus function generating a clustering of clusters. This can improve the quality of individual clustering algorithms. The proposed method is implemented and publicly available in the R package IntClust which is a wrapper package for a multitude of ensemble clustering methods. In addition, visualization for a comparison of clustering results and cluster specific secondary analyses such as differential gene expression and pathway analysis are available.";"oral presentation"
"De Cillia Gregor";"persephone, seasonal adjustment with an object-oriented wrapper for RJDmetra";"The R-package persephone is developed to enable easy processing during the production of seasonally adjusted estimates. It builds on top of RJDmetra and provides analytical tools such as interactive plots to support the SA expert. The package should make it easy to construct personalized dashboards containing selected plots and diagnostics. Furthermore it will support hierarchical time series and tackle the issue of direct vs indirect adjustment.";"lightning talk"
"Berland Magali";"In-silico benchmark of methods for detecting differentially abundant features between metagenomics samples";"Metagenomics studies microbial communities by sequencing their genetic material. This is done by targeting either a marker-gene (barcoding) or all the genes present in the samples (shotgun sequencing). It has been extensively used to characterize taxonomic and functional profiles of many ecosystems including the human gut microbiota. In the later, the shifts in the abundancy of specific species are used as biomarkers for many biological or clinical conditions such as cancer, diabetes or inflammatory bowel disease. However a lot of technical difficulties arise when working with such data: high-dimension, noise, high sparsity, low number of replicates... Thus, detecting these shifts with satisfying precision and recall is challenging. Many statistical methods were introduced in the past decade, each trying to overcome specific constraint of the data. In this study we present a benchmark of the most commonly used methods for detecting differentially abundant features between samples. By using semi-simulated data, statistical performances such as true/false positive or negative rates are assessed. In addition, results on real microbiome datasets are qualitatively discussed.";"poster"
"Cano Emilio L.";"R and Shiny to support real estate appraisers: An expert algorithm implementation for Automated Valuation Models";"Automated Valuation Models (AVM) allow mass valuation of real estate quickly and easily. There is a need for such valuation in the banking sector, not only for accounting purposes but also for regulations compliance. Traditionally, appraisers with expert knowledge following some common practices and standards made those valuations. This expert method leads to a set of so-called ""comparables"", i.e., close properties with similar characteristics, to value a property. The emergence of Big Data and other breakthrough Data Science technologies has triggered a lot of new approaches using statistical tools, such as regression, tree-based methods or neural networks, among others. However, an expert algorithm approach proves to be useful and more understandable for the real estate market stakeholders. In this work, we present the implementation of an expert algorithm in R that mimic the behavior of human appraisers searching comparable properties in a curated database. The set of rules has been set after a thorough analysis in cooperation with an important appraisal company. These rules include outliers detection, quality data classification and distance-based estimation. The scripts are embedded into Shiny applications that allow analysts to assess the accuracy of the method and appraisers to seamlessly run the AVM over a given property, checking all the steps of the estimation method.";"poster"
"Castellano-Escuder Pol";"POMA: Shiny tool for targeted metabolomic data statistical analysis and visualization";"Similarly to other high-throughput technologies, metabolomics usually faces a data mining challenge to provide an understandable and useful output to advance in biomarker discovery and precision medicine. Biological interpretation of the results is one of the hard points and several bioinformatics tools have emerged to simplify and improve this step. However, sometimes these tools accept only very simplistic data structures and, for example, they do not even accept data with several covariates. POMA is a free, friendly and fast Shiny interface for analysing and visualization data after an analytical targeted metabolomics process and its hosted on https://polcastellano.shinyapps.io/POMA/. POMA allows the user to go from the raw data to statistical analysis. The analysis is organized in three blocks: ?Load Data? (where user can upload metabolite data and a covariates file), ?Pre-processing? (value imputation and normalization) and ?Statistical analysis? (univariate and multivariate methods, limma, correlation analysis, feature selection methods, random forest, etc.). These steps include multiple types of interactive data visualization integrated in an intuitive user interface that requires no programming skills. Finally, POMA also generates different automatic statistical and exploratory reports to facilitate the analysis and interpretation of the results.";"oral presentation"
"Nocairi Hicham";"Stacking Prediction for a binary Outcome";"A large number of supervised classification models have been proposed in the literature. In order to avoid any bias induced by the use of one single statistical approach, they may be combined through a specific ?stacking? meta-model. To deal with the case of multiclass response, we introduce several improvements to stacking: combining models is done through partial least squares discriminant analysis (PLS-DA) instead of ordinary (OLS) due to the strong correlation between predictions, and a specific methodology is developed for the case of a small number of observations, using repeated sub-sampling for variable selection. Five very different models (Bagging, Naive Bayes, Support vector machine (SVM), Sparse PLS-DA, Weighted k-Nearest-Neighbor) are combined through this improved stacking, and applied in the following context: according to the 7th Amendment of the European Cosmetic Directive, L'Oréal is committed to stop animal testing. Consequently, l'Oréal must develop alternative approaches for safety evaluation of chemicals. Stacking methodology is applied to the prediction of potential sensitization of chemicals, based upon in vitro and in silico characteristics in order to predict a two groups. Results show that the stacking meta-model have better performances than each of the five models taken separately.  ";"poster"
"Kosmidis Ioannis";"Making sense of CRAN: Package and collaboration networks";"The Comprehensive R Archive Network (CRAN) is a diverse software ecosystem that is growing fast in size, both in terms of packages and package authors. The numbers at the time of writing this abstract are that CRAN involves more than 13.7K packages from about 19.2K authors. Keeping track of the ecosystem is, at least to me, hard, particularly in terms of effectively searching packages and authors through the network, summarising it and discovering new package and author associations. In this talk, I will introduce the cranly R package (https://github.com/ikosmidis/cranly) which streamlines and simplifies the analysis of the CRAN ecosystem. In particular, we will cover cranly's capabilities for cleaning up and organising the information in the CRAN package database, building package directives networks (depends, imports, suggests, enhances, linking to) and collaboration networks, and computing summaries and producing interactive visualisations from the resulting networks. I will also touch upon modelling, particularly on how directives and collaboration networks can be coerced to igraph (https://CRAN.R-project.org/package=igraph) objects for further analyses and modelling.";"poster"
"Galatolo Gabriele";"aRangodb: an R Package to Store, Retrieve and Visualize Data with the Multi-Model Database ArangoDB";"An increasingly important technology is represented by graph-oriented databases: in these systems information is organized considering relationships between data as ""first class citizens."" This makes it possible to express the inherent complexity in and between data more easily than with relational databases. In addition, more complex requests can be made with simpler queries. ArangoDB is a multi-model database which allows to use both the simplicity of document-like storaging and the capacity to express relationships with graphs. We developed a package to help users to get in touch with ArangoDB by combining ease of use as well as more sophisticated controls. The aRangodb package allows the user to: manage multiple databases contained in an ArangoDB instance; create collections of heterogeneous information managing them in a structured way by using R6 wrappers; filter and retrieve data from collections; create graphs and add relationships between existing data; traverse and visualize graphs using visNetwork as a visualization engine. For advanced users, the package offers the possibility to use the Arango Query Language to define R functions which, firstly, can be invoked in the normal workflow as R functions and, secondly, can be utilized to interact directly with the connected ArangoDB instance. The package is available at the URL: https://gitlab.com/krpack/arango-driver";"poster"
"Binder Martin";"mlr3pipelines: Machine Learning Pipelines as Graphs";"mlr3pipelines is an object-oriented dataflow programming toolkit for machine learning in R6. It provides an expressive and intuitive language to define ML workflows as directed acyclic graphs that represent data flows between computational units, e.g., preprocessing, model fitting and model combination. This chains data and model manipulation steps in a modular way to form powerful data processing pipelines. Many complex ML concepts, for which special purpose packages are usually provided, can now be expressed in few lines of graph definition code: e.g., unions of feature views, bagging, stacking and hurdle models. Resulting pipelines are parameterized, so all components can jointly be tuned to obtain an optimal configuration. Graphs can contain ?branching? nodes which allow selective, conditional processing of execution paths. The tuning of such tasks allows complex model selection. The modular, object-oriented concept of mlr3pipelines facilitates convenient extension with custom operations, while the compatibility with mlr3 allows convenient tuning, benchmarking, nested resampling and more. Project page: https://github.com/mlr-org/mlr3pipelines";"oral presentation"
"Schlackow Iryna";"Facilitating external use with user-friendly interfaces: a health policy model case study";"Health policy models are increasingly being used by clinicians, analysts and policy makers to predict patients' long-term outcomes, how these outcomes are affected by interventions, and whether the interventions are (cost-)effective and should be recommended for use. Usability of models as well as reliability and transparency of methods and results is therefore vital. Even when the code is freely available, laws preventing the sharing of sensitive individual patient data mean that the published results are still not fully reproducible. Additionally, model users may want to change input parameters, and therefore need to possess skills, and time, to understand the underlying code. We present a Shiny-based user-friendly web interface for a health policy model predicting progression of chronic kidney disease and cardiovascular complications. The interface is freely available and users can change different parameters using drop-down menus or .csv files; the output is a detailed downloadable .csv file, and a userguide is provided together with a range of templates. We discuss how, in addition to aid with the usability, such interface may help with debugging and transparency, and what the key considerations during the development are.";"oral presentation"
"Chiarello Filippo";"StakeholdeRs: Mapping People Impacted by R";"The aim of my work is having a clear picture of who are the stakeholdeRs of R (i.e. people impacted by, people that are advantaged by the software and packages). Based on the application of Tidy Natural Language Processing (NLProc) techniques, my system identifies phrases which refer to actual or potential stakeholders (e.g. women, programmers, biologists, artists etc...). The results are mapped in a graph in which nodes are the stakeholders and edges are the relationships (co-occurrences) among them. Further classifications have been done in order to discriminate between generic and specific stakeholders and to identify macro-classes of stakeholders (e.g. gender, professionals, hobbies). Since the discourse about the use of R is nowadays widespread into academia and to society as a whole, I analyzed two different sources of documents about the use of R language: scientific papers and twitter. I will present a map for each source, in order to have a comparison between the two. I will also present the graphs in different years (2008 and 2018) in order to understand the dynamics of the stakeholdeRs ecosystem. The stakeholdeRs maps will be discussed during the conference and published online. Besides discussing my system with NLProc experts, I hope that my work will start a conversation within the community about the actual (and future) impacts that the language we love has on society.";"poster"
"Fontez Bénédicte";"Use of sentinel-2 images in Agriculture";"This work gives some insight to the two following questions:
 \begin{itemize}
 \item From satellite images, can we identify the impact of agronomic practices? 
 \item Is it possible to predict performance of durum wheat based on the NDVI?
 \end{itemize} We will first explain our experience with the Copernicus web service (API) to import sentinel-2 images with the package {\it getSpatialData}. Next, we will show how to compute the NDVI at different dates from sentinel-2 and then plot the correlation between NDVI field data and these computed values. We will give R code associated to different NDVI maps. Finally, we will give our results and conclusions from our analysis on early NDVI and yield values in function of genotype and nitrogen treatment using a bivariate mixed model. This work is the result of a technical challenge submitted to the engineering students of Montpellier SupAgro ";"poster"
"Gavras Konstantin";"Implementing a Classification and Filtering App for Multilingual Facebook Comments ? A Use Case Of Data For Good with R";"What is the biggest challenge in data science? Some say it is messy data, others say company politics, but for us at CorrelAid one of the biggest challenges is its unused potential. Larger companies, universities or governmental organizations can afford professional data scientists, but what about civil society or NPOs? Currently, the resources to generate and use data science insights are highly imbalanced. Using R, we show how to leverage this unused potential by applying a cutting-edge multi-language classification approach.

In collaboration with Minor ? a German organisation offering legal advisory for marginalised groups ?, we present a data for good use case. We use NLP techniques such as multi-language word embeddings (word2vec), unsupervised classification (e1071, caret) and topic modeling (stm) to enable Minor volunteers to better allocate their time. We demonstrate the implementation of an interactive Shiny Dashboard app for classifying and filtering multilingual Facebook comments, including English, Bulgarian and Arabic. The filter mechanisms first identify and filter out comments in the Facebook groups which Minor monitors. The topic model then allocates comments to the respective volunteer at Minor. As a result, volunteers can spend more time on actually helping their clients on Facebook instead of sifting through unstructured comments to find the relevant cases.";"lightning talk"
"Jayanthakumaran Muhunthan";"Analysing Impact of Demonetization on Agricultural Sector in India";"The Indian Government announced demonetization of India's 500 and 1000 Rs notes, which made up 86% of the country's currency in 2016. The intention of this drive was to address tax evasion, eliminate black money and promote a cashless economy. Agriculture sector, which contributes 23% of the GDP and employed around 59% of the Indian workforce, saw severe impact on employees in the industry as well as consumers, which ultimately affected the Indian economy. The majority of farmers depend on cooperative banks for their loans to finance day-to-day activities, which were not functioning following demonetization announcement. Post demonetization, social media platforms such as Twitter was flooded with public opinions on the change. Analyzing social media data it was possible to identify positives and negatives of demonetization implementation. Using R, network diagrams and modelling were applied to identify the word combinations/bi-grams which revealed better understanding of the information contained in the social media datasets. Emotions were a predictive variable when analyzing sentiment through SVM modelling, and after controlling for emotion it was possible to identify key concerns through text based feature engineering. This includes farmers and small traders inability to use digital platform and restricted seed purchase during sowing season because old notes were not accepted.";"poster"
"Singh Mehar";"Building Data Science Infrastructure at enterprise level";"Modern day organizations spend lots of resources in data science R&D and the need for having a dedicated data science team has increased exponentially. Companies are looking at external partners with dedicated competence and experience in this field to assist in building a comprehensive ?one place for all tools'. We were brought in by a world famous biotech company to design, develop and deploy an integrated data science development platform for their team of 100+ data scientists. The ask was to build a comprehensive solution - where users can use either R or Python, develop models and share results through a single platform. One major constraint was to provide a solution which can handle multitude of user sessions, but also provide high performance computing at the same time. Safe option was to build a high availability, load balanced environment, though it is not future proof as users keep on increasing and computing resources need to be optimized accordingly. We decided to take a two-pronged approach, where a kubernetes backed containerized solution will be the primary interface and a load balanced product for backing up additional load. Users launch their own containers and kubernetes takes care of backend resource allocations. They can run both Jupyter notebooks (through jupyterhub) and R scripts (through RStudio Server Pro) in the container and perform multiple assignments concurrently.";"poster"
"Binois Mickaël";"The GPareto and GPGame packages for multi and many objective Bayesian optimization";"The GPareto package provides multi-objective optimization algorithms for expensive black-box functions and an ensemble of dedicated uncertainty quantification methods. Popular methods such as efficient global optimization in the mono-objective case rely on Gaussian processes or Kriging to build surrogate models. Several infill criteria have also been proposed in a multi-objective setup to select new points sequentially and efficiently cope with severely limited evaluation budgets. They are implemented, in addition to Pareto front estimation and uncertainty quantification visualization in the design and objective spaces. Rather than estimating the entire Pareto front, the GPGame package focus on finding equilibrium solutions (e.g., Nash equilibrium) that are better suited for larger number of objectives.";"lightning talk"
"Olteanu Madalina";"Focal distances and distortion coefficients: assessing the individual perception of multiscalar segregation";"The complexity of urban segregation continues to challenge interdisciplinary research, and these last years even more so, with more fine-grained massive data becoming available. We have recently introduced a general method for assessing how inequalities are individually perceived, from any location in the city. By building trajectories which measure each how different the image an individual has in an ever expanding neighbourhood around her location, from that of the whole city, we obtain a multiscalar fingerprint of segregation. Furthermore, we summarize these trajectories by features called focal distances and distortion coefficients. With these, one may easily characterize how distorted the image of the city one has in a given location is, and which is the distance one has to walk from her house to get a correct image of what the city really looks like, similarly to zooming with a camera lens. Detailed maps of segregation may then be drawn, and hotspots of segregation thus isolated. We applied this method on various datasets (social housing distribution in Paris, migrant integration in European capitals, ...), with promising results and new insights on the segregation phenomenon. We illustrate the method and some results with an intuitive R implementation, mainly oriented towards visualisation, and aimed essentially to be used by social scientists for spatial data exploration. ";"poster"
"Lang Michel";"mlr3: A new modular framework for machine learning with R";"The package mlr (Machine Learning with R) was released to CRAN in 2013, and its core design dates back even further. The new mlr3 package is its modular, from-scratch reimplementation in R6. Data is stored primarily as data.tables. mlr3 relies heavily on the reference semantics of R6 and data.table, which enable efficient and elegant programming on the provided machine learning building blocks. The package is geared towards scalability and larger datasets by natively supporting parallelization and out-of-memory data-backends like databases. With a clear object-oriented design, mlr3 focuses on core computational operations, while add-on packages allow seamless integration of extended functionality. For example, mlr3survival implements tasks and learners for survival analysis, and mlr3pipelines extends mlr3 with graphs of (pre)-processing operations, which can be jointly tuned with the mlr3tuning package. Project page: https://mlr3.mlr-org.com";"oral presentation"
"Ledell Erin";"Building and Benchmarking Automatic Machine Learning Systems";"This talk will provide a brief overview of the field of Automatic Machine Learning (AutoML), with a focus on software and benchmarking. The term ""AutoML"" refers to automated methods for model selection and/or hyperparameter optimization and includes such techniques as automated stacking (ensembles), neural architecture search, pipeline optimization and feature engineering. AutoML tools are designed to maximize ease-of-use by simplifying the API. We will discuss the common AutoML software design patterns and take a detailed look at the AutoML algorithm inside of the ""h2o"" R package.

An important part of the development process to evolve and improve an AutoML system is a comprehensive benchmarking strategy. Benchmarking AutoML software across a wide variety of datasets allows the algorithm designer to identify weaknesses in the algorithm and software. This enables tool designers to make incremental, measurable improvements to their system over time. We will present a new open source platform for benchmarking AutoML systems which is part of the OpenML.org ecosystem for reproducible research in machine learning. The system is extensible, so anyone can write a wrapper for their software in order to benchmark it against the most popular open source AutoML systems. We will also present benchmarking results for H2O AutoML against a variety of (mostly Python-based) AutoML systems.";"oral presentation"
"Srinivasan Arun";"Summary of developments in R's data.table package";"data.table is an R package that allows for fast and memory efficient data manipulation with a concise and flexible syntax. It has over 8000 questions on StackOverflow, is frequently a featured package and one of the most downloaded packages (https://www.r-pkg.org), has over 60 contributors (https://github.com/Rdatatable/data.table/graphs/contributors) and is active in development for over 10 years. Over the last couple of years, several new functionalities and improvements, including parallelisation of several operations, have been made including file reading (`fread`), file writing (`fwrite`), subset operations, automatic secondary indexing, ordering, non equi joins, overlapping joins, rolling functionalities etc. In this talk, all such improvements will be summarised showcasing different tasks of varying complexity that can be tackled using data.table with ease and with little code. The intuition behind the syntax of data.table will also be explained. Finally, a quick benchmark of some common tasks will be highlighted to clearly show the performance of data.table. This is particularly useful to R users who might benefit a lot from performance. Main author: Matt Dowle, hacker at H2O.ai
Co-author: Arun Srinivasan (me), Analyst, Millennium Management.
Project page: https://github.com/Rdatatable/data.table/wiki";"oral presentation"
"Fitzjohn Richard";"Describing and solving differential equations with a new domain specific language, odin";"Solving differential equations in R presents a challenge because one must choose between implementations that are either expressive but slow to compute, or cumbersome to write but faster. I present a new package ""odin"" for removing this tradeoff by creating a domain specific language (DSL) hosted in R that compiles a subset of R to C for efficiently expressing and solving differential equations (JavaScript and R are compilation targets under current development). By treating the set of differential equations as a directed acyclic graph, the DSL is declarative rather than imperative. ""odin"" leverages the well established ""deSolve"" package to interface with a number of well understood solvers. I present applications of ""odin"" both in a research context for implementing epidemiological models with 10's of thousands of equations and in teaching contexts where we are using the introspection built into the DSL to automatically generate shiny interfaces.";"oral presentation"
"Le Meur Nolwenn";"queryMed: Linking pharmacological and medical knowledge using semantic Web technologies";"Care trajectory analysis from medical administrative information systems data requires integrating multiple medical data sources (hospital care, drug consumption...) stored in various specialized codifications (ICD10, CIP...), more or less detailed and not always intelligible at first glance by epidemiologist researchers. Tools and methods are needed to facilitate the annotation and the integration such codes to decipher and compare care trajectories. Because medical data are often codified according to international nomenclatures, they can be linked to knowledge representations from medical and pharmacological domains. Linked Data initiatives and Semantic Web technologies have led to the spread of knowledge representations through ontologies, thesauri, taxonomies and nomenclatures. However, the use of these standards, technologies and knowledge representations is still hesitant for non-computer scientists as non-trivial to organize and query for pharmaco-epidemiologist researchers.The R package queryMed (https://github.com/yannrivault/queryMed) provides user-friendly methods to access biomedical knowledge resources from the Linked Data. It proposes methods for the enrichment of health care data to facilitate care trajectory analysis (e.g., pharmaco-surveillance, pharmaco-vigilance), making the most of ontologies or nomenclatures properties.";"lightning talk"
"Lefort Gaëlle";"ASICS: a new R package for identification and quantification of metabolites in complex 1H NMR spectra";"1H Nuclear Magnetic Resonance (NMR) is a high-throughput technology that allows to obtain metabolomic profile from easy-to-obtain fluids at moderate cost. It is a promising tool to detect practically usable biomarkers. However, its interpretation can be hard to make, because metabolites present from the spectrum of a complex mixture are often identified and quantified only from expert knowledge. To facilitate the use of such data, we developed a new R package, ASICS, that implements a method for the automatic identification and quantification of metabolites in 1H NMR spectra. The package combines all the steps of the analysis (management of a reference library, preprocessing, quantification, diagnosis tools to assess the quality of the quantification, post-quantification statistical analyses). 
To assess the performance of ASICS, both the quantification and its impact on a post-quantification differential analysis were evaluated. Correlations between ASICS relative quantifications and biochemical dosages were computed and a similar analysis was performed with other quantification methods.
These comparisons showed that ASICS allows for a faster and simpler direct biological interpretation than the classical bucket approach and obtains more precisely identified and quantified metabolites than other quantification methods. ASICS is released as an R/Bioconductor package.";"poster"
"Cetinkaya-Rundel Mine";"Data Science in a Box";"Data Science in a Box (datasciencebox.org) is an open-source project that aims to equip educators with concrete information on content and infrastructure for designing and painlessly running a semester-long modern introductory data science course with R. In this talk we outline five guiding pedagogical priniples that underlie the choice of topics and concepts introduced in the course as well as their ordering, highlight a sample of examples and assignments that demonstrate how the pedagogy is put into action, introduce `dsbox` -- the companion R package for datasets used in the course, and share sample student work and feedback. We will also walk through a quick start guide for faculty interested in using all or some of these resources in their teaching.";"oral presentation"
"Jaksons Rodelyn";"Spatio-temporal Analysis of Diabrotica Emergence";"Diabrotica or more commonly known as the cucumber beetle or corn rootworm is a species of beetle. It is a major a pest and is known to cause major economic damage to corn growers. Diabrotica was previously only found in the United States and some parts of South America but is believed to have been introduced to Europe during the Yugoslav wars. Due to its potential devastating economic repurcussions it is important to understand the population life cycle of the beetles to ensure adequate controls and measures are in place to minimise impact. Through the use of the Gompertz curve and Bayesian Hierarchical Models in R, we can model the observed dynamics of the beetles emergence to infer when emergence starts and how space, time, and climactic factors affect the dynamics.  ";"lightning talk"
"Mateus William";"Interactive visualisation App for the reading and features identification of the electrocardiogram using R and Arduino Leonardo microcontroller";"The electrocardiogram (ECG) records the electrical signals of the heart by means of electrodes attached to the chest and sometimes the limbs. It's a common test used to detect heart problems and monitor the heart's status. An ECG is a non-invasive, painless test with quick results. Thanks to the technological advances of the 21st century, there are portable medical devices for recording and storage of the ECGs, thereby facilitate study of these. Generally, studies use mathematical and statistics methodologies, which lead to the reduction, identification, and extraction of information. These can contribute significantly to the analysis and decision making by medical personnel. An interactive application was made using Rstudio and its Shiny library, which uses an Arduino microcontroller (Leonardo) to read, store and graph the data from the electrical activity of the examined patient. The App through signal processing tools identifies and illustrates persistent features, commonly associated with the components of the cycles, such as the P wave, the QRS complex and the T wave, these features are useful to analyze times and electrical intensities resulting from the measurement. Therefore, the App reads, stores, graphs and identifies characteristics of the ECG that contribute to the enhancement in the analysis and the decision making by medical personnel.";"lightning talk"
"Jaksons Peter";"Using AI and R to help improve the quality and health of your personalised food basket.";"At Plant and Food Research we constantly aim to improve to quality and health properties of various fruit and vegetables. Genomic prediction is the science that aims to predict certain fruit and vegetable characteristics based on its DNA. For example, can we use the DNA of an apple to predict if it will taste sweet or has a high vitamin C concentration? Ultimately, can we use the DNA to predict if a consumer might say ?this apple is tasty? or ?I like the beneficial health effects?? To tackle this daunting problem of linking a plant's DNA to a consumer's spoken word, we need to know everything about the plant, the consumer, and all the steps in between. In this presentation I will discuss the challenges we have to overcome and how R and AI has helped us with genomic prediction by collecting vital information at higher volumes and accuracy that was not possible before. Recent examples are an app we developed that automates sentiment analysis of consumer data, image recognition tools to monitor plants and deep learning methods to evaluate fruit performance in storage rooms.";"oral presentation"
"?gde Özge";"NETWORK ANALYSIS: DETECTION OF RISK GROUPS WITHIN BANKING SYSTEM OF TURKEY";"According to Banking Law of Turkey, a real person and his spouse and children, the undertakings where they are members of board of directors or general manager or the undertakings which they or a legal per- son control individually or jointly, directly or indirectly or participate with unlimited responsibility, constitute a risk group. Risk groups should have been monitored in order to trace the indirectly credit relationships between groups, firms and real persons. To conduct the study, original reports of banks to the Banking Regulation and Supervision Agency (BRSA) are reviewed; however, due to the existence of incorrectly reported risk groups, a function that cleans the overlapped risk group names and reconstructs data has been developed. Using the data generated by this function, a networked data has been constructed via the R package ?igraph?. Real risk groups are aimed to be detected by network analysis techniques, which are subgroup analysis, brokerage and bridge roles, articulation points and similarity measurements in network. After determination, steps of detection and real risk groups will be presented to BRSA through the channel of R package ?shiny? and R services of business intelligence tools.";"poster"
"Paul Stevenson";"An Approach to Project Workflow for Professional Biostatistical Services";"Large research groups commonly employ a biostatistician to work across their portfolio of research projects, however, this is not feasible for many research active clinicians and ?low-profile/establishing? research groups who often struggle to access biostatistical support. Our group offers ?low-barrier? initial consultations for analysis, data management and database development on a ?per-project? basis that is attractive to the local health research community as it makes biostatistical expertise affordable and easily accessible. To facilitate our workflow, we have developed and refined a template project (skeleton) in R (using the ?ProjectTemplate? package) that, along with version control systems, conforms to the principles of reproducible research. We have also developed R?markdown templates to produce documents following our Institute's style guide. This approach allows us to streamline our workflow to expeditiously initiate projects and produce professional looking reports in multiple formats directly from the analysis package without wasting time on the non-analytical aspects of our projects; this approach is identical and successful for both simple and large-scale projects.";"lightning talk"
"Bhatnagar Sahir";"A flexible approach to time-to-event data analysis using case-base sampling";"In epidemiological studies of time-to-event data, a quantity of interest to the clinician and the patient is the absolute risk of an event given a covariate profile. However, time matching or risk set sampling (including Cox partial likelihood) eliminates the baseline hazard from the likelihood expression for the hazard ratios. This has to be recovered using a non-parametric method which leads to step-wise estimates of the cumulative incidence that are difficult to interpret. The analysis can be further complicated in the presence of competing events. Using case-base sampling, we explain how a single event type or competing risk analysis that directly models the hazard function parametrically can be performed using logistic or multinomial regression. This formulation thus allows us to leverage all the machinery developed for GLMs including goodness-of-fit tests and variable selection. Furthermore, because we are explicitly modeling time, we obtain smooth estimates of the cumulative incidence that are easy to interpret. We demonstrate our approach using data from patients who received stem-cell transplant for acute leukemia. We provide our method in the casebase R package published on CRAN with extensive documentation and examples.";"oral presentation"
"O'hara-Wild Mitchell";"Flexible futures for fable functionality";"The fable ecosystem provides a tidy interface for time series modelling and forecasting, leveraging the data structure from the tsibble package to support a more natural analysis of modern time series. fable is designed to forecast collections of related (possibly multivariate) time series, and to provide tools for working with multiple models. It emphasises density forecasting, whilst continuing to provide a simple user-interface for point forecasting.  Existing implementations of time series models work well in isolation, however it has long-been known that ensembles of forecasts improve forecast accuracy. Hybrid forecasting (separately forecasting components of a time series) is another useful forecasting method. Both ensemble and hybrid forecasts can be expressed as forecast combinations. Recent enhancements to the fable framework now provide a flexible approach to easily combine and evaluate the forecasts from multiple models. The fable package is designed for extensibility, allowing for easier creation of new forecasting models and tools. Without any further implementation, extension models can leverage essential functionality including plotting, accuracy evaluation, model combination and diagnostics. This talk will feature recent developments to the fable framework for combining forecasts, and the performance gain will be evaluated using a set of related time series.";"oral presentation"
"Hyndman Rob";"A feast of time series tools";"Modern time series are often high-dimensional and observed at high frequency, but most existing R packages for time series are designed to handle low-dimensional and low frequency data such as annual, monthly and quarterly data. The feasts package is part of new collection of tidyverts packages designed for modern time series analysis using the tidyverse framework and structures. It uses the tsibble package to provide the basic data class and data manipulation tools. feasts provides Feature Extraction And Statistics for Time Series, and includes tools for exploratory data analysis, data visualization, and data summary. For example, it includes autocorrelation plots, seasonality plots, time series decomposition, tests for units roots and autocorrelations, etc. I will demonstrate the design and use of the feasts package using a variety of real data, highlighting its power for handling large collections of related time series in an efficient and user-friendly manner.  ";"oral presentation"
"Kumar Rajeev Ranjan";"Extreme learning machine model for forecasting drought index";"Drought is a complex hydrologic feature of arid and semiarid regions with strong implications on the sustainability of water resources, agriculture and environmental management. A drought forecasting model is a practical tool for drought-risk management. Drought models are used to forecast drought indices (DIs) that quantify drought by its onset, termination, and subsequent properties such as the severity, duration, and peak intensity in order to monitor and evaluate the impacts of future drought. Some of the well-known indices used in drought studies, monitoring and management are: Palmer Drought Severity Index (PDSI), Standardized Precipitation Index (SPI), Effective Drought Index (EDI), Surface Water Supply Index (SWSI), etc. In this study, Standardized Precipitation Index (SPI) and Effective Drought Index (EDI) are used for the forecasting of drought. An effort have been made to develop R code for converting the weather variable data in to Effective Drought Index. By using the developed R code, precipitation data are converted in to Effective Drought Index. Forecasting of drought index have been performed by using extreme learning machine (ELM). Different performance measure like MAPE, MSE, RMSE etc. are used for evaluation of the performance. The result indicated that ELM based model perform better compare to the ARIMA and ANN models.";"poster"
"Choisy Marc";"rama: an R interface to the GAMA agent-based modeling platform";"Agent-based models (ABM) are discrete-event computer models focusing on the interactions between autonomous entities. The downsides of a high flexibility are that (i) ABM must be defined algorithmically (requiring programming) and (ii) ABM are black-box models, the study of which resort to large numbers of simulations. The GAMA modeling platform offers a user-friendly language (GAML) and environment to develop and efficiently run ABM, potentially in an explicit spatial environment. It lacks, however, the statistical tools to exploit ABM simulations. The rama package is an R interface to GAMA intended to fill this gap. The package defines the S3 class experiment that inherits from the tbl_df class and that is tidyverse compatible. An experiment object is linked to an ABM model defined in a GAML (text) file and contains, for this model, a number of simulations (in rows) that differ by their seed and/or parameters values (in columns). Methods are available to intuitively subset and combine experiment objects, as well as to generate experimental designs for experiment objects. Among them, run_experiment() seamlessly calls the GAMA engine to run the simulations of an experiment object and returns it augmented with a list-column of data frames of time series of the simulations outputs. Simulations outputs can then be analysed in R for model calibration, sensitivity analysis and hypotheses testing.";"lightning talk"
"Trinh Thi Huong";"Assessing impacts of retail supermarkets on the relationship between household food purchase patterns, socioeconomics and demographics Empirical evidence for Vietnam";"Food environments in developing economies are dynamic. Fast paced changes in socio-economic and demographic characteristics of the Vietnamese population, the introduction of retail supermarkets, and a dwindling number of traditional markets will have profound effect on dietary patterns. We re-analyse existing data sources, based on numbers of retail supermarkets (at provincial levels) and household level dietary diversity and dietary quality indices (based on Vietnam Household Living Standard Survey, VHLSS, at household levels), to quantify and compare this relationship between socio-economic characteristics and diets. First, time series clustering analysis is used to classify provinces into three distinct clusters with high (HighSM), low (LowSM) and medium (MediSM) numbers of retail supermarkets. Then, we use Poisson regression and Compositional data analysis (CODA) method to explore the link between socio-economic characteristics and diets, under each supermarket cluster. Results from this re-analysis of accessible administrative data, provides potential intervention opportunities, which will enable ?rewiring? of the local food environment, and address the challenge of double burden of malnutrition in the country. These analysis are based on varous R packages, including TSclust, Sandwich, lmtest, msm, sjPlot, jtools, compositions and robCompositions.";"poster"
"Alimi Eyitayo";"Why you should make Engagement and Retention go side by side.";"Wahoooo! You have built your community, how do you sustain engagement, improve retention and reduce community down time.Some communities have already lost their existence but the Manager still thinks all is well, maybe because people still show up for events or still make comments. When do you know when your community is dead or about dying ? Why don't you have a single Community member be at your side in times of community negativity? How do you detect and revigourate discouraged members? All these and more would be answered as we take a deep dive to help you understand how to ensure your community members are actively participating so it won't stand as a huge impediment on your ROI.";"lightning talk"
"De Valpine Perry";"nCompiler: C++ code-generation from R code";"Many package developers boost performance by coding key steps in C++, using R's C headers and/or Rcpp. The nimble package, predecessor to nCompiler, includes a system for automatic generation of C++ for a core subset of R's math and distribution functions. nimble implements vectorized and recycling-rule operations by code-generating to the Eigen C++ library and automatic differentiation via the CppAD library (in development versions). It includes basic flow control and static data types. However, as a general R programming tool, nimble has design limitations. nCompiler is a new package, designed to be a more general programming tool, with some refactored components from nimble. nCompiler allows definition of classes that mix R and C++ (code-generated or embedded) data and methods as well as pure functions. Much numerical work becomes C++ without coding any C++ by hand. nCompiler plays well with Rcpp and harnesses its compilation tools; harnesses Eigen more deeply, including its Tensor features; supports automatic differentiation via CppAD; and is designed for embedding code in packages, parallelizing, serializing C++ objects, and providing a natural workflow.";"oral presentation"
"Patil Indrajeet";"ggstatsplot: ?ggplot2? Based Plots with Statistical Details. R package version 0.0.9";"ggstatsplot is an extension of ggplot2 package for creating graphics with details from statistical tests included in the plots themselves and targeted primarily at behavioral sciences community to provide a one-line code to produce information-rich plots. In a typical exploratory data analysis workflow, data visualization and statistical modelling are two different phases: visualization informs modelling, and modelling in its turn can suggest a different visualization method, and so on and so forth. The central idea of ggstatsplot is simple: combine these two phases into one in the form of graphics with statistical details, which makes data exploration simpler and faster.";"lightning talk"
"Mckenzie Andy";"A Shiny Web Application To Easily Access Climate Data From An SOS (Sensor Observation Service)";"NIWA (National Institute Of Water and Atmospheric Research) based in NZ have a SOS (Sensor Observation Services) that provides climate and hydrometric data. The sensor and timeseries records from this can be accessed with web requests, though the syntax for this is somewhat arcane. However, the newly updated sos4R package simplifies the syntax, and allows sensor and timeseries records to be accessed with R. Using this package we have developed a Shiny application that allows for the easy selection of climate data by temporal range, site, and location along with associated metadata. We explain what a SOS is, web requests for accessing data, the sos4R package, and how this is all put together in a shiny application.";"oral presentation"
"Talagala Thiyanga";"Feature-based Time Series Forecasting";"This work presents two feature-based forecasting algorithms for large-scale time series forecasting. The algorithms involve computing a range of features of the time series which are then used to select the forecasting model. The forecasting model selection process is carried out using a pre-trained classifier.
In our first algorithm we use a random forest algorithm to train the classifier. We call this framework FFORMS (Feature-based FORecast Model Selection). The second algorithm use efficient Bayesian multivariate surface regression approach to estimate forecast error for each method, and then using the minimum predicted error to select a forecasting model. Both algorithms have been evaluated using time series from the M4 competition, and is shown to yield accurate forecasts comparable to several benchmarks and other commonly used automated approaches in the time series forecasting literature. The methods are made available in the seer and fformpp packages in R.";"oral presentation"
"Quaglieri Anna";"From sequencing to decision making: a suite of R tools to explore RNA-Seq time-course mutation and expression data";"The large amount of bioinformatics results that we produce everyday needs to be efficiently summarised and shared with clinicians and biologists, in order to influence and inform decision making in medical research. When analysing cohorts of relatively small sample sizes (~30 patients) clinicians often need a way to efficiently navigate through a large amount of bioinformatics results for every single patient to investigate mechanisms of response to therapy. Single base mutations, complex insertions and deletions and gene expression data were analysed from the RNA-Seq samples of a cohort of Leukemia patients using several bioinformatics tools, including the R package superFreq (github.com/ChristofferFlensburg/superFreq) and an in-house pipeline developed to call mutations from RNA-Seq (github.com/annaquaglieri16/RNA-seq-variant-calling). The lineplots R package (github.com/annaquaglieri16/lineplots) was created to standardise the mutations outputs. Finally, the Mutexplore R Shiny app was developed to allow researchers and clinicians to explore the large amount of genetic results obtained. The application offers different ways of visualising single patients information and combined views across a cohort of patients. It is available on GitHub (https://github.com/annaquaglieri16/mutexplore) and can be easily adapted or extended to other types of genetic analysis.  ";"oral presentation"
"Talagala Priyanga Dilini";"Anomaly Detection in R";"Anomaly detection problems have many different facets that lead to wide variations in problem formulations. At present, there is a fairly rich variety of R software packages supporting anomaly detection tasks using different analytical techniques. Some of these use an approach to anomaly detection based on a forecast distribution. We locate over 75 R packages with anomaly detection capabilities via a comprehensive online search. We first present a structured and comprehensive discussion on the functionality and capability of these publicly available R packages for anomaly detection. Despite the large number of packages available, there are some anomaly detection challenges that are not supported with existing packages. We reduce this gap by introducing three new R packages for anomaly detection, `oddstream`, `oddwater` and `stray`, with special reference to their capabilities, competitive features and target applications. Package `oddstream` introduces a framework that provides early detection of anomalous behaviours within a large collection of streaming time series. This includes a novel approach that adapts to non-stationarity in the time series. Package `oddwater` provides a framework for early detection of outliers in water-quality data from in situ sensors caused by technical issues. Package `stray` provides a framework to detect anomalies in high dimensional data.";"oral presentation"
"Bengtsson Henrik";"A Future for R: Simplified Parallel and Distributed Processing";"In this talk, I'll present the future framework, how to use it, what is new, and what is on the roadmap. The future ecosystem provides a simple, unified framework for parallel and distributed processing in R. It allows you to ""write parallel code once"" regardless of the operating system and compute environment. At the same time, it allows the user to decide on where and how to parallelize your code. In programming, a 'future' is an abstraction for a value that may be available at some point in the future. The non-blocking nature of futures makes them ideal for asynchronous evaluation of expressions in R, e.g. in parallel on the local machine, on a set of remote machines, via an HPC job scheduler, or in the cloud. At its core, there are two construct: 'f ";"oral presentation"
"Welty Leah";"Connecting R/R Markdown and Microsoft Word using StatTag for Collaborative Reproducibility";"Although R Markdown can render documents to Microsoft Word, R/R Markdown users must sometimes transcribe statistical content in to separate Microsoft Word documents (e.g., documents drafted by colleagues in Word, or documents that must be prepared in Word), a process that is error prone, irreproducible, and inefficient. We will present StatTag (www.stattag.org), an open source, free, and user-friendly program we developed to address this problem. StatTag establishes a bi-directional link between R/R Markdown files and a Word document, and supports a reproducible pipeline even when: (1) statistical results must be included and updated in Word documents that were never generated from Markdown; and (2) text in Word files generated from R/R Markdown has departed substantially from original Markdown content, for example through tracked changes or comments. We will demonstrate how to use StatTag to connect R/R Markdown and Word files so that all files can be edited separately, but statistical content ? values, tables, figures, and verbatim output -- can be updated automatically in Word. Using practical examples, we will also illustrate how to use StatTag to view, edit, and rerun R/R Markdown code directly from Word. ";"oral presentation"
"Romanes Sarah";"multiDA and genDA: Discriminant analysis methods for large scale and complex datasets";"Classification problems involving high dimensional data are extensive in many fields such as finance, marketing, and bioinformatics, with unique challenges with such datasets being numerous and well known. We first introduce the R package multiDA (https://github.com/sarahromanes/multiDA), a new method of performing high dimensional discriminant analysis. Starting from multiclass diagonal discriminant analysis classifiers which avoid the problem of high dimensional covariance estimation we construct a hybrid model that seamlessly integrates feature selection components. We compare our method with several other statistical machine learning packages in R, showing marked improvements in regard to prediction accuracy, interpretability of chosen features, and fast run time. We then introduce genDA (in active development), a DA method capable for use in multi-distributional response data - generalising the capabilities of DA beyond Gaussian response. It utilises Generalised Linear Latent Variable Models (GLLVMs) to capture covariance structure between the different response types and provide an efficient classifier for such datasets. This model leverages the highly efficient TMB package in R for fast and accurate gradient calculations in C++.";"oral presentation"
"Obriant Kelly";"Art of the Feature Toggle: Patterns for maintaining and improving Shiny applications over time";"Creating one-off shiny applications is easy to do, but what happens when you need to maintain an application over a longer period of time? How do you introduce new application features to an active user-base without disrupting their experience? These can be difficult questions to answer for data scientists who are unfamiliar with the basic principles of software and web application deployments and release cycles. I'll introduce a few core concepts used in the software development world: environment-based and application-based release patterns, and talk about how we can map those techniques to simpler Shiny application maintenance. Finally, I'll demo a Shiny application that leverages ?session$user? data to mimic feature toggles for the roll-out of a new feature to a set of users.";"oral presentation"
"Granjon David";"A Shiny powered eLearning platform to teach calcium and phosphate homeostasis";"Disturbance of calcium?phosphate (Ca-Pi) homeostasis negatively affects the structure and function of the bones, kidneys, intestine and blood vessels, and may lead to severe morbidities. While it is crucial that medical students understand the corresponding regulatory mechanisms and their interactions, conveying these in a frontal lecture with hard time constraints is rather challenging. We present two web applications for the teaching and exploration of Ca-Pi homeostasis, building upon on a previously published mathematical model. To ensure efficient computations, the model was first translated to C. In parallel, we developed novel user-interfaces with the web-framework R-Shiny, embedding the compiled model outputs in highly coupled modules. The first application explores the fundamentals of Ca-Pi homeostasis, while the second provides interactive case studies for in-depth exploration of the topic, thereby seeking to foster student engagement and an integrative understanding of Ca-Pi regulation. These applications are hosted on RStudio Connect at http://physiol-seafile.uzh.ch.  In conclusion, we showed that R, C and Shiny can be coupled to emulate a real-life environment, thereby providing a promising starting point for future innovative teaching approaches. This project was a unique opportunity to extend the RShiny framework with shinydashboardPlus and bs4Dash.";"poster"
"Selosse Margot";"ordinalClust: an R package for analyzing ordinal data.";"Ordinal data are a specific kind of categorical data occurring when the levels are ordered. They are used in a lot of domains, specifically when measurements are collected from persons by observations, testings, or questionnaires. ordinalClust is an R package that proposes efficient tools for modeling, clustering, co-clustering, and classification for ordinal data. The data are modeled through the BOS distribution, a gaussian-like distribution parameterized with a position and a precision parameter. On one hand, the co-clustering framework uses the Latent Block Model (LBM) and an SEM-Gibbs algorithm for the parameters inference. On the other hand, the clustering and the classification methods follow on from simplified versions of this algorithm. ";"oral presentation"
"Sontrop Herman";"Creating beautiful Shiny apps with Vue, Vuetify and D3";"Shiny is a great platform for making interactive web applications. Most shiny apps are based on the standard Bootstrap 3 theme. Because of this, most apps look very similar. By now though Bootstrap 3 is quite old and new, more powerful frameworks and design philosophies have emerged, such as Material Design by Google. Vue is one of the hottest, most popular of these newer web frameworks. Vue is an incrementally adoptable UI framework that makes it possible to compose your UI via a set of small web components. Vue also makes it quite easy to create your own set of components, from which other, bigger components can be created, much like Shiny modules. One of the reasons Vue is so popular is that it's also quite easy to learn. Fortunately, it's also quite simple to empower Shiny applications with Vue! A popular Vue implementation of Material Design is Vuetify, a UI component framework with over 80 web components. In this talk we show what Vue and Vuetify are and how they can be combined with Shiny to create awesome looking frontends. We also show several examples of how interactive Vue components can be created using the power of d3.js, a popular visualization library and show how these components can work together. Last, we present a showcase that combines the above libraries in an interactive network viewer, using data from Elastic Search and the neo4j Graph engine, all controlled directly from R!";"oral presentation"
"Moritz Steffen";"Experiences from dealing with missing values in sensor time series data";"Sensors are omnipresent in our modern world. From manufacturing industry, finance, up to biology in nearly every domain sensors provide us with essential information. Although everybody seems to be well-equipped with sensors, operating these can be error prone. Especially missing values are a problem often encountered. Since follow-up analysis and processes often require complete data, missing data replacement ('imputation') is needed as a preprocessing step. In this talk we will give insights about our experiences on handling missing data in several industry related projects. We will also give a short introduction to the imputeTS package, which we developed as a result of these experiences.  Imputation itself is a well-established area in statistical surveys and CRAN offers a surprisingly large choice of packages (i.a. 'mice', 'missMDA', 'Amelia'). However, there are differences between imputation for statistical surveys and sensor time series. We will discuss typical problems arising with missing data imputation in sensor time series and also present for which kind of data / missing data patterns / missing data mechanisms imputation proved to be successful. Overall, this talk is supposed to provide a first glance at sensor data imputation in R along with giving an introduction to the imputeTS package.";"oral presentation"
"Li Angela";"Teaching reproducible spatial analysis in R";"In this talk we will discuss a workshop taught to social scientists and econometricians at the Center for Spatial Data Science at the University of Chicago with little to no background in spatial data or programming. Unlike a conventional spatial statistics or analysis course, the workshop integrated learning to code in R with learning to think spatially. Researchers learned how to explore, manipulate, and visualize spatial data using recently-developed spatial packages in R, while at the same time learning habits for project management and reproducible research. This talk discusses pitfalls and success stories from the workshop, along with considerations when putting together a spatial data curriculum in R. It describes how teaching researchers led to increased programming literacy among researchers, as well as contributions to open source spatial packages. Finally, it puts forth suggestions for a spatial data curriculum that can be shared openly to teach spatial thinking in R worldwide.";"lightning talk"
"Sasso-Schafer Katie";"Photon : Building an electron-shiny app using a simple RStudio add in.";"Have you ever wanted to deliver your model and Shiny application to someone that can't allow their data to leave their computer, or to deliver your Shiny application as a quick prototype that a user could use as a simple Desktop application without deploying over the Internet? Last year at useR! 2018 Katie Sasso showed how to do so by building an Electron-Shiny executable, which allows you to deliver your Shiny app as a Desktop application (https://www.youtube.com/watch?v=ARrbbviGvjc).  This year Abbas Rizvi is going to take it a step further and show a super easy was to use a newly available RStudio Add-In that makes the process super simple and convenient. RStudio Add-Ins allow the IDE to be extended and customized, creating automated and faster work streams. (https://www.rstudio.com/resources/webinars/understanding-add-ins/). After this talk you will be able to create a Windows or Mac Desktop Shiny App with a few simple clicks of the RStudio Add-in.";"lightning talk"
"Campos Gabriela";"Unlocking Brazilian elections - CEPESPData packahe";"Brazilian electoral data can be confusing and hard to work with, both for foreign and Brazilian researchers. The data is provided by Brazilian Electoral Supreme Court (TSE) in multiple datasets separated by year, state and theme. As a consequence, simple questions such as ""How many votes did women have to the house of representatives?"" can take hours of programming to answer. This means lesser transparency on elections. CEPESPData (cepesp.io) was developed by the Center of Politics and Economics of the Public Sector at the Getulio Vargas Foundation (CEPESP-FGV) with support from the São Paulo Research Foundation (FAPESP) and the Brazilian Electoral Supreme Court (TSE) to handle this problem. All databases found in CepespData were originally made available by the TSE. In order to assure data integrity, all procedures to clean and process the data are registered on GitHub (available at https://github.com/Cepesp-Fgv). These procedures, for example, correct repetitions and inconsistencies that exist in TSE's original databases and makes it possible to join mutiple databases. CepespData inovation is the user friendly API entirely developed in R that can be used for mutiple applications through which the data is delivered and the graphical and user friendly interface of cepesp.io that also feature Shinny based apps to help vizualize the data.";"poster"
"Louveaux Marion";"Tools for 3D/4D interactive visualisation of cells and biological tissue";"Modern microscopy techniques allow to image a high number of biological samples (cells, tissues, whole organs and organisms). Some tools even allow to follow a sample over time. Image processing techniques can extract many information from the images about biological objects (number, location in 2D or 3D space and time, biological status, shape...). Analysis of such biological data is facing to two main challenges: the need to take into account the spatial and temporal context of each piece of information, and the need to gather and compare many images pre-processed with multiple softwares from many samples. The R package {cellviz3d} (https://github.com/marionlouveaux/cellviz3d) is a wrapper on the {plotly} package to help visualization of meshes and points structures in 2D, 3D and 3D+time of one or several samples. Data can be extracted directly from images in R or with image analyses softwares like Image/Fiji or MorphoGraphX. Typical use cases that will be presented in this talk are the visualization of the surface of a biological tissue or the visualization of nuclei. Data presented will be data outputs read with packages like {mgx2r}, which bridges MorphoGraphX with R, or {mamut2r}, which bridges MaMut Fiji plugin with R, both available on GitHub (@marionlouveaux).";"lightning talk"
"Wójcik Piotr";"Measuring inequalities from space. Analysis of satellite raster images with R";"Data on night-time light intensity is increasingly used by social science researchers as a proxy for economic development. Calculated from weather satellite recordings, it provides annual data for the whole globe in gridded format with pixels covering less than one square kilometer. This allows researchers to aggregate these data at the level of subnational units and analyze it together with other socio-economic indicators. Satellite images are freely available as large raster files. The aim of this presentation is to show how to analyze these data step by step in R ? starting from importing the data to R, then correctly imposing a map of a selected area (e.g. shapefile) on the raster object, limiting the satellite image to the selected spatial extent and finally aggregating the data for the analyzed territorial units and visualizing the result. In addition, correlation between night-time light intensity and selected socio-economic indicators (e.g. population, GDP) will be analyzed for world countries, US states and UE regions (NUTS2 and NUTS3). R packages: dplyr, sf, raster, ggplot2, leaflet, WDI, eurostat";"lightning talk"
"Goyal Savin";"Machine Learning Infrastructure at Netflix";"At Netflix, our data scientists apply machine learning to an ever-increasing range of business problems, from title popularity predictions to quality of streaming optimizations. However, building and operating production grade machine-learning systems is a highly non-trivial endeavor requiring expertise in two highly evolved disciplines: systems engineering and ML. To empower our data scientists to be able to build, deploy and operate machine learning solutions without any external support, we provide a robust infrastructure for machine learning, ensuring models can be promoted quickly and reliably from prototype to production, and enabling reproducible and easily shareable results. Given the breadth of techniques and the state-of-the-art algorithms available within the R ecosystem, a significant fraction of our data scientists, prefer R as their lingua franca, necessitating the need to provide first-class infrastructure support in R. In this talk, we introduce the techniques and underlying principles driving our approach with a particular focus on models written in R. We talk about Metaflow, our human-centric machine learning infrastructure, which enables our users to iterate on their machine learning pipelines quickly by leveraging existing libraries and familiar abstractions.";"oral presentation"
"Valentin Todorov";"Monitoring robust estimates with dynamic graphics in R";"The forward search for multivariate analysis is an algorithm for avoiding outliers by recursively constructing subsets of good observations. The underlying idea can be extended to many other robust techniques like S- and MM-estimates. Through this approach, monitoring of robust estimates computed over a range of key parameter values, which is a technique advocated in a number of recent articles, the diagnostic tools of choice can be tuned in such a way that highly robust estimators which are as efficient as possible are obtained. These tools are available already for quite some time in the MATLAB FSDA (Flexible Statistics for Data Analysis) toolbox which has evolved into a comprehensive software package for robust statistics. Now the functionality of FSDA is made available for R users through the CRAN package ?fsdaR' which implements an R interface to a MATLAB engine running in the background. Such a technical solution is made possible by the MATLAB Runtime which allows running compiled MATLAB applications on computers that do not have MATLAB installed. A serious challenge is the design and implementation of the interface (the function calls) in a way acceptable for an R user. To illustrate the monitoring of robust estimates for multivariate analysis we start with a simple example and then analyze a real life data set presenting the technological structure of manufactured exports.";"lightning talk"
"Sauder Cécile";"BibliographeR : a set of tools to help your bibliographic research";"The number of scientific articles is constantly increasing. It is sometimes impossible to read all the articles in certain areas. Among this great diversity of articles, some may be more interesting than others. It is difficult to select which articles are essential in a field. The contemporary way to judge the scientific quality of an article is to use the impact factor or the number of citations. However, these parameters may lead to a lack of certain articles that are not very well cited but are very innovative. It is therefore essential to ask the question of what makes an article fundamental in a field. Using the ""fulltext"" package in our Shiny web application we show how the analysis of a bibliography using a network is a good way to visualize the state of the art in a field.  We  searched for different parameters to judge scientific quality using data science approaches. Recent research has shown that the work of small research teams can lead to scientific innovations. In this sense, the analysis of scientific articles by global techniques could play an important role in the discovery of these advances.  ";"lightning talk"
"Sindelar Stepan";"FastRCluster: running FastR from GNU-R";"FastR is an open source alternative R implementation that aims to be compatible with GNU-R, provide significantly faster execution of R code, and high-performance interoperability with other languages including Python. Although FastR can run complex R packages including Rcpp, dplyr or ggplot2, achieving near full compatibility with GNU-R is still work in progress. However, there is already a lot that FastR can offer, but switching completely to FastR may look like a big step. The goal of FastRCluster is to provide the intermediate ground and let the GNU-R users easily try FastR and/or use FastR to run only selected parts of their apps. FastRCluster is an R package, targeted at GNU-R, that provides a seamless way to run FastR inside GNU-R using the existing interface of the parallel package, which also integrates well with the future and promise packages. The talk will, for example, present how FastRCluster can be leveraged in the context of Shiny applications.";"oral presentation"
"Sindelar Stepan";"Mixed interactive debugging of R and native code with FastR and Vistual Studio Code";"Interactive debuggers are one of the most useful tools to aid software development. The two most used approaches for interactive debugging in the R ecosystem, the built-in debugger and R Studio, do not support interactive debugging of both R and C code of R packages at the same time and in one tool. FastR is an open source alternative R implementation that, apart from being compatible with GNU-R, puts emphasis on the performance of R execution and tooling support. FastR is part of the multilingual virtual machine (GraalVM) that, among other things, provides language agnostic support for interactive debugging. One of the other projects built on top of GraalVM is a C/C++ interpreter. FastR can be configured to run the native code of selected R packages using this C/C++ interpreter, which should yield the same behavior, but since both languages are now running in one system, it opens up many exciting possibilities including seamless cross-language debugging. In the talk, we will demonstrate how to configure Visual Studio Code and FastR for cross-language interactive debugging and how to debug a sample R package with native code.";"oral presentation"
"Hocking Toby";"Fast and Optimal Peak Detection in Large Genomic Data via the PeakSegDisk Package";"We describe a new algorithm and R package for peak detection in genomic data sets using constrained optimal changepoint algorithms. These detect changes from background to peak regions by imposing the constraint that the mean should alternately increase then decrease. An existing algorithm for this problem exists, and gives state-of-the-art accuracy results, but it is computationally expensive when the number of changes is large. We propose an algorithm with empirical O(N log N) time complexity that jointly estimates the number of peaks and their locations by globally minimizing a non-convex penalized cost function. We also propose a sequential search algorithm that finds the best solution with K segments in O(N log(N) log(K)) time, which is much faster than the previous O(K N log N) algorithm. Our empirical results show that our disk-based implementation in the PeakSegDisk R package can be used to quickly compute constrained optimal models with many changepoints, which are needed to analyze typical genomic data sets that have tens of millions of observations. ";"oral presentation"
"Schalk Daniel";"compboost: Fast and Flexible Component-Wise Boosting Framework";"In high-dimensional prediction problems, especially in the case where the number of features exceeds the number of observations, feature selection is an essential and often required tool. Component-wise gradient boosting is a modelling technique which provides embedded feature selection for additive statistical models. It allows automatic and unbiased selection of ensemble components from a pool of - often univariate - base learners. Boosting these kinds of models maintains interpretability of effects.  The R package compboost implements component-wise boosting in C++ and Rcpp. It provides a modular object-oriented system which can be extended either in R for convenient prototyping or directly in C++ for optimized speed, the latter at runtime without full recompilation of the framework. The package also allows visual inspection and selection of effects, feature importance, risk behaviour and optimal number of iterations. In contrast to the well-known mboost package, compboost is more suitable for larger datasets and also easier to extend, whereas compboost currently lacks some of the large functionality mboost provides.  Project page: https://github.com/schalkdaniel/compboost";"oral presentation"
"Au Quay";"fxtract - Feature Extraction from Grouped Data";"Researchers and practitioners nowadays have access to large longitudinal datasets. These datasets often need to be aggregated by groups in order to be utilized in statistical analyses or machine learning models, and to enable better understanding by users. Especially for large datasets, this process can be difficult and is often prone to errors.  We present the package ""fxtract"", which provides an easy to use API to facilitate the extraction of user-defined features from grouped data. Similar to the summarize-functionality of the dplyr package, our package helps reducing multiple rows of data into one row for each group. fxtract offers easy manageability of large datasets, since data for each group is only read into memory when needed. The package is written in R6 and parallelization is supported by using the R-Package future.   Project page: https://github.com/QuayAu/fxtract ";"lightning talk"
"Kettunen Henna";"Teaching data analysis with R as a part of an upper secondary school science project";"We taught basics of data analysis with R to a group of 24 mathematically gifted Finnish upper secondary students during a summer school week in June 2018. Our purpose was to pilot data analysis education at upper secondary school level, as the students increasingly need these skills in their post-secondary studies. Data analysis teaching was integrated to a field biology project, in which the students collected and analyzed an empirical data set, and wrote a short report on their findings. The main method used for data analysis was linear regression modeling. A guest lecturer with a data analyst background was responsible for teaching data analysis and the use of R during the course.
 
 The first course day was dedicated to learning basics of data analysis and R. Students were eager to learn R, but reported having found this 8-hour day too intensive. On next day, the students collected an empirical data set on tree height and environmental factors potentially affecting it. They analyzed the collected data in small groups at their own pace over the next few days, and showed motivation towards analyzing their own data. Everybody managed to do the analysis and reporting tasks required for completing the project course. Some students even continued learning independently more advanced data analysis methods afterwards, winning a local science fair competition with a random forest model for tree height.";"poster"
"Yen Chia-Yi";"A new best practice in style? A computational analysis of the dynamics of R style based on 94 million lines of code from all CRAN packages in the past 20 years";"The coexistence of multiple programming styles confuses new users and makes enforcing best practice difficult. This problem is aggravated by the lack of a universally accepted style guide in the R community. To investigate that, we quantified the programming style variation (PSV) in all CRAN packages and observed an emerging consensus in style since 2016, as indicated by the dampened increasing trend in PSV. It seems that a new consensus-based best practice is forming, which is a mixture of various R style guides. Concretely, we summarized the ?ins & outs? of different styles based on popularity across years (e.g., rapid rise of underscore_fun_name and fall of dotted.fun.name since 2013) and pointed out the least agreed style elements (e.g., -> v.s. =, space after a comma). Moreover, we identified a source of PSV (the ?Naughty, Naughty!?) by looking into the style differences between clusters of related packages (e.g., Finance v.s. Biostatistics). Our analysis raises an open question to all stakeholders of the R community, i.e., the R Foundation, opinion leaders, package developers, and ordinary users: should we adopt an official R style guide as in the case of Python's PEP8? The findings from this study validate the R community's effort in reducing PSV and suggest future directions.";"poster"
"Quiroga Riva";"Translating datasets using ""datalang"": the development of ""datos"" package for the R4DS Spanish translation";"Not mastering English language can be a very high entrance barrier for people who want to learn how to program, specially in contexts like Latin America, where learning English is not accessible to everyone due to socioeconomic inequalities.  The aim of this talk is to present the collaborative translation to Spanish of ?R for Data Science? (Whickham & Grolemund, 2017), a project that the Latin American R community is currently leading as a way to shorten this linguistic gap. Unlike other translations, this one not only considers translating the text to Spanish, but also the datasets and part of the code used in the book. After briefly describing the general context of the project, the presentation will focus on how we developed ?datos? (Ruiz, Quiroga, Vargas & Lepore, 2019; https://cienciadedatos.github.io/datos/), the package that contains the R4DS datasets translated. The talk will present the motivations to develop this package, the workflow used for programatically translate the datasets using ?datalang? package (Ruiz, 2018; https://github.com/edgararuiz/datalang), and the challenges we have faced. Opportunities for other non-English speaking R communities will be discussed. ";"oral presentation"
"De Los Santos Hannah";"mwshiny: Connecting Shiny Across Multiple Windows";"We present mwshiny, a package that extends Shiny applications across multiple windows. Shiny lets R users develop interactive applications, alleviating the need for web development languages. Increasingly, users have access to multi-monitor system configurations; further, with Shiny apps hosted online, the possibility of a remote controller driving a visualization-focused monitor provides another necessity for a multi-window environment. Using mwshiny, users set up the interface and functionality of these multi-window applications by using Shiny's familiar syntax and conventions. By elegantly utilizing Shiny's reactive structure, we break down app development into a simple workflow with three parts: user interface development, server computation, and server output. We demonstrate this workflow through three case studies, including the aforementioned multi-monitor and controller-driving situations, in which we focus on population dynamics and cultural awareness, respectively. We also show mwshiny as applied to an immersive healthcare visualization; the Rensselaer Campfire, a new interface designed for group interaction, consists of two connected monitors in the form of a cylindrical fire pit, as well as an outside controller. These case studies show the impact of mwshiny as we move into a future with ever more immersive applications and structures.";"oral presentation"
"Battauz Michela";"Regularized estimation of the nominal response model";"The nominal response model is an Item Response Theory (IRT) for polytomous items model that does not require a predetermined order of the response categories. While providing a very flexible modeling approach, it involves the estimation of many parameters at the risk of numerical instability and overfitting. The lasso is a technique widely used to achieve model selection and regularization. In this talk, we propose the use of a fused lasso penalty to group response categories and perform regularization. An adaptive version of the penalty is also considered. Simulation studies show that the proposal is quite effective in grouping the response categories, thus leading to a more parsimonious model. A remarkable advantage of the procedure is the reduction of both bias and root mean square error in small samples, while no difference is observed in large samples. An application to TIMSS data will illustrate the method. The R package regIRT (available at https://github.com/micbtz/regIRT) implements the methods.  ";"oral presentation"
"Hester Jim";"Real-time file import with the vroom package";"File import in R could be considered a solved problem, with multiple
widely used packages (data.table, readr, and others) providing fast, robust
import of common formats in addition to the functions available in base R. However I feel there is still room for improvement in existing approaches. vroom is able to index and then query multi-Gigabyte files,
including those with categorical, text and temporal data, in near real-time.
This is a huge boon for interactive data analysis as you can jump directly into
exploratory analysis without sampling or long waits for full import. vroom leverages the Altrep framework introduced in R 3.5 along with lazy,
just-in-time parsing of the data to provide this improved latency without
requiring changes to existing data manipulation code. I will throughly explain the techniques used in vroom to ensure good
performance, describe challenges overcome in implementing it, and provide an
interactive demonstration of its capabilities.";"oral presentation"
"Smith David";"A DevOps process for deploying R to production";"So you've built an amazing model in R. It generates great predictions or recommendations on your desktop. Now, how do you get it into production? Deploying R functions within Docker containers, and exposing them with the 'plumber' package, is a simple and effective way to integrate R into applications via a REST API. We can further provide scale for high-volume workloads by deploying that container to Kubernetes. As the complexity grows, however, managing and updating deployments can be challenging. In this talk, we describe a CI/CD based process in Azure DevOps to automate the entire build, test and deploy process for R-based models in production. The process emphasizes model reproducibility, by capturing and tracking changes in the code, data, tests and configurations that define the model. You will learn how, with a check-in to GitHub as the trigger, your model can be automatically retrained, optimized, built, validated, and ? with human approval if necessary ? released. Once deployed to the production environment ? in this talk we'll focus on scalable open-source frameworks like Kubernetes ? the model will be subject to continuous monitoring and performance tracking until such time that a new model version is warranted, and the DevOps lifecycle begins again. ";"oral presentation"
"Dervieux Christophe";"Native Chrome Automation using R";"The Chrome web browser can be fully controlled in headless mode. A headless browser is a convenient tool for web scraping and automated testing of web sites: one can program all the actions performed in a web browser thanks to the chrome devtools protocol. It is also useful for creating a PDF or taking a screenshot of a web page. Headless Chrome automation is widely used in the node.js ecosystem mainly through the Puppeteer library.

In this talk, we will show you how to take the full control of Chrome directly from R without any dependency on another language or tool (node.js or Java are not required). We will be looking at the principles that make it possible and at some working examples like the chromeprint() function in pagedown package. We will also illustrate what is possible with some examples using the in-development crrri package (https://github.com/RLesur/crrri) that offers a low level access to the chrome devtools protocol.";"lightning talk"
"Rubak Ege";"Resample-smoothing of Voronoi intensity estimators";"Voronoi estimators are non-parametric and adaptive estimators of the intensity of a point process. The intensity estimate at a given location is equal to the reciprocal of the size of the Voronoi/Dirichlet cell containing that location. Their major drawback is that they tend to paradoxically under-smooth the data in regions where the point density of the observed point pattern is high, and over-smooth where the point density is low. To remedy this behaviour, we propose to apply an additional smoothing operation to the Voronoi estimator, based on resampling the point pattern by independent random thinning. Through a simulation study we show that our resample-smoothing technique improves the estimation substantially. The proposed intensity estimation scheme is also applied to two datasets: locations of pine saplings (planar point pattern) and motor vehicle traffic accidents (linear network point pattern). Everything is implemented in R and released in the `spatstat` package available on CRAN. The oral presentation will explain the basic concepts, which are very simple, and leave out the mathematical details. Instead, focus is on the relevant objects and classes in the R implementation and how e.g. the Voronoi/Dirichlet tesselation is handled on a linear network such as a road map.";"oral presentation"
"Frick Hannah";"goodpractice - A Tool for Good Package Development";"Building an R package is a great way of encapsulating code, documentation and data, in a single testable and easily distributable unit. Whether you work by yourself or with others, the goal is always to keep your code easily maintainable and bug-free. R CMD check offers a set of checks on the source code of a package to ensure a quality standard required for packages on CRAN. However, it does not cover other aspects of writing good quality software such as code complexity (1) and does not require testing. The goodpractice package leverages several R packages addressing these aspects and, in one place, calculates code coverage (via covr) and cyclomatic complexity (via cyclocomp), runs linters (via lintr), includes all checks from R CMD check (via rcmdcheck) and gives further advice on good practices for R packages, e.g., to include a URL for a bug tracker. The package currently contains 230 checks to help package developers write high quality packages to a common standard. It is both configurable and extensible, so you can use it with your custom set of checks.

(1) TJ McCabe (1976) A Complexity Measure. IEEE Transactions on Software Engineering (SE-2:4).

";"lightning talk"
"Thebault Patricia";"rGSAn: a R package dedicated to the gene set analysis using semantic similarity measures.";"The revolution in high-throughput sequencing technologies, which enables the acquisition of gigabases of DNA sequences, is leading to great applications for understanding the relationships from the genotype to the phenotype. The accumulation of massive amounts of omics data is then providing relevant biological information thanks to the integration of annotation information coming from sources available in the ?omics? domains. These knowledge resources are then essential for interpreting the functional activity of genes, even though, managing the large number of annotation terms associated to a gene set level is a difficult task. To address this issue, we introduce rGSAn, a R package dedicated to the Gene Set Annotation. By adopting a strategy combining semantic similarity measures, and data mining approaches, rGSAn is able to perform an unified and synthetic annotation for a gene set of interest.To do so, the best partition of the annotation terms is computed using semantic similarity measures. Then, the most relevant terms are identified using a decision tree algorithm and an heuristic implementation of the general set cover problem. In addition, rGSAn offers new visualization facilities to interactively explore the annotation results according to the hierarchical structure of Gene Ontology.";"lightning talk"
"Constantinescu Caterina";"Adjusting reviewer scores for a fairer assessment via multi-faceted Rasch modelling";"Selecting submissions for a conference can be viewed as a measurement problem: in principle, organisers aim to accept the ?best' submissions, but the precise manner in which this is achieved can vary considerably. It is also common to involve multiple reviewers in the process, and it may not always be the case that all reviewers manage to rate all submissions. Hence, there is a chance that some particularly harsh reviewers may rate the same submission (and put it at a disadvantage), or some more lenient reviewers may happen to rate the same submission and propel it higher in the ranking. A solution to this issue is offered by multi-faceted Rasch models, which view submission scores as a function of not just the quality of the submission in itself, but also reviewer severity. This allows to adjust submission scores accordingly, providing a fairer measurement process. Conveniently, the R package `TAM` allows to estimate this type of model. In this talk, I will walk you through an example of how `TAM` was used on data collected as part of the review process for a data science conference in Scotland.";"lightning talk"
"Sundqvist Martina";"ClusteRstab - a package for cluster stability in unsupervised learning";"Cluster stability has become a commonly used method for class discovery and class determination in unsupervised classification. It stipulates that a stable clustering reveals the true structure of the data and thus, the number of groups, k, can be selected as the one giving the most stable clustering. Several variants of cluster stability have been developed of which most are based on the same principle: 1) create a large number of sub-datasets from the original dataset, for example by subsambling observations or variables, 2) cluster each obtained sub-dataset into k distinct clusters and 3) apply a measure of cluster comparison from which the average cluster stability is computed. However, cluster stability is based on heuristics, is parameter dependent and do not always allow to select the true number of groups. We therefore propose with the clusteRstab package, a global clustering stability algorithm for which the user can specify and test different clustering algorithms, similarity measures and methods to create sub-datasets. By doing so, the user will be able to get a more global picture of the datasets' structure, and thus be less parameter dependent when selecting k.";"lightning talk"
"Lyu Xiaodan";"Applications of R Shiny to Evaluate and Improve Total Survey Quality";"Maintaining and assessing total survey quality on a large scale and complex survey such as the National Resource Inventory (NRI) often involves iterative human interaction. Tools such as R Shiny that allow graphical display and user interaction turn out to be practical and useful. This paper introduces two web-based applications we developed, viscover and iNtr. viscover is an interactive tool based on Shiny and Leaflet to visualize soil survey data, cropland data layer and their overlay. It helps verify the accuracy of an overlay operation needed to define the covariates for a unit-level small area model. viscover can be easily adapted to visualize other official statistics and facilitate cross-reference of geospatial data from different sources. iNtr is an interactive NRI table review tool that has been helping 2015 NRI make exhaustive comparisons among different estimation runs.";"lightning talk"
"Blondel Emmanuel";"Strengthening of R in support of spatial data infrastructures management: geometa and ows4R packages";"The amount of data to manage is increasing across institutions. Metadata plays a key role to make this data findable, accessible, interoperable and re-usable, and becomes a pillar through legal frames (eg INSPIRE) or with the emergence of data management plans (DMPs). Data managers have thus to deal with these requirements by applying standards to manage (meta)data formats and access protocols. It is especially the case for spatial information which is ruled by ISO/OGC standards. The use of R has been spreading worldwide as preferred tool for data managers and scientists. In this context, some projects were initiated to support metadata handling for specific domains (eg EML), while the capacity to produce standardized ISO/OGC geographic metadata with R was limited. The geometa and ows4R packages aim to fill this gap by providing functions to write and read ISO/OGC metadata, and interfaces to the OGC Web Services. We explain their functioning, including recent features provided with the support of the R Consortium. We present then how they contribute to several national and international information systems in different domains such as fisheries, marine monitoring, ecology and earth observation. Based on these packages, the geoflow initiative as orchestrator for spatial data management in R will be introduced to demonstrate how R can be used for managing spatial data infrastructures.";"oral presentation"
"Luján Criscely";"An S3 approach to the analysis of computer simulations: an illustration with the marine ecosystem model OSMOSE";"Computer simulations are programs that numerically solve a model of a system or a process. The use of simulation tools is widely extended around science. Particularly,
computers simulations and mathematical models with different levels of complexity and sophistication have been developed by ecologists. With the increasing number of R users, there are many packages already implemented to analyze computer simulations in ecology (from running the model to the analysis of model outputs). The aim of this work is to present four limitations of the packages currently developed for the analysis of computer simulations in ecology: i) inclusion of too many functions, with ii) non-standard R naming conventions, to produce iii) non-customizable plots, which are iv) produced as side effects of long calculations. We propose how to make use of the S3 object-oriented approach to analyze computer simulations and avoid these problems, by providing an illustrative example using the marine ecosystem model OSMOSE and its associated R package osmose (https://CRAN.R-project.org/package=osmose). We also provide a reflection for R programmers, software developers and scientists in general; aiming to improve future development of packages and tools in ecology.";"poster"
"Akbaritabar Aliakbar";"Giving cRedit: Acknowledging R packages in scientific publications";"Reward system of science relies heavily on citations. While citation behavior is heavily studied, the debate on how to measure the value of software is still ongoing. Evidence suggests software are rarely cited in publications. There are often no standard rules among authors on how to acknowledge software. However, there are several attempts to construct standard metadata scheme for citations to software, exemplifying the stronger need for rewarding this form of scholarly output. We study how three groups of R packages are cited in scientific publications: 500 most downloaded on CRAN, Specialized packages in CRAN task views and Multi-disciplinary packages used by diverse population of scientists (e.g., statnet, igraph, lme4, nlme and bibliometrix). We query Scopus database and present empirical results of the disciplinary differences in citation behavior when it comes to multi-disciplinary packages (e.g., humanities, social sciences versus statistics and hard sciences). We control whether a publication introducing the package in the Journal of Statistical Software (from 1996 to 2018 including 948 publications) has significant effect on citation rates. We compare also citations with downloads on CRAN as a proxy of package usage and present a network analysis of R packages' position in the CRAN dependency network. We confront these perspectives in our conclusion.";"poster"
"Gonzalez-Arteaga Teresa";"An R package to deal with generalizations of weighted means and OWA operators.";"Weighted means and the ordered weighted averaging (OWA) operators are two families of functions defined through weighting vectors, which are widely used in the field of aggregation operators. Due to their importance, several procedures have appeared in the literature with the aim of generalizing simultaneously both families of functions. Given that weighted means and OWA operators are specific cases of the Choquet integral, the approach followed by most authors is to construct a capacity parametrized by two weighting vectors (one for the weighted mean part and the other for the OWA operator part) so that we can recover the capacity associated with the weighted mean (the OWA operator) when the weighting vector associated with the OWA operator (the weighted mean) has a neutral behavior; that is, is that of the arithmetic mean. In this research we develop an R package called WEMOWA which can compute capacities associated with two families of Choquet integrals that generalize weighted means and OWA operators: SUOWA and Semi-SUOWA operators.";"poster"
"Omerovic Sanela";"flexmixNL: an R package for mixtures of Generalized Nonlinear Models";"Finite mixture models provide a natural statistical framework for dealing with heterogeneity in data. In this context, heterogeneous patterns are attributed to latent classes which yield to the assignment of the data to distinct components under a probability-based clustering method. Generalized Nonlinear Models (GNMs) comprise nonlinear regression in a flexible way by embedding the classical nonlinear regression model within the exponential family distribution. Mixtures of GNMs represent a natural advancement in order to detect distinct components along nonlinear mean functions under flexible distributional assumptions. We introduce the flexmixNL package which performs an efficient fitting framework for mixtures of GNMs in R. The technical implementation of mixtures of GNMs builds on the modular framework of the package flexmix which was developed by Friedrich Leisch and Bettina Grün. We show applications to real world data where mixtures of GNMs prove as an appropriate model class as they face the problem of occurring heterogeneity within typical nonlinear patterns. ";"poster"
"Navarro Ivan";"Comparison of High Performance Techniques for storing and accessing simulated big data for Bayesian inference and designs for survival data.";"The prediction of event times is crucial for planning interim and final analysis in studies with time to event endpoint(s). In this case, metrics from Bayesian statistics such as the predictive power, probability of success/failure and predictive probability of success/failure, help decision makers decide on the future course of any trial. In this context, modelling and simulating survival data is crucial to obtain a high precision in event time predictions during interim stages of a trial. Bayesian inference and decision making can be very expensive computationally. A great number Monte Carlo samples (posterior draws) are needed to make precise and relevant predictions. We present a comparison of high performance computational techniques that may help to deal with storage and accessing of large R object under constrained computational resources.";"poster"
"Chalkis Apostolos";"volesti: Efficient R package for geometrical statistics";"Our new open source package volesti (https://github.com/GeomScale/volume_approximation/tree/develop) can be used for efficient sampling in high dimensions and volume estimation of convex polytopes. All the algorithms in volesti can be used for all the possible representations of a polytope. It is implemented in C++ provided with an R package using Rcpp library. The R package which is currently under submission to CRAN repository: a) outperforms current R packages for volume estimation. It scales to 200-300 dimensions depending on the input polytope. Current R packages (e.g. package geometry) can not scale efficiently for dimensions larger than 15. Other softwares, mainly in matlab, are almost 10 times slower than volesti and contain methods that are also implemented in our package.  b) is the first R package that provides such a variety of implementations of state-of-the-art algorithms and methods in geometrical statistics (three random walk methods implemented for sampling and two state-of-the-art methods for volume estimation). c) provides volume estimations for specific bodies that can be used in finance for crisis prediction and portfolio optimization. volesti is more efficient and complete than all the current software packages for geometrical statistics and it is easy to maintain and for other scientific communities or programmers to contribute.  ";"lightning talk"
"Bryan Jennifer";"DRY out your workflow with the usethis package";"Usethis is one of the packages created in the recent ""conscious uncoupling"" of the devtools package. Devtools is an established package that facilitates various aspects of package development. Never fear: devtools is alive and well and remains the public face of this functionality, but it has recently been split into a handful of more focused packages, under the hood. Usethis now holds functionality related to package and project setup. I'll explain the ""conscious uncoupling"" of devtools and describe the current features of usethis specifically. The DRY concept -- don't repeat yourself -- is well accepted as a best practice for code and it's an equally effective way to approach your development workflow. The usethis package offers functions that enact key steps of the package development process in a programmatic and documented way. This is an attractive alternative to doing everything by hand or, more realistically, copying and modifying files from one of your other packages. Usethis helps with initial setup and also with the sequential addition of features, such as specific dependencies (e.g. Rcpp, the pipe, the tidy eval toolkit) or practices (e.g. version control, testing, continuous integration).";"oral presentation"
"Gillespie Colin";"R and security";"Data science using R is increasing performed in the cloud or over a network. But how secure is this process? In this talk, we won't look at complex hacking but instead, focus on the relatively easy hacks that can be performed to access systems. We'll use three R related examples of how it is possible to access a users system.  In the first example, we'll investigate domain squatting on the Bioconductor website. By registering only thirteen domains, we had the potential to run arbitrary on hundreds of users. In the second example, we'll look at techniques for guessing passwords on RStudio server instances. Lastly, we'll highlight how users can be a little too trusting when running R code from blogs.
    ";"oral presentation"
"Cougnaud Laure";"Patient profile visualization";"In the early phase of a clinical study, time profile visualization of patient clinical endpoints, associated with the adverse events and the treatment exposure is a informational tool for discovery and diagnostic of drug safety signal. The R package: 'patientProfilesVis' automates the creation of such patient profile visualization. A modular approach enables the creation of four different types of visualization: 'line module' for continuous variables (e.g. laboratory measurements), 'interval module' for variables with start/end time point (e.g. treatment exposure, adverse events), 'event module' for fixed time point event (e.g. consecutive laboratory measurements) and 'text module' to display subject-specific information (e.g. demographics, medical history of the patient). For a study in-hand, visualization (plotting object) of each topic of interest can be created via a dedicated plotting function, available for each module type.
The plots are combined across modules by patient and exported into a patient profile report (pdf format), via automated Sweave reports. A Shiny application automates the creation of standard subject profile reports, to leverage the applicability for non-expert R user, as clinicians. The creation of subject profile plotting modules and report and its applicability is demonstrated with a real-life dataset from the PhUSE Working Group.";"poster"
"Meyer Fanny";"A world full of little Addin";"Addins are powerful tools to boost your productivity or simplify your daily workflow in RStudio: initialize a structured project, define cron jobs, generate roxygen tags, modify the colors of a graph, edit spatial data...
Many addins are already available, but it is also very easy to develop your own, according to your needs. 
Through various examples of addins that we have developed, we will explain in what format they can be made available to the user (including in the form of a Shiny application), what the fields of action of an addin can be and how it can interact with global environment and scripts within RStudio. 
Our addins can be used throughout an analysis:
- Initialize an ""RStudio"" Project with {addinit}
- Explore and Visualize Your Data Interactively with {esquisse}
- open data.frame(s) Interactively in Excel with {viewxl}
- {prefixer} to manage your dependencies and add them to a package
- And for more fun, test your geography skills with {where} and pick up amazing color with {colorscale} !";"poster"
"Buridant Caroline";"An unsupervised classification methodology of heterogeneous datasets based on MFA";"In multivariate exploratory analysis, the study of common structures between different datasets measuring the same individuals is a growing issue. This issue is classically tackled by using methods such as Generalized Canonical Analysis (GCA) or Multiple Factor Analysis (MFA). However, it becomes hard, not to say impossible, to interpret the results when a large number of datasets is to be compared. It seems reasonable to go through a preliminary step of datasets clustering.  The purpose of this talk is to present a new methodology for clustering heterogeneous datasets: the distance between datasets is based on the RV coefficient, the linkage criteria between groups of datasets relies on the average representation of the individuals provided by MFA.   The algorithm was implemented in R and applied to epidemiological data, making the clustering of 15 different datasets regarding the same individuals possible; then clusters of datasets were analyzed using MFA";"poster"
"Abedin Md Jaynal";"A shiny app to identify latent research themes in published abstracts over time";"Systematic literature reviews are a widely used approach to accumulate and summarise existing research findings in medical research focusing on clinical trials, but are applicable in many domains. Publication rates are increasing rapidly and creating challenges to conduct complete and efficient systematic reviews. In domains with a large number of publications it is almost impossible to manually review all papers. The classic way of conducting a literature review is to undertake a search using relevant combinations of keywords within electronic databases and retrieve the search results. Reviewers download the corresponding abstracts and perform manual screening which is most time consuming with no guarantee that all relevant abstracts have been included. We have developed a shiny app to analyse collections of abstracts, apply topic modeling and visualise the results to identify latent themes and how they have changed over time. Users will be able to investigate the growth rate of research in an area of interest. This shiny app has the potential to a valuable tool when undertaking a literature review. Using this app user can perform text pre-processing, explore ngarms distribution of terms, tune topic modeling for optimal number of topics, visualize the topic over time and perform network analysis of topics. We will present a case study in elite soccer. ";"poster"
"Csárdi Gábor";"pak: a fresh approach to package installation";"pak is a new package manager that makes package installation fast, safe and convenient. Fast   Fast downloads and HTTP queries. pak performs all HTTP requests concurrently.   Fast installs. pak builds and installs packages concurrently.   Metadata and package cache. pak caches package metadata and all downloaded packages locally. It does not download the same package files over and over again.   Lazy installation. pak only installs the packages that are really necessary for the installation.   Safe   Private library (pak's own package dependencies do not affect your regular package libraries and vice versa).   Every pak operation runs in a sub-process, and the packages are loaded from the private library. pak avoids loading packages from your regular package libraries.   pak warns and requests confirmation for loaded packages.   Dependency solver. pak makes sure that you end up in a consistent, working state of dependencies. If finds conflicts up front, before attempting installation.   Convenient   BioC packages. pak supports Bioconductor packages out of the box. It uses the Bioconductor version that is appropriate for your R version.   GitHub packages. pak supports GitHub packages out of the box. It also supports the Remotes entry in DESCRIPTIONfiles, so that GitHub dependencies of GitHub packages will also get installed.    Package sizes. For CRAN packages pak shows the total sizes of packages it would download.  ";"oral presentation"
"Ozenne Brice";"Analyzing regional brain images using latent variable models with the R packages lava and lavaSearch2";"Latent variable models (LVM) provide a convenient framework to relate multiple outcomes to one or several exposures while accounting for the effect of covariates and risk factors. Traditionally LVM are estimated by maximum likelihood (ML) and statistical inference is performed using Wald tests. Software such as the R package lava can be used to carry out such analysis. When dealing with regional brain data, one faces several challenges: (i) small samples, (ii) multiple comparisons, and (iii) complicated correlation structures. We present an extension to the lava package called lavaSearch2 implementing (i) corrected Wald tests that better control the type 1 error ? similar to the Kenward-Roger correction in mixed models, (ii) a Dunnett adjustment to efficiently handle multiple comparisons, and (iii) a function that automatically detect and correct model misspecifications identified based on the observed data. We illustrate the use of lavaSearch2 on regional SPECT data where we aim at quantifying the neuroinflammatory response to mild traumatic brain injury.

";"lightning talk"
"Perrier Victor";"Our journey with Shiny : some packages to enhance your applications";"Shiny has become a must in the R environment. In any sector of activity, {shiny} is no longer only used for prototypes but also for applications in production, to create reports, dashboards, tools... Shiny offers a basic foundation for application development and has a very rich API that allows developers to write their own extensions. During our journey with shiny we were able to write several packages to include new features and take advantage of its hidden gems. We will therefore present you our ecosystem of packages around Shiny: - Make the interface more attractive with {shinyWidgets} - Build Proxy for htmlwidgets for smooth integration into Shiny, an example with {billboarder} - Minimal busy indicator with {shinybusy} - Monitor app usage with {shinylogs}";"lightning talk"
"Correa Angie";"DESCRIPTIVE AND PREDICTIVE ANALYSIS OF THE PM 2.5 POLLUTANT IN THE ABURRÁ VALLEY (COLOMBIA)";"One of the pollutants with greater presence in urban environments and serious effects on health are fine particles less than 2.5 microns in diameter (PM 2.5). In recent years, warnings have been issued for air pollution in the Metropolitan Area of the Aburrá Valley (Colombia) related to the levels of this pollutant. In response to this, different works have been developed to estimate the PM 2.5 concentration in air quality monitoring stations through methodologies such as: time series, regression and correlation analysis, neural network models, among others. The objective of this paper is to develop a descriptive and predictive analysis for the concentration of the PM 2.5 pollutant in the Aburrá Valley through meteorological and temporal variables. Daily data were extracted from 5 monitoring stations of the Early Warning System of Aburrá Valley (SIATA), and models such as random forests and GAMLSS models were developed, in which it was found that variables such as month, hour, type of station, zone of the station, air temperature, wind speed, wind direction and liquid precipitation influence significantly the concentration of PM 2.5. Finally, a web app was built using the Shiny R package where users can visualize and interact with the data and the developed analysis.";"poster"
"Engels Guyliann";"Better learning of data science in a biology curriculum by using R, RStudio, learnr & Github Classroom";"After switching to modern tools for teaching data science (http://biodatascience-course.sciviews.org) with R, RStudio, learnr & Github ClassRoom, a higher participation rate and better overall results were observed in comparison to the old, traditional biostatistics course. By using richer, interactive learning material, we were able to increase interest, participation and learning of undergraduate students enrolled in a biology curriculum at UMONS, Belgium. The content of the course increased by 40% without increasing the number of in-class hours (75h). Among tools used, a fully configured virtual machine (svbox) with preinstalled R, RStudio, Python, Jupyter, Spyder and 1346 R packages is used both in class and at home. An e-book (bookdown) is used to collect pedagogical material in a centralized place. Twenty interactive tutorials (learnr) are proposed and individual learning progression is recorded (mongodb). Individual or group assessments are provided in Github Classroom. Data wrangling, plots and reproducible workflows are emphasized using tidyverse, and R Markdown. 85% of learnr exercises were finished, and a total of 187 Github repositories were created (37 students). Overall this approach was very successful with 92% success at the exam. We never got such a high success rate so far, with students that are not chiefly motivated to use a computer or to crunch numbers.";"poster"
"Soulanis Konstantinos";"An R implementation of a model-based estimator ? a UK case study";"The UK Office for National Statistics (ONS) uses a model-based conditional ratio estimator to estimate the product by industry sales of products produced by the UK manufacturing sector (the PRODCOM survey) and provided by the UK services sector (the Annual Survey of Goods and Services - ASGS). As ASGS is a recently developed survey by the ONS, R was chosen to be part of the production system over other software packages because of its abilities to handle large datasets (approximately 2000 product by 300 industries by 40000 businesses) and complex calculations efficiently and in a timely manner. R also had its advantages in that the results can be easily visualised as part of the processing. Using the recent set of tidyverse based packages this was able to be done in such a way for the methodology to be implemented concisely. This talk will briefly describe the methodology, the pipeline of functions created and a comparison of the outputs to the design-based Horvitz-Thompson (expansion) estimator. It is the author's hope that this pipeline will be packaged up and be made available for other National Statistics Offices and interested researchers who have surveys which require a similar estimation scheme. ";"lightning talk"
"Grenié Matthias";"Insights in developing an API package ? rromeo";"When developing their package rromeo an R client for the SHERPA/RoMEO API, the authors gain insigts on how to design an API package in R. What tools should be used to ease the development process? The goal of this lightning talk is to share these insights: how to design unit tests for API? How to accomodate encoding issues with old API? How to understand HTTP status codes and warn the user appropriately?
How to then submit your package to rOpenSci?";"lightning talk"
"Chase William";"Use aRt to learn algorithms, math, and R";"I'm bad at math; algorithms look like a foreign language to me; and until recently, I thought of R as a tool just for statistics. All of that changed when I discovered generative art. Almost overnight I went from being afraid of math to dreaming of the Mandelbrot set and reading papers on Wang tiling algorithms. I desperately wanted to make my own art, but I had just become comfortable with R, and the idea of learning Processing or Javascript was daunting. So I barrelled forward with R, transforming my attitude towards coding and what is possible with R. In this talk I will take useRs along my journey from math-phobe to algorithm evangelizer through my ?12 Months of aRt? project (which they can read about on my blog williamrchase.com). I will discuss how the beauty of generative art engaged me more than any math class, and I will inspire useRs to do something fun with R and learn in the process. Along the way, we will learn how R is a fully capable creative coding environment, and how you can leverage a wide breadth of tools such as the tidyverse, spatial libraries, and even Rcpp to turn your creative visions into reality.   ";"lightning talk"
"Kotthaus Helena";"Typical Mistakes in Data Science: Should you Trust my Model?";"Analyzing data with predictive machine learning models is error prone and not all conclusions are valid. Various mistakes can occur in distinct stages of a data science project - during the data preparation process and all the way to the choice of machine learning algorithms and their parameters. Mistakes include, for example, not properly understanding the meaning of missing values before cleaning the data, not properly accounting for bias in data, and even simple programming errors. This can result in invalid or overly optimistic models that are not applicable to real-world problems.
To reveal hidden errors in data analysis, which are frequently hard to detect, we conduct a survey asking questions such as: What are the errors that have affected your work? What are the kinds of errors occurring at different stages of analyses? What are their consequences? How to classify errors? Which strategies do you use to avoid them?";"poster"
"Da Silva Natalia";"Enhancements to the Projection Pursuit Tree Classifier for Heterogeneity and Nonlinear Separation";"This paper presents extensions to the projection pursuit tree (PPtree) algorithm for classification problems to enhance its performance in multi-class problems, and in the presence of nonlinear separations. An interactive web app is also provided to explore the operation of the PPtree classifier and modifications under different scenarios.
The PPtree classifier finds separations between classes on linear combinations of variables by optimizing a projection pursuit index. One of its drawbacks is that a rigid tree structure is generated, the depth of a PPtree object is at most G-1 (where G is the number of classes) with each class forming a single terminal node.
The modifications described here improve the predictive performance in multi-class problems, and in the presence of outliers or asymmetries.
The goal is to make the classifier more flexible, to tackle more complex problems, while maintaining interpretability.
The new algorithms are implemented into an R package, called PPtreeExt, which is available on https://github.com/natydasilva/PPtreeExt.
The interactive web app is usefull because allowed us to identify the main issues of the original algorithm and find better alternatives.";"oral presentation"
"Delignette-Muller Marie Laure";"Dromics: an R tool for modelling omics dose-response data";"Many tools were recently developed to analyze omics data especially in the context of differential analysis, where the purpose is to compare two or more conditions. In ecological risk assessment, the purpose can be different: in order to describe the impact of a contaminant on the response of an organism/community, bioassays are performed with a gradient of exposure concentrations to finally derive an effect concentration (e.g. a benchmark dose) from the concentration-response curve. An efficient use of such data for risk assessment requires the development of specific workflows and of turnkey tools, that should take into account monotonic and non monotonic curves and dose-response designs favoring a great number of tested doses rather than a great number of replicates. In this context we started the development of an R package and a shiny application both named DRomics (already available online in its first version https://lbbe.univ-lyon1.fr/-DRomics-.html) to help ecotoxicologists to automatically select the significant responsive molecular items, model them, derive a benchmark dose for each, classify the responses, and render results in a form usable for ecological risk assessment. In the presentation we will describe the main steps of the proposed workflow applied on a microarray data set of 61535 probes.";"poster"
"Francois Romain";"n() cool #dplyr things";"dplyr, which provides tools for data summary and transformation, is one of the key user-facing packages of the tidyverse. dplyr is so powerful because each function is small and does one thing well. But this can also make it hard to learn, because it's not obvious what you can do, and there are often non-obvious tricks that can make your life much easier. In this talk, I'll summarise() a set of n() cool tricks, arrange(desc(NEWS)) to highlight some of the recent changes, and give you glimpse() of some of our thinking about the future.";"oral presentation"
"Cinar Ozan";"The current status of methods for combining dependent p-values and extending them with a novel package, poolR";"Combining p-values is a useful method to synthesize information from different tests, especially when the data at hand do not lend itself to a conventional meta-analysis. There are several well-known methods for combining p-values which can also be performed in R, for example with metap package. An important issue with the methods for combining p-values is that they assume independence among the p-values to be combined which is known to be violated in some contexts such as genome-wide association studies. Several modifications to current methods have been proposed to generalize them to account for dependence and some of these methods have already been implemented into R with packages like EmpiricalBrownsMethod and CombinePValue. However, these methods, and thus their implementations, address only a portion of the possible problems (e.g., use of only one-sided p-values or only positive correlations) which may even lead to misuse of these methods. In this study, we present the methods for combining independent and dependent p-values and their available R implementations. Then, we outline the points where these tools can be modified and extended to address common problems thoroughly for which we propose our solutions. Finally, we introduce a novel R package, poolR, where we implement our solutions to address the most if not all dependence structures.";"poster"
"Figueroa Luciana";"Newspaper analytics and the dynamics of the exchange rate in Peru";"This poster explores the contribution of macroeconomic news to forecasting the rate of growth of the exchange rate and its volatility in Peru. The data are daily and covers the period of 01/01/2014 - 31/12/2017. Newspaper articles, published online during the times when the central bank does not intervene in the foreign exchange (forex) market, are used to construct ?news indexes? using text analysis.
One group of indexes measure the occurrence and intensity of key macroeconomic words related to the forex market. A second group is built by combining individual words into ""good news"" and ""bad news"" indexes, using text mining techniques. We use linear and nonlinear time-series methods in order to estimate the contribution of macroeconomic news to forecasting exchange rate. Based on the evidence found we conclude that the models used can improve their forecasting accuracy by including these type of news.     ";"poster"
"Topcu Deniz";"Analysis of laboratory test requests in a university hospital: A Shiny App for association analysis as a demand management tool";"Increasing medical expenses throughout the world necessitate more reasonable utilization of all healthcare resources, as well as clinical laboratory tests. Laboratory tests are known to be ordered either to screen individuals for diseases or to confirm the diagnosis of individuals with disease symptoms. Diagnostic tests should be utilized appropriately according to current guidelines and clinical algorithms. Overutilization of tests is considered as incorrect practices in both medical and financial contexts. In this study, to assess utilization of test requests in a hospital environment, we applied apriori based method of association analysis to determine the relationship between different laboratory tests and clinical settings. Using the apriori and shiny package we have development interactive application. Our application is user-friendly and users who are not familiar with R programming or statistical software can import data from Laboratory Information System and oversee test utilization. Managing test requests is an emerging topic in clinical laboratory; data science tools can help laboratory medicine specialists to be more proactive.";"lightning talk"
"Siregar Erika";"EnumeRouteR: An R Shiny Application for Determining the Optimal Sequence of Enumeration Locations in Censuses and Surveys";"One of the most critical problems in censuses and surveys at BPS-Statistics Indonesia is determining the optimal location sequence for field enumerator allocation. Ideally, each enumerator will be assigned the same workloads in terms of distance and time. However, various factors such as the number of household members, respondent's educational level, and distances between houses lead to workload inequity in the completion time. This situation is similar to the Multi-Depot Vehicle Routing Problem (MDVRP) where consumers, vehicles, and depots are analog to respondents, enumerators, and residences. Our goal is to optimize the variance of completion time between enumerators and minimize the total distance traveled. We introduce EnumeRouteR, a Shiny Application that calculates and visualizes the best enumeration routes in the form of a map by utilizing R and Leaflet. R is used to built MDVRP solver and find the best path by executing several iterations of the greedy algorithm. The solver will output the most optimal locations sequence to visit in census and survey. EnumeRouteR can be a breakthrough to improve field enumeration efficiency and increase statistical quality in Indonesia.";"lightning talk"
"Held Verena";"GitHub actions for R";"Continuous integration and delivery (CI/CD) has evolved as a software development best practice, and it also strengthens reproducibility in (data) science. GitHub actions is a new workflow automation feature of the popular code repository host GitHub. It is a convenient service layer on top of the popular container standard docker, and is itself partly open source, thus limiting vendor lock-in. GitHub actions may offer better CI/CD for the R community, but most importantly, it is simple to reason about if things go wrong. The ghactions project presented here offers three avenues to bring GitHub actions to the R community: 1. Developing and curating actions to run R-specific jobs on GitHub, including arbitrary R code or deploying to shinyapps.io.
2. Furnishing users with some out-of-the-box workflows for different kinds of R projects.
3. Documenting experiences and evolving best practices for how to make the most of GitHub actions for R. More information on the ghactions package and project can be found at: http://maxheld.de/ghactions/.";"oral presentation"
"Claeys Emmanuelle";"Dynamic allocation optimization in A/B tests using classification-based preprocessing";"In traditional A/B testing, for instance on two treatments A and B, the objective of the user is to decide which of these two treatments is the best. In order to do that, a randomized test can be used in which each treatment is alternatively chosen and applied to incoming patients for a given time. However, one problem with this approach is the non-adaptivity of the test. For example, if one treatment quickly appears as having a very stronger positive or negative impact than the other one, the test could be stopped earlier.
One way to avoid this is to apply a bandit-based algorithm. A such algorithm is able to automatically decide if one of these two treatments should be chosen and applied more often than the other one. This approach, called dynamic allocation, allows to add adaptivity or early stopping to the A/B test. However, bandit theory by traditional methods requires assumptions which are not always verified in reality. This is mainly due to the fact that the subject tested are not homogeneous. In this work, we will present our R package which includes a new method that finds the best variation for homogenous groups in a short period of time.";"poster"
"Webborn Ellen";"Using fridges to balance the electricity grid";"The electricity grid requires supply and demand to be kept in almost perfect balance at all times. This is done (in part) by generators monitoring the system frequency and adjusting their output accordingly. In future, a large population of appliances such as fridges or air-conditioners could perform this role by letting the system frequency control their temperature set points to delay or advance their switching on and off. In this talk I present the results of simulations of a large group of fridges responding to the system frequency every second for 10-day periods. I use data from the GB Electricity System Operator to capture real-world system conditions over one year. When the population is identical the fridge temperature cycling starts to synchronise causing undesirable power fluctuations. I find that a very small (realistic) amount of parameter heterogeneity prevents these problems, and allows the fridges to provide a valuable service to the grid.";"poster"
"Perepolkin Dmytro";"{polite} - web etiquette for R users";"Data is everywhere, but it does not mean it is freely available. What are best practices and acceptable norms for accessing the data on the web? How does one know when it is OK to scrape the content of a website and how to do it in such a way that it does not create problems for data owner and/or other users? This lightning talk with introduce {polite} package - a collection of functions for safe and responsible web scraping. The three pillars of {polite} are seeking permission, taking slowly and never asking twice.";"lightning talk"
"Bergstedt Jacob";"Using R to predict blood cell composition, age, smoking behaviour and infection serostatus from whole-blood DNA methylation profiles";"Quantification of immune cells provide key indicators of human health but are difficult to assess in patients. As cells differentiate, their genome accumulate epigenetic modifications, i.e., stable chemical additions to the DNA, which give rise to cell-specific gene expression. Some epigenetic marks are expected to be cell lineage specific. They therefore convey information about cell identity. We sought to leverage DNA methylation (DNAm), an epigenetic mark involved in cell differentiation, to predict immune cell proportions in whole blood. Because very few DNAm loci are expected to be markers of cellular differentiation, the optimal predictor is linear and sparse. DNAm at 850K loci and proportions of 70 immune cells were measured by the EPIC array and flow cytometry respectively, on the 1,000 individuals of the Milieu Intérieur cohort. We fitted prediction models of blood cell proportions, as well as age, smoking and infection status, using glmnet and stabs in R. Models predicted 30 cell types and the other traits with high accuracy. They will be valuable for various applications, such as correction for cellular heterogeneity, a critical confounder in medical epigenomic studies.";"poster"
"Spijker Job";"Using a data cube to efficiently manage data for machine learning";"To safeguard a healthy environment the Dutch National Institute for Public Health and the Environment (RIVM) monitors the quality of air, water and soil. RIVM maintains the National Pollutant Release and Transfer Register (which includes carbon emissions) and curates large repositories with (spatial) data about the ecosystem, public health and energy. Data driven approaches are used to predict the fate of pollutants, nutrients and carbon. E.g. random forests is used to predict nitrogen leaching from the root zone or to link the environmental impact of agricultural practice to diets. Combining monitoring data with data of pollutant emissions, or other spatial data, is labour intensive due to the large amount and diversity of available data. To make this process more efficient we use the principle of a ?data cube'. Spatial data is projected into a multidimensional georeferenced feature space and combined with monitoring data. Using a dedicated R package, data is stored in a database and feature matrices and target variables can be selected easily for subsequent analyses with machine learning algorithms. Through two examples we show how this has greatly enhanced the efficiency of our data driven approach.  ";"poster"
"Collier Andrew";"Spatial Optimisation with OSRM and R";"Open Source Routing Machine (OSRM) is a high-performance routing engine for calculating the shortest paths through a road network. These calculations are available via Google Maps. However, queries against a local OSRM server are orders of magnitude faster than Google Maps. The osrm package for R exposes these routing data to a wide range of potential applications. In this talk I'll show how to easily spin up and provision an OSRM server and use it to solve some interesting spatial optimisation problems in R.";"lightning talk"
"Grimm Fiona";"Using Shiny to track winter pressures in the UK National Health Service (NHS)";"The NHS in England is under considerable pressure during winter. Within the context of existing funding pressures, demand for hospital care increasingly exceeds the capacity of emergency departments. In recent years, performance targets have consistently been missed on a national level with potentially worrying consequences for care quality and safety. This trend has also received growing media and political attention. Throughout winter the NHS regularly releases provider-level data on performance indicators, such as A&E waiting times and hospital bed occupancy, which are key to understanding quality of care and to inform future planning efforts. However, partly due to inconvenient formatting of the spreadsheets, it takes considerable analytical skill and effort to routinely produce aggregate metrics, examine trends and assess regional variation. We have developed a Shiny app as an interface for visualisation and comparison of a range of NHS performance indicators over winter, aimed at the public, the media and NHS analysts (to be released before useR). The app also shows historical context and the option to aggregate indicators within local areas. With this we aim to provide a convenient, consistent and consolidated way of tracking NHS winter performance indicators. We also want use it as a case study and learning resource to promote the use of R within the NHS via the NHS-R community.";"lightning talk"
"Robbins Hilary";"What R we doing at the International Agency for Research on Cancer?";"The size and scope of cancer data are rapidly increasing, and increasingly diverse analytical approaches are required to understand and prevent cancer. Here, we illustrate how we use R across the continuum of cancer research through specific examples from our work in the Genetic Epidemiology Group at the International Agency for Research on Cancer (IARC). First, data management is the foundation of successful statistical analysis. We use R to generate clean and harmonized datasets for clinical, socioeconomic, and omics data (packages: lubridate). Second, cancer surveillance includes trend analysis and extrapolation. We estimated cancer incidence in Thailand in 2030 using breakpoint analysis (segmented), poisson age-period-cohort (apc) analysis (Epi), and power5 apc projection (nordpred). Third, two-sample Mendelian randomization uses genetic proxies to infer whether non-genetic risk factors (eg BMI) cause cancer (TwoSampleMR). Fourth, we predicted individual risk of lung cancer based on biomarkers and other risk factors using penalised regressions and ROC curves, which may improve selection of individuals into CT screening (glmnet, pROC). Finally, a promising clinical application is to identify individuals with sufficiently low lung cancer risk to lengthen their CT screening interval (dplyr, ggplot2, survival, geepack, glmnet, rms).";"poster"
"Raillard Nicolas";"Using R and Shiny to access and analyse hindcast database of sea-states";"The IFREMER, French Institute for Research and Exploitation of the Sea develops a hindcast database of sea-state called HOMERE. This database consists in numerous variables (wave height, direction and period, wind and current speed and direction...) describing the main characteristics of a sea-state along the Atlantic coast, from the North of Spain to the South of Ireland and the English Channel. The database covers 23 years with an irregular and high resolution space grid, from 10km offshore to a few hundred meters onshore. This results in more than 100000 points and 1Tb of data, stored in as many individual netcdf files as nodes. 
In addition, IFREMER and BRGM, the French Geological Survey, have been collaborating for a few years to develop statistical methods for extreme value analysis and modelling, particularly for near-shore applications. This ranges from the assessment of marine structure reliability to the evaluation of coasting flooding hazards. This work results in many R scripts, making it difficult to have a consistent workflow between collaborating teams. 
In this talk, we present a web interface using R and Shiny, to provide an easy access to the HOMERE database and to the developed statistical analysis tools for the environmental conditions: directional aspects, joint distributions and modelling of extreme values (return levels, environmental contours...).";"poster"
"Petersen Anne Helby";"Discovering the cause: Tools for structure learning in R";"Enormous amounts of observational data are being produced every day from internet users, health care providers and satellites alike. This opens up a lot of new possibilities for what observational data may be used for. But if the subject is causality, it is still common to solely rely on externally proposed, ?hypothesis-driven? models, which limits the range of causal inquiries. However, in some cases it is possible to construct a causal model from the data using structure learning. This is not only relevant for those interested in inferring causality. Even when prediction is the goal, knowledge of causal structures is useful for helping domain adaption because the mechanistic nature of causal structures make them more stable. A myriad of packages for structure learning have been developed in R, including pcalg, bnstruct, bnlearn, deal, catnet and stablespec, each of them dedicated to a certain class of causal models (e.g. linear), a specific algorithmic approach (e.g. constraint-based), or a combination of both. In this presentation, I provide an overview of existing R packages for structure learning, focusing on overlaps and differences in functionality, interface and possibilities to include external information. I also discuss how the packages may be integrated into a joint tool, thereby facilitating structure learning without settling on a model class or learning approach a priori. ";"poster"
"Bellio Ruggero";"Modern likelihood-frequentist inference with the likelihoodAsy package";"The talk illustrates the R package likelihoodAsy, available on CRAN and implementing some tools for higher-order likelihood inference. The basic functionality of the package is the implementation of the modified directed deviance for inference on a scalar parameter of interest, that could be seen as a fast method to approximate the most accurate parametric bootstrap inferences for the same task. The usage of the package requires to provide code for the likelihood function, for generating a sample from the model, and for the formulation of the interest parameter. The latter is allowed to be a rather general function of the model parameters. The code includes some functions for computing the modified profile likelihood for a multidimensional parameter of interest, and it could also be used to approximate median-unbiased estimation in a parametric statistical model. The features of the package will be illustrated by means of some examples on survival data models, IRT models and mixed models. ";"oral presentation"
"Geniaux Ghislain";"Spatially varying coefficients models: going beyond space-time, spatial autocorellation and non-linearity problems";"The use of GWR frameworks to estimate spatially varying coefficients models is growing in numerous domains and disciplines, espacially in ecology, geography and economy. Geniaux and Martinetti (2017)* proposed a spatial econometrics method that allows to deal simultaneously with spatial heterogeneity and spatial autocorrelation. Their mgwrsar package allows fast computation of locally weighted regression with and without spatial autocorrelation. They also provides experimental general kernel product support for dealing in the same time with main non linear covariates and with space-time or discrete cases. Recently they developped two new methods that allow, for all type of mgwrsar likes models, to dramatically reduce estimation time with limited loss of accuracy by focusing estimation on a selected sample of focal points or using boosting like methods.
This example-driven book is aimed primarily at researchers and graduate students, who wish to perform mgwrsar estimation with spatial and/or spatio-temporal data. *Geniaux, G. and Martinetti, D. (2017). A new method for dealing simultaneously with spatial autocorrelation and spatial heterogeneity in regression models. Regional Science and Urban Economics. (https://doi.org/10.1016/j.regsciurbeco.2017.04.001)";"poster"
"Kamvar Zhian";"Advancing data analytics for field epidemiologists using R: the R4epis innovation project";"Data analysis is integral to informing operational elements of humanitarian medical responses. Field epidemiologists play a central role in informing such responses as they aim to rapidly collect, analyse and disseminate results to support Médecins Sans Frontières (MSF) and partners with timely and targeted intervention strategies. However, a lack of standardised analytical methods within MSF challenges this process. The R4epis project group consists of 18 professionals with expertise in: R programming, field epidemiology, data science, health information systems, geographic information systems, and public health. Between October 2018 and April 2019, R scripts were developed to address all aspects of data cleaning, data analysis, and automatic reporting for outbreaks (measles, meningitis, cholera and acute jaundice) and surveys (retrospective mortality, malnutrition and vaccination coverage). Analyses and outputs were piloted and validated by epidemiologists using historical data. The resulting templates were made available to field epidemiologists for field testing, which was conducted between February and April 2019. R4epis will contribute to the improvement of the quality, timeliness, consistency of data analyses and standardisation of outputs from field epidemiologists during emergency response.";"oral presentation"
"Llobell Fabien";"ClustBlock: a package for clustering datasets";"The clustering of datasets is of paramount interest in multivariate data analysis. In presence of several datasets which pertain to the same individuals but not necessarily the same variables, CLUSTATIS method (Llobell, Cariou, Vigneau, Labenne & Qannari, 2018) operates a cluster analysis of these datasets. This method stands as the core of ClustBlock package. CLUSTATIS strategy consists of a hierarchical algorithm followed by a partitioning algorithm, and yields graphical displays and indices to assess the quality of the solution. A noise cluster option can be activated, with the aim of setting aside atypical datasets. ClustBlock includes specifics functions to perform CLUSTATIS with data from Free Sorting task. An adaptation of CLUSTATIS to the data from a Check All That Apply task, called CLUSCATA (Llobell, Cariou, Vigneau, Labenne & Qannari, 2019), is also available. References Llobell, F., Cariou, V., Vigneau, E., Labenne, A. & Qannari, E. M. (2018). Analysis and clustering of multiblock datasets by means of the STATIS and CLUSTATIS methods. Application to sensometrics. Food Quality and Preference. Llobell, F., Cariou, V., Vigneau, E., Labenne, A., & Qannari, E. M. (2019). A new approach for the analysis of data and the clustering of subjects in a CATA experiment. Food Quality and Preference, 72, 31-39.";"oral presentation"
"Monteil Celine";"Automatic Calibration by Evolutionary Multi Objective Algorithm: the caRamel R package";"Environmental modelling is complex, and models often require the calibration of several parameters that are not directly evaluable from a physical quantity or a field measurement. Multiobjective calibration enables to establish a compromise between these different objectives by defining a set of optimal parameters. The R package caRamel has been designed to easily implement a multi-objective optimizer in the R environment. The algorithm is a hybrid of the Multiobjective Evolutionary Annealing Simplex method (MEAS) and the Nondominated Sorting Genetic Algorithm II (?-NSGA-II algorithm). The optimizer was initially developed for the calibration of hydrological models but can be used for any environmental model. The main function of the package requires to define a function that evaluate the calibration objectives for a set of parameters of the model, and bounds on the parameters to optimize. caRamel is well adapted to a complex model. As an example, caRamel is converging quickly and has a stable solution after 5,000 model evaluations with robust results for a model with 8 parameters and 3 objectives of calibration.";"poster"
"Wittmann Andreas";"Visualizing Huge Amounts of Fleet Data using Shiny and Leaflet";"When using huge amounts of fleet data data scientists like me often use Shiny in combination with Leaflet to further investigate such data and furthermore to present the user a first prototype of a future data product. When using Leaflet, you can quickly reach limits on the amount of data. As a rule of thumb, a maximum of 10,000 data points for visualization seem possible here. The solution is often clustering, but sometimes you want to be able to see all the data. Therefore a tile layer based approach is used for the visualization. With this technique, there is virtually no limit to the amount of data. In this Talk I will show how R can be used with the packages Leaflet and Plumber to accomplish this. As an example record, millions of taxi pickups in NYC are used. The data is here persisted in a NoSQL database like Cassandra.";"lightning talk"
"Irorere Dennis";"AfricaR";"Africa R is a consortium of passionate Africa R user groups and users innovating with R every day and are inspired to share their experience as well as communicate their findings to a global audience. This consortium was birth from the underrepresentation and minority involvement of African population in every role and area of participation, whether as R developers, conference speakers, educators, users, researchers, leaders and package maintainers.    As a community, our mission is to achieve improved representation by encouraging, inspiring, and empowering African population of all genders who are underrepresented in the global R community.    With a primary objective of supporting already existing R Users across Africa and R enthusiasts to embrace the full potential of R programming, through fostering a collaborative continental network of R gurus, mentors, learners, developers and leaders to help facilitate individual and collective progress worldwide.   Africa R talk includes a presentation of our work plan, collaborators, partners and mentors. We will also be using this opportunity to show statistics of members, R user group in our network and launching our website.    #AfricaRusers - Twitter handle.  ";"oral presentation"
"Croissant Yves";"Data frames for grouped data: the gdata.frame package";"The formula-data interface is a critical advantage of R in order to describe models to be estimated. However, more enhanced formula and data are often usefull. The new gdata.frame package tackle the case when the data set is characterized by two indexes. Two examples are panel data, for which observations are defined by an individual and a time period, and random utility models, for which observations are defined by a choice situation and an alternative. Moreover, indexes may have a nesting structure: for example, for a panel data of countries, the individual (country) index can be nested in a continent index. With gdata.frame, the indexes are stored as an attribute of the data.frame and extracted series inherit from it, thanks to specific methods for generic extractor functions. Usual operations, like for example group means or deviations from group means can then be computed in a very natural way, ie without having to define each time the series which defines the structure of the data set. gdata.frame therefore provides a generic solution to deal with grouped data. More specific data structure can then be defined that inherit from it. This feature is illustrated using the plm (for panel data) and the mlogit (for discrete choice models) packages.";"oral presentation"
"Rea Alethea";"Teaching R and statistics to higher degree research students and industry professionals";"The University of Western Australia's Centre for Applied Statistics has for many years offered short courses for higher degree research students and industry-based professionals. In 2019 we have released stage one of our new programme of courses, all of which are based in R using the tidyverse approach. This presentation will describe our experience of moving from courses in base R to a tidyverse approach, highlighting advantages and disadvantages for us as educators and our students as learners. We will also describe our current programme, planned future courses and the pathways for participants based on their statistical background.";"oral presentation"
"Meltzer Dan";"Motivations for migration towards urban centers for men who have sex with men: the search for safety, community, and health";"Introduction If urban settings improve safety, community, and health for men who have sex with men (MSM), urbanicity (a county level measure of how urban an area is) of origins might relate to motive for migration. Methods We used data subset from the Seattle Mobile Study, a 2014 survey on migration and HIV risk among 339 MSM aged 18-59, to examine migration trajectories and motivations. Reasons for move, measured on a scale from 1-7 (1 being most important) included: to avoid discrimination related to sexual orientation, live near like-minded people, be near social support, seek sexual partnering opportunities, or find gay-friendly health care. Urbanicity was determined using census data and 1990 NCHS Urban-Rural Classification Scheme for Counties. Average urbanicity for each participant over pre-Seattle life course was calculated and linearly regressed with mean importance score. All analysis was done using R. Results Increasing ruralness was significantly correlated with increased mean importance score (coef = 0.19, p = 0.01). Motivation importance among MSM moving to Seattle appears higher for those moving from less urban places. Discussion Importance of sexual orientation-related motivations for MSM urban-centric migration was associated with urbanicity of previous locations. Further studies should use larger samples, compare other cities, and assess change over time.  ";"lightning talk"
"Kuo Kevin";"Community Driven Data Science in Insurance";"We introduce Kasa AI, a community driven initiative for open research and software development in insurance and actuarial analytics. Open source software has been credited for recent rapid advances in machine learning and its applications in various industries. The insurance industry, being a heavily regulated industry, has been slower to embrace open source, but recent trends indicate that actuaries are shifting their workflows to adapt to new technologies. We discuss motivations for the community, current projects, which span both life and nonlife insurance, and how tooling in the R ecosystem has enabled reproducible research at scale.";"lightning talk"
"Lyttle Ian";"Vegawidget: Composing and Rendering Interactive Vega(-Lite) Charts";"Vega-Lite, alongside Vega, is a JavaScript implementation of an interactive grammar-of-graphics, developed by the Interactive Data Lab at the University of Washington. You build chart specifications using JSON; Vega(-Lite) renders your specifications as charts in your browser.  The vegawidget package (on CRAN) is an htmlwidgets interface to Vega(-Lite), letting you compose specifications using R lists. The package offers functions to help build chart specifications and to render them as htmlwidgets. It also offers functions to define interactivity between Shiny and Vega(-Lite) via datasets, events, and signals (reactive variables). You can also define interactivity using JavaScript in an R Markdown document.  Although Vega-lite offers an interactive grammar-of-graphics, this package offers a low-level interface for composing chart-specifications. As a result, vegawidget is designed to be extensible, making it easier to develop higher-level, user-friendly packages to build specific types of charts, or even to build a general ggplot2-like framework, using vegawidget as a foundation. Package website: https://vegawidget.github.io/vegawidget";"oral presentation"
"Prunello Marcos";"Teaching Sampling Distributions with Interactive Simulations in R";"One of the most challenging topics to learn and to teach in any basic Stats class is sampling distributions. Students usually experiment difficulties to assimilate the notion of statistics as random variables with their own probability distribution. This work presents the strategy used at the National University of Rosario, Argentina, where we use R to simulate and analyze interactively the distribution of statistics, making use of sliders, pickers and other controls to choose different characteristics for the simulation. The function simularDistrMedia lets us study the sampling distribution of the mean, with controls for the number of simulations, sample size, original distribution of the variable (normal or not) and its population mean and variance. For each selection of parameters plots are updated to show a histogram of the simulated means, a boxplot and a quantile-quantile normal plot. Also, the plots present theoretical and empirical density curves and summary statistics, so that the students can compare and draw conclusions. The functions simularDistrS2 and simularDistrProp let us study the sampling distributions of the variance and proportion in an analogous fashion. This tool is available at GitHub with detailed instructions and code, is currently transformed into a Shiny App and has proved to be useful in the teaching-and-learning process of sampling distributions.";"poster"
"Wang Earo";"Missingness in time: speaking the language of data";"Can missing values methods speak the language of data, to go beyond the probabilistic taxonomy, MCAR, MAR, MNAR? Yes they can! The naniar package provides a suite of exploratory methods and imputation choices for tidy missing data analysis, that fit neatly into a tidy data workflow. However, temporal data is left behind. This talk will present exploratory methods for temporal missing, equipped with a new data-centric taxonomy. The work will be available in the R package called mists.";"poster"
"Nie Yao";"Small Area Estimation (SAE) of All-Cause Mortality and Life Expectancy in British Columbia (BC), Canada, 2000-2017";"Significance Community level health data is critical in enhancing healthcare planning and service deliveries which is often unavailable due to small numbers and the complexity of statistical modelling for obtaining accurate estimations. Goal To develop and validate models for estimating age- and sex-specific all-cause mortality rates and life expectancy at birth for local health areas (LHAs) in BC. Methodology Bayesian Spatially Explicit Mixed-Effects Regression models were applied to BC Vital Statistics Agency death registry to estimate annual all-cause mortality rates by sex and age group for LHAs (population size ranged from 494 to 461,323 in mid-2017). We applied an established empirical validation framework to evaluate model performance under different population sizes and to determine the minimum population size for the model. We used the Geographic Aggregation Tool (GAT) to join neighboring LHAs until the minimum population size was reached. All the analyses were conducted using the Integrated Nested Laplace Approximations (INLA) package in R. Results The SAE model produced stable and valid mortality estimations for a population of 1,000 and more. Geographic patterns in the age- and sex-specific all-cause mortality rates and life expectancy at birth were found. Discussion Variations in mortality in BC can be explained by LHA-level factors.";"poster"
"Rochette Sébastien";"The ""Rmd first"" method: when projects start with the documentation";"Analysis, R programs or packages are easier to use when correctly documented. However, documentation is perceived time consuming and is the poor child of development projects.
We assume that any kind of R project (data analysis, R package, shiny applications) can embrace the literate programming paradigm: explanation of the program logic in natural langage interlaced with code snippets. Hence, Rmarkdown files are ideal candidates for reproducible projects and workflows.
The ""Rmd first"" approach proposes a project in four parts: (1) Prototype: keep track of data explorations, graphs and tables using Rmd files. (2) Package: regularly transform code chunks into functions to simplify and clarify reading. Extracted functions are documented and tested. The Rmd file ends as the vignette of the package. (3) Modularize: separate projects into multiple sub-parts, hence Rmd files. It simplifies participation of multiple developers by reducing potential files conflicts. (4) Deploy: use available tools to share your work, confidentially or publicly. In a vignette documented package, automated report may be built with {bookdown}, a website may present the full documentation with {pkgdown}.
This approach allows developers to explain their process during development. At low cost, documentation is already available, the analysis is directly useable and reproducible, the report already written.";"oral presentation"
"Petzoldt Thomas";"antibioticR: An R package to identify resistant populations in environmental bacteria";"Antibacterial agents have made modern medicine possible. However, the dramatic increase of resistant and multiresistant bacteria is now recognized as a global challenge for human health. Phenotypic resistance can be measured in growth experiments where bacterial isolates are cultivated under drug exposure. This can be done in liquid media on multiwell plates to identify minimum inhibitory concentrations (MIC), or as diffusion test on an agar dish, where the diameter of the inhibition zone (ZD) is recorded. This is repeated with a large number of strains, because environmental populations are composed of different geno- and phenotypes. The MIC or ZD values form multi-modal distribution mixtures.
Package antibioticR (https://github.com/tpetzoldt/antibioticR) implements methods to separate sub-populations from environmental samples and to estimate distribution parameters and quantiles. It provides:

1. Kernel density smoothing to estimate location parameters and an initial guess of variance,
2. A web-based (shiny) implementation of the ECOFFinder algorithm (Turnidge et al, 2006), 
3. Maximum likelihood estimation of multi-modal normal and exponential-normal mixtures.

The package analyzes sensitivity, tolerance and resistance on a sub-acute level to compare populations of different origin. The package contains also visualization tools and interactive web-applications.";"lightning talk"
"Cordano Emanuele";"An R Package for the Distributed Hydrological Model GEOtop";"Eco-hydrological models are increasingly used in the contexts of hydrology, ecology, precision agriculture for better management of water resources and climate change impact studies at various scales: local, hillslope or watershed scale. However, with increasing computing power and observations available, bigger and bigger amounts of raw data are produced. Therefore, the need to develop flexible and user-oriented interfaces to visualize and analyze multiple outputs, e.g. performing sensitivity analyses, comparing and optimizing against observations (for specific research) or extraction of information (for data science), emerges. We present here the R open-source package **geotopbricks** (https://CRAN.R-project.org/package=geotopbricks), which offers an I/0 interface and R visualization and optimization tools for the GEOtop hydrological distributed model (https://www.geotop.org - GNU General Public License v3.0). This package aims to be a link between the work of environmental engineers, who develop hydrological models, and the ones of data and applied scientists, who can extract information from the model results. Applications related to the simulation of water cycle dynamics (model calibration, mapping, data visualization) in some alpine basins are shown.";"oral presentation"
"Beckett Megan";"Exploring the South African School Directory with R";"South Africa has an incredibly diverse, expansive education system, with over 25 000 schools dotted across the country, ranging from dilapidated mud huts and blackboards underneath trees to state-of-the-art, high tech facilities.  As a country, we partake in many assessments and research projects, both local and international, constantly collecting data to inform policy, striving to find ways to improve the education system. I wanted to see how I could add value by taking some seemingly forgotten Excel files from the Department of Basic Education website, and providing a way to explore and appreciate the diversity and complexity of our education system. I'll show how I used OpenStreetMaps to help geocode missing data for school locations (and overcame some unique challenges!), literally helping to put them on the map. Then, I'll show how I used Shiny and Leaflet to create an app which allows anybody, from members of the public to NGO teams to education officials, to visually explore, understand and locate schools across our country, without having to delve into those enormous Excel files. By the end, not only will you get an idea of how easy it is to turn legacy geospatial data into something visual, interactive and informative, but you'll also have a unique perspective into the challenges posed by South Africa's multifarious, multilayered, multicultural and multicoloured education system.  ";"poster"
"Wolf Patrick";"Analyzing Energy-Research in Europe";"Analyzing research collaboration, previous studies most often focused on the study of national innovation systems (Lundvall, 1992). However, in many cases today's research activities are not limited by national borders and numerous research projects are performed in collaboration with partners from different countries, spanning network ties across national borders (Song & Vannetelbosch, 2007). Since we are still lacking a deeper understanding of the European research network, especially in the field of energy research, we seek to shed more light on this blind spot. In order to gain a better understanding of the European research area, we exploit the extensive information of European research projects provided by the CORDIS database of the European Commission. Following clearly structured steps, we first portray the methodological process of data collection and processing including Data-Mining techniques as well as multi-label classification and topic-modelling approaches. Using this information, we afterwards analyze the European energy-research network as well as its competence clusters; and therefore contribute to a deeper understanding of the knowledge distribution and cooperation in Europe.";"poster"
"Richter Jakob";"rt - R Tools for the Command Line";"rt - R tools for the command line - is an R package containing a collection CLI programs to simplify R package development, daily routines and managing the R user library. It runs on Unix-alikes, macOS and Windows. rt packs many basic operations in handy command line instructions, effectively isolating tasks in a separate R process, to not stand in the way of your coding experience. Developing R packages often is a tedious matter involving many repetitive tasks, which mainly are testing and checking. Reducing the effort for those tasks sums up to a more efficient workflow and more available time for coding. rt allows you to test, check, build and spellcheck packages, upload them to the winbuilder and rhub from the command line.  Simple routines like installing packages from CRAN or GitHub, knitting documents, starting shiny apps and updating the package library can be run directly from the shell. For users that maintain R installations on multiple machines, a dotfile configuration allows them to keep the package libraries synchronous.   Project Page: https://github.com/rdatsci/rt";"lightning talk"
"Krainski Elias";"A toolbox for fitting non-separable space-time log-Gaussian Cox models using R-INLA";"Many processes have space-time non-separable dynamics (e.g. disease spread and species distribution) which should be accounted for during modeling. A non-separable stochastic partial differential approach (SPDE) can be used to consider the realistic space-time evolution of the process in which the spatial and temporal autocorrelation in the latent field are linked (Krainski 2018). Observations of these processes are often measured as point-referenced locations in time, i.e. space-time point patterns. The log-Gaussian Cox process model is a popular class to model point patterns. However, it can be difficult fit in practice because the likelihood depends on an integral over the spatial and temporal domains. Implement these models in R-INLA is challenging because it involves several steps. We provide a step-by-step approach to constructing a space-time model in R-INLA using fox rabies as a case study. We discuss several useful updates to the R-INLA package (e.g. inlabru), including improvements to the integration methods in space and time and options to improve computational performance. We also discuss several practical considerations for users to consider in model construction including mesh generation, model implementation, model checking and extensions to the basic model. The goal of this work is to help users avoid common pitfalls when constructing and interpreting these models.";"oral presentation"
"Yang Guang";"gbp: A 4D multiple bin sizes bin packing problem solver";"In the transportation and logistics management, shipping cost is associated with not only the package's three-dimensional physical sizes but also the package's weight. Furthermore, in practice, distribution centers often possess several types of standard bins with different dimensions, rather than a single type of bin. This multiple types of bins extends the problem complexity in the sense that the objective is not only minimizing the number of bins but also maximizing the overall bin utilization rate with such number of bins. In this paper, we proposed a score function based best-fit-first with adaptive recursion approach (SB-BFF-AR) to solve the four-dimensional multiple bin sizes bin packing problem (4D-MBS-BPP). We demonstrate the effectiveness and efficiency of the proposed approach with extensive computational results. In particular, we see that the proposed approach achieves 98% of utilization rate with 1000x faster in time comparing to the global optimum solution. We then show the usefulness of having this solver with a business case where it empowered us developing a machine learning approach for shipping box design.";"oral presentation"
"Henry Lionel";"Reusing tidyverse code, the easy way";"In 2017 the tidyverse grammars were reimplemented on top of tidy evaluation, a metaprogramming framework from the rlang package. Tidy eval makes it possible to program flexibly and robustly with data masking functions from packages like dplyr or ggplot2. However, possible does not mean easy. Tidy eval has the major downside of requiring to learn new programming concepts and tools. In this talk, we'll focus on easier techniques to reuse code from tidyverse pipelines without such a steep learning curve: mapping columns, using fixed column names, passing dots, subsetting the `.data` pronoun, and interpolating expressions. These techniques will help you write functions around tidyverse pipelines and reduce code duplication in your scripts.";"oral presentation"
"Owokotomo Olajumoke Evangelina";"Predicting Risk Groups for Survival of Cancer Patients Using a Robust Metabolomic Signature: The MetabolicSurv R package";"Recent research in oncology is focused on developing robust signature for patient survival using various form of high dimensional omics platforms such as gene expression data, metabolic data and RNAseq, in order to gain a better insight in identifying patients with high-risk for mortality. This method has received a lot of attention in the literature with many authors advocating its usage based on its delineation in discriminating patients. For metabolic data, despite the several research carried out in this area, the software to conduct this type of analysis is not yet available. To address this, a new R package was developed, the MetabolicSurv package (https://cran.r-project.org/web/packages/MetabolicSurv/index.html), aimed at closing the gap by providing a user-friendly data analysis tool both for modelling and visualization. The package incorporates different methods (supervised PCA, supervised PLS, majority voting, Lasso and elastic net) to develop the signature for predicting survival and classifying patients into low and high-risk group for mortality. The package allows integrating patient-specific clinical data and metabolic data in order to develop the signature and therefore allows the usage of the most updated and relevant patient data in developing the signature. We illustrate the usage of the MetabolicSurv package using a study of 149 lung cancer patients.";"poster"
"Guernec Gregory";"OTRECOD: Using Optimal Transport theory in database fusion for recoding heterogeneous variables";"Database merging is a current operation for any user who frequently manipulates data. Nevertheless, when the databases of interest are constructed from heterogeneous sources (by example collected at different places and/or at different times), it is not unusual that different encodings (different scales by example) are used for some shared keys variables, making data fusion often difficult or even impossible.

For resolving this problem, we propose an original recoding approach using optimal transportation theory (G. Monge, 1781). Indeed, this approach gives us an application to map the measure associated with the key variable in the first database to that associated with the same key variable in the other database. This idea was made possible by minimizing the expectation of a cost function using a distance measure in the space of the common covariates and by introducing an allocation function using an adapted k-nearest neighbors algorithm. 


We now propose an R package called OTRECOD to make easily accessible this theory to R users, which gather the algorithm of this original approach in few flexible functions with some interesting options.




    ";"poster"
"Leodolter Maximilian";"runDTW: An algorithm to detect prototypical patterns in long time series";"Subsequence matching and Dynamic Time Warping (DTW) are two well-known research areas in time series analysis [1] . The combination of both concepts in terms of matching time series subsequences via the distance measure DTW is not trivial due to the quadratic runtime complexity of DTW and the typical large number of possible subsequences within one long time series. We propose the algorithm runDTW (to be released with version 1.0.6 of IncDTW on CRAN by end of March 2019) which accelerates the search in a long time series for the k-nearest subsequences of a multivariate time series query pattern by (1) incrementally updating of the normalization and the DTW cost matrix by recycling previous computation results, and (2) by lower bounding and early abandoning to skip and abandon unnecessary computations. We apply runDTW on a database of multivariate accelerometer time series collected via smartphones while travelling with different transport modes. runDTW enables detecting transport mode specific patterns in accelerometer records in acceptable runtime, and provides insight into the transport mode specific data to improve transport mode classification.  [1] Fu, Tak-chung. ""A review on time series data mining."" Engineering Applications of Artificial Intelligence 24.1 (2011): 164-181.";"poster"
"Poinsot Romane";"A Shiny Webapp for nutritional reformulation of food products according to French front-of-pack ?Nutri-Score? label.";"In France, over half of processed food consumed on daily basis is made by food industry. Dietary behaviors play a key role in development of chronic diseases. In order to help people make healthier choices, French government, among others actions, implemented Nutri-Score label, on a voluntary basis. This scoring adapted from Food Standard Agency score ranges from A (green, best score) to E (red, worst score), taking into account contents of 7 favorable and unfavorable common components in food. We developed a web-based Shiny app to help agribusiness professionals calculate and improve the Nutri-Score of their products. A first feature consists in loading user's product nutritional composition table and automatically calculating score for all products. Using ggplot2 and ggiraph, the user visualizes score repartition in all her/his products database, filtering according to her/his own variables. Officer allows user to pick up desired graphs for automatic report. Then, from a product and score target (from A to E) selected by user, all combinations of components (expressed as contents ranges) complying with target are generated. Only the closest combinations to the initial nutritional composition are displayed to user. She/he can also choose varying components. In this way, user gets conceivable solutions to improve nutritional quality of her/him products by saving her/him valuable time.";"lightning talk"
"Baker Laurie";"Outfoxing Rabies: A case study for fitting non-separable space-time log-Gaussian cox models using R-INLA";"Many processes are strongly linked in space and time for which it is natural to model these two characteristics of the process jointly (e.g. disease spread, fish aggregations). However, developing models that can capture spatiotemporal processes remains a challenge, due to a lack of computationally efficient statistical approaches for model fitting. The integrated nested Laplace approximation algorithm (INLA) speeds up computation and provides a flexible framework for fitting latent Gaussian models in a Bayesian context (Rue et al., 2009). Within the INLA framework, the stochastic partial different equation (SPDE) approach constructs flexible fields to model datasets with complex spatial structure (Lindgren et al. 2011). Krainski (2018) has extended the SPDE model to consider the realistic space-time evolution of the process through a non-separable space-time SPDE model in which the spatial and temporal autocorrelation in the latent field are linked. Using fox rabies as a case study, we illustrate how this approach can be used to capture the spatiotemporal nature of disease transmission. We model the fox rabies case data as arising from a log-Gaussian Cox point process and fit the model using R-INLA. In this poster we present the results of this analysis and discuss several practical considerations for users to consider in model construction, implementation, and model checking.";"poster"
"Vanwindekens Frédéric";"Visualisation of open-ended interviews through qualitative coding and cognitive mapping";"Open-ended interviews are common approaches in social sciences for catching the respondents' worldviews, feeling and knowledge. Their popularity is growing in natural science, particularily for studying social-ecological systems like farms, forests or fisheries. In these systems, decision-making processes and practices are no easily taken into account by classical experimental approaches.

We developed an original approach that aims to visualize and analyse perceptions, knowledge and practices of managers in social-ecological systems. Based on the usage of R, we contributed to and developed two Shiny Applications that aims to (i) qualitatively code textual documents, like transcribed open-ended interviews ('qcoder') and to (ii) treat the outputs of the qualitative coding in order to draw cognitive maps ('cogmapr'). Cognitive maps are digraphs of variables that can be used as a semi-qualitative model of interviewees' worldviews. The ?cogmapr' tool contains major functions from our 'Cognitive Mapping Approach for Analysing Actors' Systems of Practices' : drawing Individual Cognitive Maps, computing Social Cognitive Maps and main indicators from the graph theory.

The presentation will be supported by case studies from various agroecological systems : grassland, forest in Belgium and Romania, crop diversification in Europe, soil management in Québec.";"oral presentation"
"Kiener Patrice";"RWsearch: a package for CRAN users and task view maintainers";"Navigating the R Package Universe was intensively discussed at useR! 2017 and is still a hot topic. Ordinary R users expect to find the best packages that match their needs whereas task view maintainers have to identify all packages that fall in the scope of their task views. With the size of CRAN, the task is immense and a dedicated search engine is required. Some new solutions appeared in 2018 and 2019 and will be discussed in this talk.   Among them, the 'RWsearch' package was developed to facilitate the maintenance of the 'Distributions' task view (which is ranked number five by the number of referred packages) but can be used by every R users. It includes a comprehensive set of functions to download the packages and task view master files from CRAN, explore these files, search packages by keywords, date of publication and sophisticated options, display the results as a data.frame or as a classical text in console, txt, html or pdf outputs, download the whole package documentation in one click (a wonderful function for people who need to work offline) and, for task views, compare the results with the list of already referred packages. Since its inception, 'RWsearch' has helped us discover more than 100 new packages.   Along with the CRAN exploration tools, 'RWsearch' has direct links to more than 60 external web search engines. We expect this list to grow with the contribution of everyone.";"oral presentation"
"Goehry Benjamin";"Random forests for time series";"Random forests were introduced in 2001 by Breiman and have since become a popular learning algorithm, for both regression and classification. However, when dealing with time series, random forests do not integrate the time-dependent structure and treat each instant as an independent observation. In this study, we propose the rangerts, an extended version of the ranger package for time series.

Goehry (2018) proved under right hypotheses on parameters and the time series that random forests are consistent. In practice, the idea is to replace the IID bootstrapping with dependent bootstrapping to subsample time series during the tree construction phase to take time dependency into account. 

We tested our package both on numerical simulations and the real world applications on electricity load and show our method improves forests' accuracy in some cases. We discuss also how to make a good choice of the key parameters. References Breiman L. Random forests, Machine Learning, vol. 45, no.1, pp. 5?32, 2001. Goehry B. Random forests for time-dependent processes, preprint, Available: https://hal.archives-ouvertes.fr/hal-01955331, 2018. Wright M.N. & Ziegler A. ranger: A fast implementation of random forests for high dimensional data in C++ and R, J Stat Softw 77:1-17, 2017.";"oral presentation"
"Vaughan Davis";"Simple Arrays";"Within the tidyverse, the core structure that powers many packages is the tibble, a modern reimagining of the data frame. Unfortunately, with the large focus on data frames, the array has been left behind. The rray package is an attempt to change that. By borrowing ideas from tibble, rray hopes to create ?simpler arrays? that are more predictable to use and program around. To accomplish this, rray provides the following new infrastructure:   An rray class, which never drops dimensions while subsetting, and consistently retains dimension names where possible.    Broadcasting semantics, using the xtensor library. rray implements the wildly popular idea of broadcasting, originally found in the Python library, numpy, to allow more intuitive and powerful operations between multiple rray objects. This opens up a much more complete set of operations than is currently possible with base R.    A consistent toolkit for common array manipulation tasks, such as computing sums and products along any axis. Each function retains dimensionality by default, making it easy to link operations together through broadcasting. Importantly, this toolkit works with base R arrays as well as with the new rray objects.  https://davisvaughan.github.io/rray/ https://github.com/DavisVaughan/rray";"oral presentation"
"Svetunkov Ivan";"Smooth forecasting in R";"There are several packages in R that implement forecasting using state space models, and only one that relies on a single source of error state space model (?forecast' package). Unfortunately, the forecasting functions in that package are not flexible enough for different research purposes. For example, exponential smoothing, implemented in ets() function does not allow using explanatory variables, setting the initial values of the states vector and does not allow fitting models to the data with periodicity higher than 24. This motivated the original development of the package back in 2015, with the main aim of making the research in forecasting area ?smooth?. Four years later, the smooth package has a handful of flexible forecasting functions useful for different research purposes, implementing ETS, ARIMA, vector exponential smoothing, simulation functions and more. In this presentation we will discuss the main functions of the package, show their advantages and disadvantages, and show how they can be applied for the solution of the real world forecasting problems and complement ?forecast' and other widely used packages.";"oral presentation"
"Nevado Mauricio";"ATTRIBUTE VALUE EXTRACTION MECHANISM OF CONSTRUCTED WETLANDS INFORMATION DATA";"Regardless of the importance of Constructed Wetlands as pollution control technology, a survey conducted by UNU-FLORES showed a demand for a global platform on CW. UNU-FLORES wants to develop the platform ?CWetlands - the Constructed Wetlands Knowledge Platform (CWKP)? in which data from + 7000 articles are to be integrated. Given the number of articles, data extraction would be time-consuming and error-prone. This study aims at developing a code in R to extract data in a reliable and less time-consuming way. It focuses on the extraction of attributes as: Country name, Wastewater type, CW area, Plant species, Type of CW, Metadata, Average BOD5 inflow/outflow concentration. The process contain 4 sub-processes: 1) .PDF are downloaded and converted to .txt 2) .txt are processed to a) remove disordered strings, stop words, etc and b) divide the text into: Abstract, Introduction, Methods, Conclusions, to allow for more targeted word searches. 3) the data is extracted by either one of the following data mining techniques: 3.1.1) Keyword Match: the tool matches a Text Document Matrix and a dataset of keywords, and 3.1.2) Web Scrap: the data is extracted from the journal web page 4) the values extracted are exported to a database. To develop and test the tool, 13 articles were used. The tool achieved a mean success rate of 79% in 30 minutes; less compared with the 480 minutes needed with a manual approach.";"lightning talk"
"Sax Christoph";"tsbox: Class-Agnostic Time Series";"The R ecosystem knows a vast number of time series classes: ts, xts, zoo, tsibble, tibbletime or timeSeries. The plethora of standards causes confusion. As different packages rely on different classes, it is hard to use them in the same analysis. tsbox provides a set of tools that make it easy to switch between these classes. It also allows the user to treat time series as plain data frames, facilitating the use with tools that assume rectangular data.

The package is built around a set of functions that convert time series of different classes to each other. They are frequency-agnostic, and allow the user to combine multiple non-standard and irregular frequencies. Because coercion works reliably, it is easy to write functions that work identically for all classes. So whether we want to smooth, scale, differentiate, chain-link, forecast, regularize or seasonally adjust a time series, we can use the same tsbox-command for any time series class. The talk provides a general overview of time series classes in R and shows how tsbox can be used to facilitate the interaction between them.";"oral presentation"
"Keydana Sigrid";"Getting started with TensorFlow Probability from R";"TensorFlow Probability is a library built on top of TensorFlow offering a vast range of functionality ranging from distributions over probabilistic network layers to probabilistic inference. Major components include (1) probability distributions, trainable distributions and bijectors, (2) probabilistic network layers, and (3) probabilistic inference (based on MCMC or variational inference). All components work seemlessly with core TensorFlow and TensorFlow Keras, and may run distributed and on GPU. In this talk, we show how to get started using TensorFlow Probability from R, and how to integrate its functionality with deep learning applications.  ";"poster"
"Vieilledent Ghislain";"Using Rcpp* packages for easy and fast Gibbs sampling MCMC from within R";"Hierarchical Bayesian models are increasingly used for applied statistics. Parameters of such models can be estimated through Gibbs sampling Markov chain Monte Carlo using a variety of algorithms (conjugated priors, Metropolis-Hastings, Hamiltonian Monte Carlo). These algorithms approximate the parameter posterior distributions through iterative simulations and are computationally intensive. Using C/C++ language to code such algorithms make computations faster. In our presentation, we will show how Rcpp* R packages (Rcpp, RcppGSL and RcppArmadillo), can be easily used to (i) call Gibbs samplers written in C++ from within R, (ii) reduce computation time through efficient random draws, and (iii) facilitate vector and matrix operations. We will illustrate this approach with the new jSDM R package for fitting joint species distribution models.";"oral presentation"
"Ko Myungji";"Forecasting and Visualizing Churn data - R-shiny";"Customer churn occurs when a customer does not like the service of an existing institution or feels that the service of another institution is better. From the agency's perspective, it is more economical to retain existing customers than to attract other customers, so forecasting customer churn is more important than ever. In this study, we conducted visualization to identify the characteristics of variables of mobile carrier termination data and made some statistical models using R. Supervised learning methods such as logistic regression, random forest, SVM and DNN were used. Using data from Kaggle, 7043 customer information with 21 features each, the results of the study show the highest accuracy of 79% in SVM when predicting churn rate and the most prominent variable for high churn rate was ""the use of Online Security"". We also developed a web-based platform with RShiny package that visualize the patterns of customers who would churn with have high probability. It also shows the probability of churn when a new customer's information entered. Through our research, telecom organizations can be aware of customers who are likely to churn and develop marketing strategies to keep these customers engaged.";"poster"
"Fras Alicja";"Optimizing children sleeping time using regression and machine learning";"Sleeping child is an everyday dream come true of every young parent. Unwilling to fall asleep child can change one's life into a nightmare and those, who calmly sleep long hours used to be the object of desire. There are many functions, that the parents try to optimize - some simply want their children to sleep as much as possible, some want to wake up later and enjoy their kids in the evenings or the opposite (you cannot have it all), in our case we also wanted to synchronize sleeping times of our two children. To reach the goal, they try to find some patterns in children`s behavior ? they impose some daily routines, do not let children nap too long during the daytime or try to put them in bed earlier, hoping that slower pace will lull them. However intuitions may be misleading, and that is why after a few months of observing my children sleeping patterns I decided to hire statistical tools. I wanted to verify the hypothesis if we should wake them up earlier in the morning (which in case of our kids turned out to be true), but the framework allowed me to detect also some more useful patterns and draw hints. After first attempts with linear regression, I decided to introduce my kids to the neural network and let it learn. My further idea is to create a web app, where one can upload a data file with observed sleeping times and habits, which would help draw reliable conclusions. ";"lightning talk"
"Grueber Friedrich-Claus";"Big data analysis for power plant operational data for optimizing engineering design in R / Shiny";"A typical gas fired power plant is equipped with sensors and instrumentation devices to capture key operating parameters of power plants. With a large fleet of power plants operating around the globe over >= 10 years , the volume of sensor data being captured enters the realm of big data and provides an opportunity to apply big data analytic techniques to mine valuable information for optimizing engineering design of power plants.

The often used ""worst case approach"" usually leads to large design margins and increased cost. With the advent of big data technology, low cost computation and memory, it is now possible to mine the sensor measurements to either validate or dispute worst case assumptions made during design. Therefore a core processing R kernel was developed performing statistical and data analysis on time series data like:   KDEs, pdfs, correlations,Time series   probability estimations in regions    conditional probability analysis   violin/box plots   Event Detection    An Shiny application has been developed integrating the core processing kernel and providing data analytic functionality available for design engineers like automatic :
- shortlisting/specification of senor names
- generatiion of fitting list of plants & units of our fleet
- download and post processing of time series data 
- GUI for data analytic techniques/visualations
";"oral presentation"
"Devailly Guillaume";"Toulouse R User Group";"Toulouse R User Group (or R-Toulouse) started its activities with a first meeting in September 2018. We organised 1-hour long events every months or two months, where two speakers introduced us to various topics, from introductions to the tidyverse and data.table to Bayesian statistics and symbolic differentiation, from shiny to metagenomic visualisations. The slides are made publicly available online shortly after the presentations. Meetings are free, without registration, and open to all, student, academic and non-academic professionals, hobbyists. A code of conduct is in place to ensure safe, harassment-free meetings. Events have been generously hosted at Institut Mathématique de Toulouse and INSA Toulouse. We encourage speakers to present in French, as many good English resources are already available, but English content is welcome too. Events are advertised on the R-Toulouse website (r-toulouse.netlify.com) build with blogdown, through our twitter account (@RUG_Toulouse), and a mailing list (r-toulouse@groupes.renater.fr). This poster will advertised for this local user group, present its activities, and explained the tools and processes used to make it lively.";"poster"
"Schmutz Amandine";"funHDDC, a R package to cluster univariate and multivariate functional data";"The emergence of numerical sensors in many aspects of everyday life leads to the collection of high frequency data. For example in sports, athletes wear devices
that collect simultaneously several variables during their training to follow their physical constants. This kind of data can be classified as multivariate functional data. To
ease the understanding of those data, there is an increasing need of methods to analyze multivariate functional data. This work presents an R package that provides a clustering technique (Schmutz et al, 2018) that eases the modeling and comprehension of those multivariate functional data. This method, named funHDDC, is based on a functional latent mixture model which allows the partition of data into homogeneous clusters, and an EM algorithm for model inference. In addition to clustering algorithm, the package provides model selection criteria for choosing the number of clusters, and allows the execution of principal component analysis for multivariate functional data. The package usage will be shown on several practical examples whose an original example of horse speed prediction.";"oral presentation"
"Devailly Guillaume";"PEREpigenomics: a shiny app to visualize Roadmap Epigenomics data";"The Roadmap Epigenomics consortium has gathered and produced abundant epigenomic data in human cell lines and tissues to build reference epigenetic maps. The data is freely available, and is exploitable through web browsers. It has been used by the consortium to classify chromatin into ?states' that can be more easily compared across cell type. Here we systematically assessed the links between epigenetic marks and transcription by generating ?stacked profiles' of epigenetic marks at transcription start sites (TSS), transcription end sites (TES) and middle exons, sorted according to expression levels or exon inclusion ratio, for each cell type and tissue in the dataset. The thousands of attractive visualisations generated are made easily browsable through a web application, www.perepigenomics.roslin.ed.ac.uk. We also investigated how change of epigenetic marks across cells were correlated to change of expression level / inclusion ratio, allowing us to build a comprehensive understanding of the associations between epigenetic marks at TSS, TES and middle exons and expression levels or exon inclusion ratio.";"oral presentation"
"Canonici Merle Elie";"Typing R";"For a long time now, programming langages have been divided in two categories, dynamically typed ones and statically typed ones.Both sides tend to argue that their system has more inherent benefits than drawbacks according to their needs. On one hand it is convenient to have a system that allows writing non well typed programs that you know to be correct. On the other hand, no programmer is ever safe from a silly mistake taking ages to be fixed, if ever detected, in his code base. Thus, with the ever growing usage of dynamically typed langages such as R or javascript, it has become increasingly important to detect mistakes as early as possible in the deveopment process. By adapting some approaches inherited from the strongly statically typed langages community we have developped a typing system for a fragment of the R programming language. We argue that it does not restrict the expresiveness of the R language beyond what is actually widely used. Moreover we have embedded a type checker in a state of the art integrated development environment leveraging the graphical interface to report useful errors to the user.";"oral presentation"
"Mørk Kristoffer";"Estimate, Estimator, Estimand?";"Over the past few years, the term ?Estimand' has become a buzzword in the pharmaceutical industry. The ICH E9 addendum[1] introduces the latest theory of estimands. An estimand can be seen as a description of what has to be estimated in order to answer a scientific question of interest in a clinical trial. Hence, an estimand forms the basis for an estimator and the corresponding estimate. Several estimators can be defined for an estimand with one being the primary estimator and the others being sensitivity estimators. We have created a shiny app to illustrate the concept of estimands. The shiny app considers a clinical trial within type 1 diabetes mellitus with a parallel group design and repeated measurements of a continuous variable. Some of the data is missing due to withdrawals and eventually due to the defined estimand. The shiny app handles the missing data problem by using some of the multiple imputation methods implemented in the R package MICE. Some extra functionalities have been developed in order to ease the use of the MICE package in a clinical trial setup. The theory of estimands may also be useful in other settings than clinical trials. 
  [1] https://www.ich.org/fileadmin/Public_Web_Site/ICH_Products/Guidelines/Efficacy/E9/E9-R1EWG_Step2_Guideline_2017_0616.pdf  ";"poster"
"Crouchley Robert";"Algorithmic Differentiation in R using the RcppEigenAD package";"Algorithmic Differentiation (AD) has been available as a programming tool for statisticians for a number of decades. However, its adoption as an alternative to symbolic and numeric methods does not seem to be very common. One possible reason for this is the difficulty that is typically encountered when attempting to integrate AD functionality into existing statistical computing environments. The RcppEigenAD package attempts to mitigate these difficulties when employing AD within R by combining the facilities of the Rcpp package for extending R with C++, the AD library cppAD, and the Eigen linear algebra library, into a single R package. The resulting package, (RcppEigenAD), allows the user to define matrix valued functions of matrix arguments in c++ and seamlessly integrate them into an R session in a way that allows not only for computing the function but also their Jacobian and Hessian. The package also includes an implementation of Faa di Bruno's formula for calculating the partial derivatives of the composition of functions defined by the user. The package has application in the areas of optimisation, sensitivity analysis and calculating covariances via the delta method, which are illustrated with examples.";"oral presentation"
"Soldánová Veronika";"Web GIS application of water reservoirs around the town of Banská ?tiavnica";"On their poster the authors will demonstrate a geographical information system for the system of historical water reservoirs in the surroundings of Banská ?tiavnica (a town in Slovakia). The application is built on the Shiny and Leaflet R packages, and the visual aspect was fine-tuned with the CSS mark-up language. The construction of this water management system began in the 16th century; in the 18th and 19th centuries its dams were among the highest in Europe. Today, it is a cultural heritage site registered in the UNESCO list. It is an important landscaping element and a recreational structure. Despite its historical and present importance, there is no public source of information that provides the available data on this system in a comprehensive way. The application created by the authors provides any interested person with a user-friendly visualisation and coherent view of the water management system of Banská ?tiavnica, along with its history, its present, its technical parameters and the level of quality of its recreational use. Moreover, there is an option to add one´s own data to this GIS system, which is used for the creation of emotional maps.   ";"poster"
"Smith Joanet";"Calibrating the fundamentals of a Multinomial Logistic System to fit a South African public transport system.";"Calibrating the fundamentals of a Multinomial Logistic System to fit a South African public transport system, through data mining and choice modelling in R. This study enables the analysis of millions of datasets within seconds and shows to power of using R as a data mining and analysing tool. This study presents the analysis of the choice made for the selection of specific Integrated Rapid Transport (IRT) bus stations by daily commuters within Cape Town (South Africa). The analysis applies the mixed nominal logit model, allowing for random distribution of tastes across decision-makers. The model is constructed in R and millions of revealed preference (RP) datasets of commuter choices are analysed within seconds. This study demonstrates the application of data mining and choice modelling techniques within R.";"lightning talk"
"Jung Hae Yoon";"Simulation of the physical movement for Machine Learning with R: Simulation of Robot gait Optimization Using GA";"Recently, machine learning algorithms are used popularly in the engineering field, and simulation of the specific situation becomes one of the important processes. In this talk, we propose an environment for simulation of the physical movement in robots with R. We carried out the optimization of robot gait which is a major issue in the robotics area. As an optimization algorithm, GA (Genetic Algorithm) was used. In our physical simulation, we considered two options. The first option is about position and velocity when forces are applied to the body. We solved this dynamic equation of a robot's body placed in 2-dimensional space using the 3rd order Runge-Kutta method. The second option is for constraining the position and velocity when a body contacts the ground. The sequences of the robot's body position were visualized with animation package in R. This virtual robot has a body, and five parameters determine its walking trajectory. GA package in R was used to optimize these parameters. We successfully get some values that enable robots to walk steady and fast. Through this study, we expect simulation in robotics engineering area can be conducted with R as well.";"oral presentation"
"Jung Bogati Binod";"Building Active Community at Your Place";"The strength of R comes from its community. It's easy to get involved as a member into community (if it exists). However, building a new community or making a community active is not easy. Here, I'll share my experience (as a student) on building first R community in Nepal (now 350+ members). What were the shared struggles of my community? How I managed to overcome challenges and made welcoming & active community. Besides that, I'll also share how our community helps student learn R in academics & research. So, come and join me to know tips on building active & engaging community at your place.";"lightning talk"
"Li Tiantian";"Estimating mortality burden attributable to short-term PM2.5 exposure in China using R";"Studies worldwide have estimated the number of deaths attributable to long-term exposure to fine airborne particles (PM2.5), but limited information is available on short-term exposure, particularly in China. In addition, most existing studies have assumed that short-term PM2.5-mortality associations were linear. There is an urgent need for a comprehensive, evidence-based assessment of the disease burden related to short-term PM2.5 exposure in China. Here, we explored the non-linear association between short-term PM2.5 exposure and all-cause mortality in 104 counties in China; estimated county-specific mortality burdens attributable to short-term PM2.5 exposure for all counties in the country and analyzed spatial characteristics of the mortality burden due to short-term PM2.5 exposure in China using R. The pooled PM2.5-mortality association was non-linear, with a reversed J-shape. We found an approximately linear increased risk of mortality from 0 to 62 ?g/m3 and decreased risk from 62 to 250 ?g/m3. We estimated a total of 169,862 additional deaths from short-term PM2.5 exposure throughout China in 2015. Short-term PM2.5 exposure contributed greatly to the death burden in China, approximately one seventh of the estimates from the chronic effect. Traditional linear effect models likely underestimated the mortality burden due to short-term exposure to PM2.5.";"poster"
"Yuki Hira";"Time Series ExploreR : Interactive time series analysis for data science in Shiny App";"Exploratory Data Analysis (EDA) is an essential process for understanding time series and conducting useful feature extraction. We introduce ""Time Series ExploreR"", which provides interactive EDA in Shiny web apps to accelerate time series analysis for data scientists. Time Series ExploreR is now deployed on data science platform in NTT Communications which is one of the largest Internet service providers in Japan. We will present the effectiveness of this tool with some real use cases and demo. Time Series ExploreR provides four functions that enable time series analysis process interactively. First, it provides time series visualization at various time interval to understand the time series. Second, it provides nessesary information for time series analysis.(e.g. missing value, frequency distribution, correlation, periodic component) Third, it provides basic feature extraction from uni- or multi-variate time series. Fourth, these features are also useful for multiple time series anomaly detection methods. By performing these processes interactively on the web apps using Shiny, it becomes possible to analyze time series data easily and quickly for data scientists.";"poster"
"Olusoji Oluwafemi";"?cyanoFilter?, An Automated Framework for identifying Synechococcus type cyanobacteria populations obtained via flow cytometry";"Flow cytometry is a well-known technique for identifying cell populations in fluids. It is largely applied in biological and medical sciences for cell sorting, counting, biomarker detections and protein engineering. Identifying cell populations in flow cytometry data often rely on manual gating, a subjective and generally irreproducible method based on expert knowledge. To address this issue, two filtering frameworks were developed in R, identifying and filtering out two strains of Synechococcus type cyanobacteria from flow cytometry data. The first framework employs a one and two-dimensional kernel density approach on two fluorescence channels believed to characterize the cyanobacteria populations while the second employs a multivariate normal approach on five fluorescence channels believed to characterize these cyanobacteria populations. We apply both proposed techniques to data from growth curve experiments of the cyanobacteria in mono- and biculture. We observe that both approaches produced similar growth curves of the cyanobacteria population and cyanobacteria counts within 10% difference range. These two frameworks are currently being developed into an R package, cyanoFilter, for use of others doing flow cytometry experiments involving cyanobacteria.";"poster"
"Dutang Christophe";"Maximum spacing estimation, a new method in fitdistrplus";"Maximum spacing estimation (MSE) introduced by [1] is a method for estimating the parameters based on the probability differences of order statistics. 
More precisely, the method consists in maximizing of the geometric mean of spacings in the data, which are the differences between the values of the distribution function at sorted observations. MSE has been proved to be reliable method both for heavy-tailed models by [2] and lighted-tailed models by [1]. Currently, only the BMT package provides the MSE for a single distribution. In this talk, we study the development and integration of this method in the fitdistrplus package [3]. This latter package provides maximum likelihood and other methods for any distribution characterized by d, p, q functions. Using the S3 class and generic methods, this estimation method nicely fits the package framework. An uncertainty analysis is carried out on simulated datasets from both light and heavy tailed models. Finally, we illustrate the relevancy of this method to model insurance losses on real actuarial datasets. [1] Ranneby, Bo (1984). The maximum spacing method. An estimation method related to the maximum likelihood method. 
[2] Wong, T.S.T; Li, W.K. (2006). A note on the estimation of extreme value distributions using maximum product of spacings. 
[3] M. L. Delignette-Muller, C. Dutang (2015). fitdistrplus: An R Package for Fitting Distributions.";"lightning talk"
"Therneau Terry";"The next generation of the survival package";"With over 650 dependent packages, two guiding lights for the survival package are that ""you can't do everything"" and ""don't break it"". As a consequence, although there has been continuous improvement in the underlying algorithms, additions of major new functionality are rare. Version 3 is an exception. Major additions include much broader support for multi-state models, cacluation of absolute risk estimates, and data checking. A design goal was to make multi-state models as easy to fit and use as a single state coxph or survfit model, and to make absolute risk estimates as easy as a Kaplan-Meier curve --- all without rocking the boat with respect to current software. Why do this? The classic triad of Kaplan-Meier, log-rank, and Cox model are reliable tools for time-to-event data that has a single endpoint. However, in the authors' own work single endpoint models have now become the minority. We will give an example from the Mayo Clinic Study of Aging where keeping track of multiple endpoints is a key part of the analysis: the progression from cognitively normal (CN) to mild cognitive impairment (MCI) to dementia; low/medium/high levels of underlying neurodegeneration, the accumulation of amyloid plaques in the brain, and of course status of alive/dead. We will illustrate the new usages, and discuss how other packages can plug into this functionality and where to find further documentation and examples.";"oral presentation"
"Bannert Matthias";"timeseriesdb - Manage, Process and Archive Time Series with R and PostgreSQL";"The timeseriesdb framework maps R time series representations to PostgreSQL key-value pair storage and links data descriptions in a relational manner. Combining key-value pairs with relational database features results in a light-weight but powerful architecture that keeps maintenance at a minimum as it allows to store a large number of records without the need for multiple partitions. The timeseriesdb framework was tailored to the needs of official and economic statistics with long term data conservation in mind: It handles data revisions (vintages), release dates and elaborate, multi-lingual meta information. 
Apart from implementation insights, this talk discusses how data management in a well etablished, enterprise level, relational database as opposed to file based management opens up the opportunity to use standard web technologies such as REST to easily build interfaces and expose time series data online.";"oral presentation"
"Da Veiga Sébastien";"R/Shiny platform at Safran for expensive experiments";"The design and analysis of expensive experiments, whether they come from numerical simulations or bench testing, is now an essential step for design engineers working on complex systems. Such a study requires a large variety of tools, ranging from design of experiments to optimization algorithms with nonlinear regression models, as well as user-friendly visualization techniques.
To facilitate our engineers work at Safran, we decided to build an R/Shiny application which gives them easy access to state-of-the-art R packages and in-house research advances.This platform is deployed via a system of secured containers for all Safran engineers: they only need to use their browser and connect to the container system to enjoy interactive visualizations and efficient algorithms. Currently more than 300 accounts have been created and the base of frequent users is around 50.
In this talk, I'll explain why we chose the R/Shiny framework for such a platform and discuss several industrial applications which where tackled by Safran engineers. The stakes of the daily management of the platform for an operational use will also be adressed, as well as our desire to publish an open-source version later this year.";"poster"
"Ooi Hong";"AzureR: talking to Azure from R";"We describe AzureR: a family of packages for working with Azure, Microsoft's cloud computing platform. They are designed to be lightweight, requiring only commonly-used R packages as dependencies, yet powerful and flexible. The family currently includes the following packages: - AzureAuth: OAuth authentication with Azure Active Directory - AzureRMR: a flexible, generic framework for deploying and managing resources in Azure; can be extended by other packages to support specific services - AzureStor: interface to storage accounts, including features such as parallelised file transfers and a shell to AzCopy, the cross-platform commandline utility - AzureKusto: interface to Azure Data Explorer (also known as Kusto), a fast, scalable data exploration service - AzureContainers: tools for working with Docker containers and Kubernetes clusters - AzureVM: virtual machine management with a particular emphasis on Data Science Virtual Machines (DSVMs), which are Azure VMs preloaded with software for statistics and machine learning    ";"poster"
"Genolini Christophe";"R++, a new Graphical User Interface for R";"R++ is a new Graphical User Interface for R. It is intended for those who are not statisticians (doctors, psychologists, salespeople,...) but who nevertheless need high-level professional statistics. R++ is a result of collaborations with different computer-science labs specialised in Human-Computer Interaction. Through twenty or so meetings with statisticians, we have identified the tasks that are considered to be arduous, annoying or of low added value. 3 aspects have arisen: reading data (encoding problems, column separators, odd formats, distant data bases), data managment (outliers' detection, modalities' merging,...) and exporting results (text or graph). R ++ offers an easy-to-use interface simplifying processing these tasks. In reading the data, a preview is displayed allowing checking of the data and thus to proceed to various settings. For processing the data, several tools are available. For example, it is possible to instantly graphicaly represent all the variables of a database or to correct the types using a colour system. For exporting the data, interface allows adjustments to graphical settings, then a simple drag&drop can export graphs. Naturally, for the sake of reproducibility of analyses, each action generates the corresponding R code. A code editor and an R console are also available for more advanced uses. R++ is free for academics, students, and associations.";"lightning talk"
"Granda Víctor";"Compiling a global database of sapflow measurements with R: Workflow and tools for the SAPFLUXNET database";"Understanding the global patterns and drivers of plant transpiration and its physiological control requires compiling and harmonising heterogeneous ecophysiological datasets. The SAPFLUXNET project (http://sapfluxnet.creaf.cat/) has compiled the first global database of transpiration from sap flow measurements, including > 10 million sub-daily records from 2732 plants and 176 species, measured in > 200 globally distributed datasets.  Here we present the SAPFLUXNET data infrastructure, which allows implementing a reproducible workflow in the R environment. The ""sapfluxnetQC1"" package (https://github.com/sapfluxnet/sapfluxnetQC1) is an internal-use package which implements data harmonisation and quality control, using Rmarkdown-generated reports and interactive Shiny apps. The ""sapfluxnetr"" package (https://github.com/sapfluxnet/sapfluxnetr) is aimed to ease data access, aggregation and analysis, using two new S4 classes (""sfn-data"", and ""sfn_data_multi""). The SAPFLUXNET database and its data infrastructure have been designed to be open and publicly available, supporting new data-driven analysis of global vegetation functioning and also facilitating future updating and maintenance.";"lightning talk"
"Kalibera Tomas";"Sustainable Package Development";"Writing and maintaining packages is an essential contribution to the R community. Despite a number of formal requirements on packages, most of the internal details of the language can be inspected and modified through reflective features and a rich C API, giving a lot of freedom to package developers. This probably contributed to the popularity of R, but poses a risk when not used responsibly.

R needs to adapt to the changing needs of its users and to changes in the software/hardware environments in which it is used. As of today, almost any change in the R runtime, however minute, breaks some packages. The causes are hard to find especially when the change is to undocumented R behavior or when it ""wakes up"" an old bug. Tests using all CRAN/BIOC packages are run routinely, requiring expensive hardware. Debugging requires skill, experience, knowledge of R internals and typically much more time than the implementation of the change that caused the bug. It is done by R Core and adds to the workload of repository maintainers.

This talk is an inspiration for package authors who want to develop packages responsibly, without unnecessarily increasing the cost of maintenance and evolution of R. It will include advice and examples based on my work on the R runtime (and debugging of packages) related to PROTECT bugs, in-place modification of immutable values, memory management at the C/C++ boundary, and parse data fixes.";"oral presentation"
"Thomas-Agnan Christine";"Modelling spatial flows with R";"We present an R implementation of spatial interaction models. Flow data represent movements of people or goods between two spatial locations, for example in migration, international trade, transportation. Gravity models, which have been extensively used for this purpose, include a function of distance between origin and destination among the explanatory variables to account for the spatial dimension. To further eliminate the spatial structure present in this type of data, we implement two fitting methods for spatial autoregressive models accounting for spatial dependence between flows: a Bayesian approach and a three stage least squares approach. We discuss impacts measures evaluation as well and extend these computations to the case of different characteristics at origin and destination. Contrary to existing programs, our implementation with the R language also allows for a different list for origins and destinations. We illustrate with an application to air transportation data.   LeSage, J. P., and Thomas-Agnan, C. (2015). Interpreting spatial econometricorigin-destination flow models. Journal of Regional Science, 55(2),188-208. Margaretic, P., Thomas?Agnan, C., & Doucet, R. (2017). Spatial dependence in (origin?destination) air passenger flows. Papers in Regional Science, 96(2), 357-380.";"oral presentation"
"Génot Benoît";"Cross-referencing catchment data: how R can provide essential tools for the development of models for flood prediction";"Hydrologists seek to better understand the processes involved in catchment response during floods using hydrological models. Model development requires databases combining hydrological, climatic and physical data. Large catchment databases have been developed at Irstea (Antony, France) over the last 30 years. Recently, a project to automate the construction of these databases was launched. The automated processing chain was developed in R and includes four main steps: - the automatic delineation of catchments boundaries from a digital elevation model by coupling R with GRASS GIS (rgrass7 package) and FORTRAN codes - the estimation of climate inputs at the catchment-scale by crossing catchment boundaries and climatic grids (raster package) - the use of a ""shiny"" interface including dynamic leaflet maps and real time upstream drainage basins to ease the expertise on the exact location of hydrometric stations - the production of synthetic sheets cross-referencing various hydrological, topographical and climatic data at the basin scale with dynamic graphs (dygraphs package) and static graphics (available at http://webgr.irstea.fr/activites/base-de-donnees/) This presentation will show how the developments made in R and existing packages were combined and implemented for building a large database at the national scale for research and operational applications in hydrology.";"poster"
"Steves Irene";"Teaching data science with puzzles";"Of the many coding puzzles on the web, few focus on the programming skills needed for handling untidy data. During my summer internship at RStudio, I worked with Jenny Bryan to develop a series of data science puzzles known as the ""Tidies of March."" These puzzles isolate data wrangling tasks into bite-sized pieces to nurture core data science skills such as importing, reshaping, and summarizing data. We also provide access to puzzles and puzzle data directly in R through an accompanying Tidies of March package. I will show how this package models best practices for both data wrangling and project management.";"oral presentation"
"Bichat Antoine";"Impact of tree choice in metagenomics differential abundance studies";"In microbial ecology, a commonly held belief states that the taxonomy reflects ecological niches and that closely related species are therefore to be abundant at the same time. This belief has prompted the development of several statistical methods that leverage the tree-like structure of taxonomy when detecting differentially abundant taxa. Although it is true for datasets spanning different ecological niches at high taxonomic ranks, in general it is not. We investigated the similarity between the tree of correlation between taxa and the taxonomic tree using a two-pronged approach: we first tested whether the taxonomy was in the confidence region of the tree of correlation before testing whether both trees were significantly closer than two random trees. To do so, we relied on three distances on the space of trees. Confidence regions were determined by bootstrapping and typical distances between random trees by permuting the leaves labels. Based on those distances applied to several publicly available datasets, we conclude that the taxonomy and the tree of correlation are in general no closer than two random trees, although some exceptions were observed in dataset with strongly differentiated niches (different body sites for example). This suggests that using the taxonomy offers no tangible benefits over other hierarchical structures when studying differentially abundant taxa. ";"poster"
"Nassiri Vahid";"clustDRM: an R package and Shiny app for modeling high- throughput dose-response data";"Dose-response modeling is a crucial step in drug discovery and safety assessment. R packages like ""drc"" and ""DoseFinding"" provide useful tools to fit dose-response models and estimate parameters such as effective doses. When it comes to modeling dose-response patterns of several compounds (e.g., in high content screening studies), one may face with three different challenges:  1. the dose-response relationship may be non-existent (flat patterns),  2. no single dose-response model fits the data well enough,  3. due to the use of often complex non-linear models the estimation could become computationally intensive (especially in a high-throughput setting).  To address these issues, we have developed an R package called 'clustDRM` that provides a unified platform to analyze dose-response relationships in a high-throughput setting. The package uses a two-stage approach. First, it filters out the compounds with a flat dose-response pattern and identifies the patterns of the non-flat ones. Next, it fits a set of appropriate dose-response models (accounting for the previously identified patterns) and uses model-averaging to estimate effective doses and their standard errors. Parallel computations are used to address the third issue. Furthermore, a Shiny app accompanies the package to make its use easier for scientists from various fields without deep knowledge of R.";"oral presentation"
"Janssenswillen Gert";"propro: Enhancing Discovered Process Models using Bayesian Inference and MCMC";"Process mining is an innovative research field aimed at extracting useful information about business processes from event data. An important task herein is process discovery; the discovery of process models from data. The results of process discovery are mainly deterministic process models, which do not convey a notion of probability or uncertainty. In this paper, Bayesian inference and Markov Chain Monte Carlo is used to build a statistical model on top of a process model using event data, which is able to generate probability distributions for choices in a process' control-flow. A generic algorithm to build such a model is presented, and it is shown how the resulting statistical model can be used to test different kinds of hypotheses, such as non-deterministic dependencies between different choices in the model. The algorithm is implemented in a new R package, named propro, for probabilistic process models. In this paper, it is shown how propro can be used in real-life process analysis scenarios and leads to valuable information about the processes under consideration, which go beyond the discovery of static control-flow. As a result, propro supports the enhancement of discovered process models by exposing probabilistic dependencies, and allows to compare the quality among different models, each of which provides important advancements in the field of process mining. ";"oral presentation"
"Ritz Christian";"Advances in dose-response analysis";"During the last few decades, dose-response analysis, which is fitting and interpreting results obtained from using fully parametric nonlinear dose-response (regression) models, has undergone dramatic changes, from cumbersome, more or less manual calculations and transformations to blink-of-an-eye operations on any laptop.

The first version of the R extension package ""drc"" for doing dose-response analysis was developed in 2005. Originally, it was developed for fitting log-logistic models that were used routinely in toxicology and pesticide science. Subsequently, the package has been extensively revised, mostly in response to feedback from the user community. By now, it has developed into a veritable ecosystem for all kinds of dose-response analyses. Similar functionality for dose-response analysis does not exist in any other statistical software.

This talk briefly reviews the development, mentinoning methods that have become obsolete, methods that have stayed in use, and new methods that have been incorporated. Changes have largely been propelled by the advent of powerful extension packages. Specifically, we will talk about modular use of multiple packages when doing dose-response analysis. We will show examples on modelling of event times, binary mixtures and species sensitivity distributions. Finally, we will outline a number of directions for future developments.


";"oral presentation"
"Stiglic Gregor";"Using R in development, validation and deployment of type 2 diabetes mellitus screening tools";"It is highly important to find solutions for early recognition of people who are at higher risk to develop type 2 diabetes mellitus and to react immediately. In this talk, we will present the development of predictive models for early detection of pre-diabetes and undiagnosed type 2 diabetes mellitus based on electronic healthcare records (EHR) data from five Slovenian healthcare centres. The SLORISK application and screening tests are based on fasting plasma glucose and Finnish Diabetes Risk Score questionnaire where data were collected from EHR of 2073 healthy individuals. We will also discuss the challenges of developing and validating the predictive models in R as well as deployment of risk estimation web application in the clinical environment. Additionally, we will illustrate the potential of using different machine learning approaches as opposed to more traditional statistical models often used in development of screening tests using R. Separate versions of the online application for general public and for healthcare professionals working at the primary healthcare level will be presented.";"poster"
"Bouchequet Paul";"Using R for automatic sleep analysis as a regular part of the clinical process";"Sleep medicine is the medical specialty devoted to the diagnosis and treatment of sleep disorders. The polysomnography (PSG) is the gold standard exam, conducted to diagnose the majority of disorders. It consists of an overnight record of body functions, including electroencephalography, eyes movements, muscles activities, respiratory flows, and heart activity. PSGs produces many data, mostly in the form of high-frequency time series. In the classical clinical process, Medical Doctors or Sleep Technologists visually score records, assigning stages and events to the multivariate time serie. This is a time-consuming task, taking a few hours for each record; therefore automating the process is a real need. At the sleep center of the Hôtel-Dieu, Paris, we use R to visualize, analyze and automatically score sleep records. Heterogeneous sleep data can be easily displayed in an interactive way using Shiny. New representations, including 3D projections of sleep stages are provided to medical teams. Machine learning algorithms are trained on previous recorded and annotated records of the center. Reports are built using R Tex and publishing related packages to bring the information smoothly in the existing clinical process. Automatic sleep analysis has not yet reached consensus in the medical community. However, it is already a valuable decision helping tool for sleep analysts.";"poster"
"Delaigue Olivier";"airGR and airGRteaching: two packages for rainfall-runoff modeling and teaching hydrology";"The use of R is growing fast on every step of hydrological studies, from the retrieval of hydro-meteorological data, to spatial analysis and cartography, hydrological modeling, statistics, and the design of static and dynamic visualizations (Slater et al., 2019, HESSD). Recently, IRSTEA developed an R package called airGR (Coron et al, 2017, EM&S, and 2018), to make the GR rainfall-runoff models widely available and facilitate reproducible science. It is available on the CRAN and includes efficient and fast-running hydrological models. The airGR package was designed to facilitate the use by non-expert users and allows the user to customize evaluation criteria, models or calibration algorithm. To help students learning and because the GR models are widely used in hydrology courses in French universities or engineering schools, we developed a package called airGRteaching (Delaigue et al., 2018, HIC, and 2018) based on airGR. This package reduces modeling to only three functions. In addition, the package offers various graphical outputs, static and dynamic (using the dygraphs package) to easily explore the model input data, as well as the results obtained during the model calibration or simulation phase. Finally, airGRteaching offers a Shiny interface allowing students to fully understand the role of each parameter or internal state of the models.";"poster"
"Krug Rainer";"MetaData schemes: Definition, Use and Management in R";"The data generated in and for research increases dramatically not only in size but also in the number of data sets. For (later) re-use of data, the metadata associated with data sets is as important as the data itself. But when ?normal' researchers are asked to provide metadata, the enthusiasm is severely dampened due to the complexities of most metadata schemes. It is generally easy to provide general (?bibliographic?) metadata, but this type of metadata is often not very useful when trying to find the data one is looking for. Going further, the definitions of the schemes are so complex, that only somebody very familiar with these (and preferebly fluent in reading and writing of xml) is able to use them to their full potential. Consequently, metadata remains the poor stepchild of data sets. I will present an approach which tries to overcome this weakness by 1) being driven by scientists in the specific field and their needs to find other data sets which can be used in their research
2) building an R package to define the metadata scheme
3) use a spreadsheet to enter the metadata per experiment and not per dataset
4) validate the metadata by using a RMarkdown report
5) export the metadata to xml format and bundling all these (i.e. definition of the scheme, the spreadsheet for entering of metadata, the validation function and report, as well as the export functions) into one convenient R package.";"lightning talk"
"Fazilleau Quentin";"Package flextable: a grammar to produce tabular reporting from R";"The flextable package provides an interface to generate tables for publication and corporate reporting.
Originally designed to work with the package officer, it has evolved this year to get compatible with the R Markdown format.

The package enables simple and complex tabular reporting composition thanks to a user-friendly grammar.
It supports output to HTML, Word, PowerPoint and recently PDF through the pagedown package.

In this talk I will introduce the main concepts of the package and demonstrate them with simples examples.
I will show how to manage the layouts, how to format the content and mix images with text.
Finally, I will expose a concrete implementation inside an R Markdown report and a Shiny application.";"oral presentation"
"Giraud Timothée";"Thematic mapping with ""cartography""";"The R spatial ecosystem is blooming and dealing with spatial objects and spatial computations has never been so easy. In this context, the cartography package aim is to create thematic maps with the visual quality of those designed with classical mapping or GIS tools. 
The package helps to design cartographic representations such as proportional symbols, choropleth, typology, flows or discontinuities maps. It also offers several features that improve the graphic presentation of maps, for instance, map palettes, layout elements (scale, north arrow, title...), labels or legends. 
cartography is a mature package (first release in 2015), it has already been reviewed in both software and cartography focused journals (Giraud, Lambert 2016 & Giraud, Lambert 2017). It follows current good practices by using continuous integration and a test suite. A vignette, a cheat sheet and a companion website help new users to start using the package. 
In this presentation we will firstly give an overview of the package main features. Then we will develop examples of use of the package along with other spatial related packages. 
Giraud, T., & Lambert, N. (2016). cartography: Create and Integrate Maps in your R Workflow. The Journal of Open Source Software, 1(4), 1-2.
Giraud, T., & Lambert, N. (2017, July). Reproducible cartography. In International Cartographic Conference (pp. 173-183). Springer, Cham.";"oral presentation"
"Crippa Alessio";"Implementation and analysis design of an adaptive-outcome trial in R";"The development of new drugs in oncology often requires the identification of predictive biomarkers for patients where a specific drug is more beneficial. The multiplicity of available treatments as well as the vast collection of biomarkers makes the design of clinical trials difficult. A platform trial with an adaptive design allows the investigator to learn from the accumulating data and to modify prespecified components of the trial while it is ongoing. This can improve power, reduce the number of patients and costs, treat more patients with effective therapies, correctly identifying subgroup of responding patients, and shortening the time of the trial. However, adaptive designs are more complex than traditional studies and require particular attention in their design and analysis. I will present the design and implementation of the ProBio study, a biomarker driven platform trial for improving treatment decision in patients with metastatic castrate resistant prostate cancer. I will describe how to assess the operating characteristics (type I error and power) in R through extensive simulations and calibrations. I will illustrate Bayesian methods for survival analysis employed in the evaluation of the accumulating data, and outline the advantages of using R in the diverse phases of the trial, such as producing randomization lists, generating automatic reports, and relevant dashboards.";"oral presentation"
"Wijffels Jan";"Recent additions to R's NLP ecosystem: udpipe, textrank, crfsuite, BTM, ruimtehol";"In this presentation, I'll showcase some recent additions that I've added to the Natural Language Processing R package landscape.
I'll cover 5 packages that I've put recently on CRAN:

udpipe 
Perform tokenization, tagging, lemmatization and dependency parsing conveniently on more than 60 languages.
Contains in addition functionalities regarding keyword extraction and general NLP flows 

crfsuite 
Allowing users to fit your own Conditional Random Fields for Labelling Sequential Data in Natural Language Processing. 
Common use cases of these are entity recognition, text chunking, part of speech tagging, intent recognition or classification of any category you have in mind...

BTM  
Build Biterm Topic Models. These topic models are particularly good on clustering short text (e.g. survey data/twitter messages/sentences) 
This is done based on modelling word co-occurrences instead of traditional LDA word document cooccurrence

textrank
Do text summarisation by applying Google's Pagerank algorithm on text and sentences ruimtehol
This package is a general package allowing to model entity embeddings. These can be used
 For text classification models
 For learning word, sentence or document level embeddings
 Finding sentence or document similarity
 Ranking web documents
 Content-based recommendation & Collaborative filtering based recommendation
 Identification of entity relationships";"oral presentation"
"Fumbarev Julia";"Process analysis and optimal allocation of parking space with R";"Optimal allocation of parking space is a difficult problem in vehicle distribution. An unnecessary movement of a vehicle can cause additional cost in the range of several hundred Euros. When scaled to high volume production and delivery of cars, the importance of optimized processes becomes clear. At the BMW Group, we are faced with the delivery of millions of cars a year. In order to optimize our parking space allocation we employed process mining methods that describe and evaluate the movements that took place. We present the analysis of these results and how weak points in the process such as bottle necks or loops could be identified. In particular, we evaluated standby times, transport times and process times and secondly, the process flow and the associated resources. This enabled us to uncover inefficiencies and quantify the optimization potential. Following this thorough analysis, we implemented a recommendation algorithm that suggests an optimal allocation of the parking spaces. The metric optimized by our algorithm is the standing time in the storage locations. All analysis is based on the BMW Group's internal distribution. We show that the algorithm can reduce the number of transfers and improves the use of parking space, as measured in terms of standing time.";"poster"
"Fox John";"A Generalized Framework for Parametric Regression Splines";"Regression splines are piecewise polynomials constrained to join smoothly at boundaries called knots. They are traditionally viewed as an alternative to other methods for modeling nonlinear relationships, such as transformations, polynomial regression, and nonparametric regression. Regression splines are parametric and are implemented by introducing a regression-spline basis into the model matrix for a linear or similar regression model. It is usual not to focus on the estimated parameters for a regression spline but instead to represent the model graphically, and traditional regression-spline bases, such as B-splines and natural splines, respectively implemented in the bs() and ns() functions in the R splines package, are selected for numerical stability rather than interpretability. The emphasis on graphical interpretation makes sense but also represents a missed opportunity. We introduce generalized regression splines, implemented in the gspline() in the carEx package, which support the specification of a much wider variety of regression-spline and piecewise-polynomial models using bases that are associated with interpretable parameters.";"oral presentation"
"Shafiei Parvaneh";"Create auto-modeling dashboard using shiny and H2o";"R is a widely used language in data analysis in cross industries, it is easy to do exploratory analysis and predictive modeling. In the business world, there are various use-cases that clients don't need just one time analysis but able to replicate analysis with new data without the need to ask data scientists to redo analysis or predictive modeling by adding various KPI, evaluate the results and repeat the same cycle. They need to have an access to a tool which gives them the freedom and flexibility of data modeling. In this talk, I am walking you through a real use-case that how I have implemented a dynamic predictive modeling dashboard utilizing shiny and H2o together to enable the client for doing multi time analysis and create predictive models by adding or removing various features directly. This dashboard made them to be independent and perform multiple data predictive modeling based on their daily usage & new data.";"lightning talk"
"Kladivova Linda";"Geostatistics: How to speed Bayesian kriging with the package R-INLA";"Geostatistics, a branch of spatial statistics, is concerned with the estimation and prediction problems or stochastic phenomena on the Earth. It applies general statistical principles in order to model and draw conclusions about geostatistical problems. When an additional prior knowledge about the model parameters can be incorporated into the prediction we use a Bayesian approach. Bayesian methods to deal with spatial data started to appear around the year 2000, with the development of Markov chain Monte Carlo (MCMC) simulative methods. However, MCMC methods are extremely time-consuming. In 2009 a deterministic algorithm based on Laplace approximations was proposed and based on this idea the package R-INLA was developed. The purpose of this work is to compare Bayesian kriging of geoR package with the package R-INLA. Finally, we perform a real geodata analysis through functions of both these packages.";"poster"
"Bista Swechhya";"shinyanimate: R wrapper around animate.css to add animation in your Shiny app";"One of the great features of shiny is that we can build web applications with just few lines of code without the knowledge of HTML, CSS or JavaScript. Since the advent of shiny package, the applications built using shiny are progressively becoming larger and complicated, and many shiny developers often resort to extending their shiny application by writing their own HTML, CSS or JavaScript to get added functionality. To avoid this we can create R packages by writing R wrappers around JavaScript libraries which could help in enhancing shiny applications without resorting to writing HTML, CSS or JavaScript code. One such library is shinyanimate - which is simply an R wrapper around animate.css. Animate.css is an open source CSS animation library with which we can add animation to UI element upon page load or after an event by adding animation classes via JavaScript. The shinyanimate package is a wrapper around this library with which you can add animation to the UI element. shinyanimate further extends animate.css to add animation on hover and scroll besides page load or event trigger. Using this package we can add animation to any target element in just two lines of code.";"lightning talk"
"Trecenti Julio";"auth0: Secure Authentication in Shiny with Auth0";"auth0 is a freemium service used to add authentication in web applications. We present the auth0 R package, which implements solutions to use auth0 with Shiny apps. With a simple interface, it is possible to add authentication interfaces to shiny apps, including social (Google / Facebook / Twitter / Linkedin), enterprise (Active Directory / LDAP), app creator's own database and many others. The auth0 R package needs only two things to work: create a configuration YML file with auth0's API keys, and replace the shiny::shinyApp() function with auth0::shinyAuth0App(). The package also includes i) helpers to collect and operate the logged user information inside the app and ii) logout buttons. The auth0 service can be used offline (localhost), in a shiny-server or even with RStudio's shinyapps.io service. In this lightning talk, we will show how auth0 R package works and, if possible, a live demonstration of an app running with auth0.";"lightning talk"
"Quartier-La-Tente Alain";"RJDemetra: an R interface to JDemetra+ seasonal adjustment software";"More and more infra-annual statistics are produced to evaluate the short-term evolution of an economy (GDP, chocolate sales...) or even to analyse in detail the downloads of CRAN packages! Most of these series are affected by seasonal and trading days effects that must be removed to perform temporal and spatial comparisons. RJDemetra (https://github.com/jdemetra/rjdemetra and soon on CRAN) is an R interface to JDemetra+, the European seasonal adjustment software officially recommended by Eurostat and the European Central Bank and used by more than 80 institutes in the world. This package offers access to all options and outputs of JDemetra+ that implements the two leading seasonal adjustment methods TRAMO/SEATS and X-13ARIMA-SEATS and their pre-adjustment methods (automatic RegARIMA modelling, outlier detection, trading days adjustment). The presentation will highlight the main options of RJDemetra and show how all the resources available in R can be used to improve the production of seasonal adjusted series (producing dashboards, quality reports...) or to carry out studies.";"oral presentation"
"Antunez Kim";"Dealing with the change of administrative divisions over time";"The administrative divisions of countries change over time, making it tricky to combine territorial databases from different dates. I will present two packages which help to solve this problem: 1) COGugaison [1], which provides functions for converting a spatial dataset from one year to what it would be (or have been) another year. The package handles when the territories gather and split. 2) CARTElette [2], which contains geographical layers corresponding to the annual division of French territories that can be loaded directly into R's popular {sf} format. Thanks to these two packages, it is for example possible to look at the evolution of the number of women in France in each French department since 1975 taking into account that some of the territories have changed during this period [3]. At this time, those packages only concern France and are therefore only documented in French. By presenting this problem and my current work internationally, I hope to inspire future extensions to other countries and collaborations with international spatial analysts. [1] https://github.com/antuki/COGugaison [2] https://github.com/antuki/CARTElette [3] https://antuki.github.io/slides/180306_RLadies_COGugaison_carto/180306_RLadies_COGugaison_carto.html#51";"lightning talk"
"Van Hecke, Dr. Ria";"The Heartbeat of My Home - A Shiny App Making Households Smarter with Modern Classification and State Detection Algorithms";"In the smart home domain, the automation of household processes and appliances is of crucial importance. Therefore, appliances should be remotely controllable. One approach to achieve this is to plug devices into smart plugs that can automatically be switched on or off. A further important aspect of smart plugs is that they provide a very precise measurement of the consumed power with high granularity. This allows for a clear energy breakdown and energy efficiency analysis. However, all of these services need a good understanding and ability to gain insights from the power consumption data. To this end, we trained a random forest classifier with high accuracy to identify big consumers when plugged into a smart plug. Furthermore, a state detection algorithm was implemented to determine the exact starting and ending times of a device in order to react precisely according to the services that are demanded by the customer. Finally, as all valuable insights are useless if not made available in an appealing manner so that they can be used by the end consumer, all our algorithms are summarized in a Smart Home Shiny App.";"poster"
"Dandine-Roulland Claire";"Gaston, an R package for Genome-Wide data";"Gaston offers in the flexible R environment functions for efficient manipulation and analysis of high dimensional Single Nucleotide Polymorphism (SNP) data in the form of large genotype matrices. Thanks to the packages Rcpp, RcppParallel, RcppEigen, Gaston functions are mainly written in C++ allowing time and memory gain. More specifically, Gaston includes functions for quality control of SNP data through computation of descriptive statistics. Package functions also allow to compute genotype correlation structure across the genome (Linkage Disequilibrium) or between individuals (Genetic Correlation Matrix). Efficient implementations for genetic applications of Linear Mixed Model parameters estimation algorithms are also available in Gaston. In genetic studies, Linear Mixed Models are used to estimate the proportion of variance explained by genotype data called heritability or to test for association with individual SNPs while taking into account family structure or stratification of population in sample. Gaston is available on CRAN (https://cran.r-project.org/package=gaston). Moreover, other R packages are proposed on Github (https://github.com/genostats) to extend Gaston, including functions for manipulation of imputed data (?dosages? genotypes), rare variants analysis, homozygosity mapping, and Gene x Environment association studies.";"poster"
"Wais Kamil";"Logging and Analyzing Events in Complex Shiny Apps";"The ?shinyEventLogger' package is a logging framework dedicated to complex shiny apps. Its main goal is to help to develop, debug, and analyze usage of our apps. Multiple-line events logging can be done simultaneously, not only to R console, but also to your browser JavaScript console, so you can see logged events in the real-time, using your app already deployed to a server (shinyapps.io, rsconnect). Moreover, your events (together with unique sesssionID and timestamp) can be saved as a text file or into a remote database (currently MongoDB), and be ready for further analysis with the help of process-mining techniques from ?bupaR' package. You can log different types of events, for example:
* log_event(?Hello World!?),
* log_value(input$variable),
* log_output(str(mtcars)),
* log_test(testthat::expect_true(TRUE)),
* and others (errors, warnings, messages). You can time nested events for performance analysis. If you are logging a value of an evaluated expression, not only the value will be logged but also the expression itself, so ?log_value(NROW(mtcars))? gives you: |#1|VALUE|NROW(mtcars)|FIRED|
|#1|32 Each event can be logged with a list of parameters that are event-specific or common for a group of events. CRAN: https://cran.r-project.org/package=shinyEventLogger
Vignette: https://kalimu.github.io/shinyEventLogger/articles/shinyEventLogger.html  ";"oral presentation"
"Wickham Hadley";"Enhancements to data tidying";"The goal of the tidyr package is to help get your data into a ""tidy"" form, where variables are found in columns and observations are found in rows. tidyr has two main drawbacks: * Many people (including me!) find it hard to remember exactly how 
 spread() and gather() work, and most usage require re-reading the 
 documentation. This suggests there are fundamental problems with their
 design.
 
* Many web APIs provide data in JSON, which turns into deeply nested lists
 when loaded into R. tidyr provides few tools for tidying or rectangling
 this sort of data.
 
In this talk, I will introduce new tools in tidyr 1.0.0 that aim to solve both problems, making it easier to tackle traditional rectangular reshaping, as well as making it easier to rectangle deeply nested lists into a convenient form.";"oral presentation"
"Czeller Ildiko";"ropsec: a package for easing operations security for the R user";"Applying security best practices is essential not only for developers or sensitive data storage but also for the everyday R user installing R packages, contributing to open source, working with APIs or remote servers. However, keeping up-to-date with security best practices and applying them meticulously requires significant effort and is difficult without expert knowledge.    The goal of the R package ropsec (github.com/ropenscilabs/ropsec) is to bring some of the best practices closer to all R users and enabling them to add a few more layers of security to their personal workstation and shared work.    In this talk I will focus on signing commits: why you should do it and how ropsec helps you do it the right way with the lowest possible risk of making a mess of your settings. I will also highlight how can you reliably test an R package whose core functionality is changing settings outside your R project.   Work on ropsec started at the 2018 ropensci unconf (unconf18.ropensci.org) and the package continuously improved since then. ropsec leverages gpg (on CRAN) that provides low-level functions for signing commits; the value added comes from the collection of high-level functions, the thorough documentation and the intuitive workflow that help R users to solve end-to-end use cases. Its aim is to mitigate the risk of doing something the user does not intend to do due to the complexity of the low level operations.";"lightning talk"
"Marini Federico";"iSEE: interactive and reproducible exploration and visualization of genomics data";"Data exploration is crucial in the comprehension of large biological datasets, generated by high-throughput assays such as sequencing, with interactivity as key aspect to generate insightful outputs. Most existing tools for intuitive and interactive visualization are limited to specific assays or analyses, and lack support for reproducible analysis. Sparked from a Bioconductor community-driven effort, we have built a general-purpose tool, iSEE - Interactive SummarizedExperiment Explorer, designed for interactive exploration of any experimental data which can be stored in a SummarizedExperiment object, i.e. an integrative data container for storing matrices of assays and tables of associated metadata. iSEE (https://bioconductor.org/packages/iSEE/) is implemented in R and Shiny, and is compatible with many existing R/Bioconductor packages for high-throughput biological data. Essential features include: - A highly customizable interface with different panel types, simultaneously viewing and linking panels to each other
- Automatic tracking of the exact R code generating all visible plots for full reproducibility
- Interactive tours to showcase datasets and findings
- Extendable analyses with custom panel types
- Seamless deployment as an online companion browser for collaborations and publications.";"oral presentation"
"Jiang Wei";"Adaptive Bayesian SLOPE -- High-dimensional Model Selection with Missing Values";"Model selection with high-dimensional data becomes an important issue in the last two decades. With the presence of missing data, only a few methods are available to select a model, and their performances are limited. We propose a novel approach -- Adaptive Bayesian SLOPE, as an extension of sorted $l_1$ regularization but in Bayesian framework, to perform parameter estimation and variable selection simultaneously in high-dimensional setting. This methodology in particular aims at controlling the False Discovery Rate (FDR). Meanwhile, we tackle the problem of missing data with a stochastic approximation EM algorithm. The proposed methodology is further illustrated by comprehensive simulation studies, in terms of power, FDR and bias of estimation.";"oral presentation"
"De Jonge Edwin";"Creating privacy protecting density maps: sdcSpatial";"R allows for creating beautiful maps and many examples can be found. Cartography is an indispensable tool in analyzing spatial data and communicating
regional patterns to your target audience. Current data sources often contain location data making maps a natural visualisation choice.
While these detailed location data are fine for analytic purposes, derived statistiscs have the
risk of disclosing information of individual persons. For example if one plots the spatial distribution of income or getting social welfare, sparsely populated areas may be very revealing. Statistical procedures to control disclosure have been readily available (sdcTable, sdcMicro), but those focus on protecting tabulated or microdata, not on protecting spatial data as such.
R package sdcSpatial (https://github.com/edwindj/sdcSpatial) allows for creating maps that show spatial patterns but at the same time protect the privacy of the target population. The package also contains methods for assessing the associated risk for a given data set.";"oral presentation"
"Akintande Olalekan";"Assessing the Skill of Machine Learning models: A Modified Cross Validation (CV) Approach";"The controversies surrounding dataset splitting technique and folklore of what has been or what should be remain an open debate. Several authors (bloggers, researchers and data scientists) have proposed various options for model validation and by extension the appropriate percentage of DS based on three disjoint datasets splitting (3-DDS); training, test, cross validation and test set respectively. Previous literature recognizes only two disjoint split (2-DDS) for validation the skill of machine learning algorithm. As an addition, we extend each of the existing cross validation (CV) approaches using the new/modern three disjoint splitting (3-DDS) approach. We also examined the relevance of various CV procedures using various m (training size). Preliminary result shows that the 3-DDS approach provides a more robust assessment of the generalization error of the final chosen model which is not available in 2-DDS approach and has equal performance in assessing the prediction error of the selected model as 2-DDS approach. Our finding also shows that, as m increases, any arbitrary splitting percentage can be adopted provided that m is large and test set is ? 50% of m while the validation and test set takes equal percentage.";"poster"
"Genuer Robin";"How to speed-up VSURF (Variable Selection Using Random Forests)?";"The VSURF package is a general tool to perform variable selection for regression or supervised classification problems. It implements a fully data-driven variable selection procedure based on random forests (RF). Its overall run-time depends obviously on the data characteristics (number of observations, n, and number of variables, p, mostly), but also on the computational performance of the randomForest package (since VSURF is heavily based on it). In the last few years, other packages implementing RF (ranger, Rborist) have emerged, and their authors claim that they are faster than randomForest: in the high-dimensional case (p very large and larger than n) for ranger, and in the Big data context (n extremely large) for Rborist. Therefore, one way to speed-up VSURF is to use those other packages instead of randomForest in the procedure. In this talk, RF basics are first given, then the VSURF procedure is detailed, and finally different implementations of RF into VSURF are compared in several frameworks and on different examples, to shed light on when each implementation can or cannot alleviate the overall computation burden.";"oral presentation"
"Le Pennec Erwan";"ggwordcloud: a word cloud geometry for ggplot2";"Word clouds provide a nice mechanism to visualize a list of weighted words. R has already two dedicated packages, wordcloud and wordcloud2, that produce respectively a base plot and a html widget. ggwordcloud has been introduced last fall to propose an alternative for the ggplot2 ecosystem. It consists mainly in a new geometry geom_text_wordcloud which places words on the ggplot2 canvas using an algorithm very similar to the one used in wordcloud. It provides some extensions: a better word scaling, the use of masks as in wordcloud2 and works with ggplot2 facet system. The code has been inspired by ggrepel and relies on RCpp so that the rendering is fast. In the lightning talk, we will present the package and some of its uses. For more details, see https://lepennec.github.io/ggwordcloud/";"lightning talk"
"Trussart Marie";"Development of new computational methods to elucidate the molecular dynamics of drug treatment using CyTOF";"A new technology which couples mass spectrometry with metal-conjugated antibodies permit the profiling of cellular phenotype and signalling cell state of millions of individual cells. Specifically, CyTOF has successfully enabled a comprehensive panel of surface and intracellular protein markers to unravel complex signalling networks and to delineate cell subsets in heterogeneous tissues such as blood, bone marrow and tumours. Indeed, mass cytometers are able to analyse simultaneously more than 40 unique parameters per sample at the single cell level. We worked on the application of an algorithm enabling correction of signal fluctuations and we are proposing a new computational method that is able not only to handle and remove technical unwanted variation but also to refine the previously developed debarcoding method in high-dimensional CyTOF data. As an application of CyTOF to elucidate the effect of drug treatment, our collaborators developed a custom-made protocol to better understand the impact of different treatments on Multiple Myeloma cell lines and Chronic Lymphocytic Leukaemia patients. By using this CyTOF, we will analyse CLL patient samples and aim to resolve disease-relevant pathways at the single-cell level to better understand the impact of each drug that underlie sensitivity or resistance to the treatment and how its efficacy is influenced by heterogeneity.";"poster"
"Solís Maikol";"Using the package `simple features` (sf) for sensitivity analysis";"The curse of dimensionality is a commonly encountered problem in statistics and data analysis. Variable sensitivity analysis methods are a well studied and established set of tools designed to overcome these sorts of problems. In most cases, those methods require a functional or computer code producing the output variable. Also, they fail to capture relevant features and patterns hidden within the geometry of the enveloping manifold projected onto a variable. In this talk we propose an index that captures, reflects and correlates the relevance of distinct variables within a model by focusing on the geometry of their projections. We first construct the 2-simplices of a Vietoris-Rips complex and then estimate the area of those objects from a data-set cloud. Afterwards, through affine transformations we propose our geometric sensitivity index. As a side result, we could also estimate the regression curve of the data-set generate solely by the estimated manifold. We compile all the analysis in an original package called TopSA (https://github.com/maikol-solis/TopSA), short for Topological Sensitivity Analysis. The package exploits the facility of the package `simple features` (sf) to handle all the geometric operations. The package returns the estimated indexes and has implemented a plot function using `ggplot` together with `geom_sf`.";"oral presentation"
"Rickert Joseph";"R Consortium Working Groups";"R Consortium (ISC) working groups are where the difficult collaborative work gets done, and there is a lot going on! In this talk, I will describe what is happening in some of the high profile working groups including the R / Medicine and R / Pharma conference planning groups; the R Validation Hub group that is developing validation standards for R packages for the pharmaceutical industry; The Census group that is working with the U.S. Census Bureau to improve the accessibility of Census Data, the R Certification group, which is attempting to establish a common certification program for proficiency in R; and the R Community Diversity and Inclusion Working Group (RCDI-WG) is to broadly consider how the R Consortium can best encourage and support diverse and inclusive community activities. Working groups are generally open to all interested individuals whether or not their companies are members of the R Consortium. I'll describe how you can get involved, or perhaps even start your own working group.  ";"oral presentation"
"Amaya-Carvajal Victor";"Topological data analysis and statistical learning for galaxies' classification.";"Modern telescopes produce data whose complexity poses, both, mathematical and computational challenges in the extraction of information and knowledge from the astronomical objects. In this talk, we present advances on a study of radio galaxies using techniques of topological analysis of data using the Mapper algorithm and statistical learning (t-SNE) tools.";"poster"
"Chamberlain Scott";"HTTP Requests For R Users and Package Developers";"Many R users request data from the web in their scripts and packages. This talk introduces a modern suite of packages for managing HTTP requests. The crul package is a modern HTTP request library, including asynchronous requests, automatic handling of pagination, and more. Importantly, crul provides an R6 based object system that makes it easier to program with relative to other tools. The webmockr package mocks HTTP requests, returning user specified mocked responses matching the format of the real thing. The vcr package leverages webmockr to cache HTTP requests and responses. Both webmockr and vcr support many HTTP libraries. Last, httpcode provides information on all HTTP codes, and fauxpas provides proper HTTP error classes for use in most HTTP R libraries. These tools together provide a modern way for R programmers to manage HTTP requests.";"oral presentation"
"Valcarcel Alessandra";"rtapas: an R package for subject-specific threshold detection in automatic lesion segmentation using multi-modal MRI";"Total brain white matter lesion (WML) volume is the most widely established magnetic resonance imaging (MRI) outcome measure in studies of multiple sclerosis (MS). To estimate WML volume there are a number of automatic segmentation methods, many of which are available in R, yet manual delineation remains the gold standard approach. These approaches often yield a probability map to which a threshold is applied to create lesion segmentation masks. Unfortunately, few approaches systematically determine the threshold employed; many methods use a manually selected threshold, thus introducing human error and bias into the automated procedure. We developed and validated an automatic thresholding algorithm, Thresholding Approach for Probability Map Automatic Segmentation in Multiple Sclerosis (TAPAS), to obtain subject-specific threshold estimates for probability map automatic segmentation of T2-weighted (T2) hyperintense WMLs. We implemented an R package, rtapas, to disseminate the approach for fitting the TAPAS model and predicting subject-specific thresholds for automatic segmentation. The rtapas package provides an intuitive and unified interface for its main functions to allow imaging statisticians and neurologists to automatically generate lesion segmentation masks from probability maps using subject-specific thresholds.";"poster"
"Gasparini Alessandro";"Analysing results from Monte Carlo simulation studies using the rsimsum package and the INTEREST shiny app";"Monte Carlo simulation studies are computer experiments that involve generating data by pseudo-random sampling; they provide an invaluable tool for statistical and biostatistical research. Consequently, dissemination of results plays a focal role to drive adoption and further development of new methods. However, simulation studies are often poorly designed, analysed, reported. One of the aspects often poorly reported - often not reported at all - is the Monte Carlo error of summary statistics, defined as the standard deviation of the estimated quantity over replications. Monte Carlo errors play a crucial role in understanding how results are affected by chance.
In order to aid researchers interested in running and analysing simulation studies, we developed rsimsum and INTEREST. rsimsum is an R package for analysing results from simulation studies: it computes the most common summary statistics and Monte Carlo errors are reported by default. INTEREST is a shiny app providing an interface to rsimsum, offering tools to analyse simulation studies interactively and export plots and tables of summary statistics for later use. rsimsum and INTEREST can aid investigating results from simulation studies and supplement the reporting of their results to a great extent, allowing researches to share the full results of their simulation study and readers to explore them freely and in a more engaging way.";"oral presentation"
"Martin Emma";"merlin - mixed effects regression for linear and nonlinear models";"The rise in availability of electronic health record data raises both challenges and opportunities for new and complex analyses. The merlin package in R provides an extended framework for the analysis of a range of data types, encompassing any number of outcomes of any type, each of which could be repeatedly measured (longitudinal), with any number of levels and with any number of random effects at each level. Many standard distributions are described, as well as non-standard user-defined non-linear models. The extension focuses on a complex linear predictor for each outcome model, allowing sharing and linking between outcome models in a flexible way, either by linking random effects directly, or the expected value of one outcome (or function of it) within the linear predictor of another. Non-linear and time-dependent effects are also seamlessly incorporated to the linear predictor through the use of splines or fractional polynomials. merlin allows level-specific random effect distributions and numerical integration techniques to improve usability, relaxing the normally distributed random effects assumption to allow multivariate t-distributed random effects. We will take a single dataset of patients with primary biliary cirrhosis and attempt to show the full range of capabilities of merlin.";"oral presentation"
"Musy Sarah";"Variation of patient turnover on a 30-minutes basis for 3 years: analysis of routine data of a Swiss University Hospital";"Introduction Hospitals face widely varying care demands intensifying nurses' work, since the number of admissions, discharges and transfers within and between units is fluctuating throughout the day. The objective was to describe patients' movements for a large Swiss University Hospital.    Methods Data from 2015 to 2017 were used containing patients' movements time and dates. With padr package, continuous time was extracted between each movement of the patient for an interval of 30 minutes. The first and last movements were recognized as admission and discharge, respectively. For the transfers, the use of the functions lead() and lag() from dplyr package were used. The goal was to identify for each unit if the patient was coming in or out. Finally, the amount of entries (admissions and transfers in) was plotted against exits (discharges and transfers out) on a pyramid bar chart using ggplot2 and scales packages.   Results Ten departments and 77 units were analyzed with 85,706 patients' observations. The final data comprised of more than 26 million data points. Patient flow varies considerably. While for some units entries and exits occur at the same time, other units have volatile periods with different times of the day where entries and exits occur leading to substantially variation in patient load.   Conclusion It is the first analysis of patient movements conducted on this level of granularity.";"oral presentation"
"Chauvel Cécile";"Evaluation of integrative clustering methods for the analysis of multi-omics data";"Recent advances in sequencing, mass spectrometry and cytometry technologies have enabled researchers to collect large-scale omics data from the same set of biological samples. The joint analysis of multiple omics offers the opportunity to uncover coordinated cellular processes acting across different omic layers. In this work, we present a thorough comparison of a selection of recent integrative clustering approaches, including Bayesian and matrix factorization approaches. Based on simulations, the methods were evaluated on their sensitivity and their ability to recover both the correct number of clusters and the simulated clustering at the common and data-specific levels. Standard non-integrative approaches were also included to quantify the added value of integrative methods. For most matrix factorization methods and one Bayesian approach, the shared and specific structures were successfully recovered with high and moderate accuracy, respectively. An opposite behavior was observed on non-integrative approaches, i.e. high performances on specific structures only. Finally, we applied the methods on the TCGA breast cancer data set to check whether results based on experimental data were consistent with those obtained in the simulations. Chauvel, Novoloaca, Veyre, Reynier and Becker (2019) Briefings in Bioinformatics https://doi.org/10.1093/bib/bbz015";"poster"
"Hennequet-Antier Christelle";"ViSEAGO: Easier data mining of biological functions organized into clusters using Gene Ontology and semantic similarity";"The main objective of ViSEAGO workflow is to carry out a data mining of biological functions and establish links between genes involved in the study. We developed ViSEAGO in R to facilitate functional Gene Ontology (GO) analysis of complex experimental design with multiple comparisons of interest. It allows to study large-scale datasets together and visualize GO profiles to capture biological knowledge. The acronym stands for three major concepts of the analysis: Visualization, Semantic similarity and Enrichment Analysis of Gene Ontology. It provides access to the last current GO annotations, which are retrieved from one of NCBI EntrezGene, Ensembl or Uniprot databases for available species. ViSEAGO extends classical functional analysis to focus on functional coherence by aggregating closely related biological themes while studying multiple datasets at once. It provides both a synthetic and detailed view using interactive functionalities respecting the GO graph structure and ensuring functional coherence supplied by semantic similarity. ViSEAGO has been successfully applied on several datasets from different species with a variety of biological questions. Results can be easily shared between bioinformaticians and biologists, enhancing reporting capabilities while maintaining reproducibility. ViSEAGO is publicly available on https://forgemia.inra.fr/umr-boa/viseago.";"poster"
"Gui Raluca";"REndo: An R Package to Address Endogeneity Without External Instrumental Variables";"Endogeneity becomes a challenge when aiming to uncover causal relationships in empirical research. Reasons are manifold, i.e. omitted variables, measurement error or simultaneity. These might lead to the unwanted correlation between the independent variables and the error term of a statistical model. While external instrumental variables methods can be used to control for endogeneity, these approaches require additional information which is usually difficult to obtain. 

Internal instrumental variable (IIV) methods address this issue by treating endogeneity without the need of additional variables, taking advantage of the structure of the data. Implementations of IIV are rare. Thereby, we propose the R package ?REndo? that implements five instrument-free methods: the latent instrumental variables approach (Ebbes et al. 2005), the higher moments estimation (Lewbel 1997), the heteroskedastic error approach (Lewbel 2012), the joint estimation using copula (Park and Gupta 2012) and the multilevel GMM (Kim and Frees 2007). 

This talk will focus on both, the theory behind each of the five methods proposed as well as on the practical implementation using real and simulated data.";"oral presentation"
"Shin Hyesop";"Bridging agent-based modelling and R with nlrx: simulating pedestrian's long-term exposure to air pollution";"Agent-based modelling (ABM) is a bottom-up simulation for simulating actions and interactions between agents. This approach it is particularly useful when modelling human-environment interactions, such as pedestrian's cumulative exposure levels related to travel patterns. Until recently, possible ways to report results were either to bring the text file after simulating the model that is tedious and time-consuming, or to use packages where the codes can only access the ABM platform, which slows the simulation speed (especially for geospatial models). Here, we use a newly released nlrx package that improves the modelling speed, iterates multiple jobs, and provides algorithms for temporal sequence plotting. From our pre-built pedestrian exposure model of Gangnam, Seoul, we demonstrate sequences of pedestrian's health status in every 100 ticks using gganimate, and plot risk population change in geographical hierarchies. Finally, we will discuss our efforts to encourage geographers and modellers to use R as a platform to conduct ABM studies.";"oral presentation"
"Raviv Eran";"Forecast Combination in R";"Introducing the R package ForecastComb. The aim is to provide researchers and practitioners with a comprehensive implementation of the most common ways in which forecasts can be combined. The package in its current version covers 15 popular estimation methods for creating a combined forecasts ? including simple methods, regression-based methods, and eigenvector-based methods. It also includes useful tools to deal with common challenges of forecast combination (e.g., missing values in component forecasts, or multicollinearity), and to rationalize and visualize the combination results.";"oral presentation"
"Jombart Thibaut";"Reproducible data science to support outbreak responses: experience from the North Kivu Ebola outbreak";"The response to emerging disease outbreaks and health emergencies are increasingly data-driven, integrating various sources of information to improve
situational awareness in real time. Outbreak analytics face many of the modern data science challenges, and additional difficulties pertaining to the
emergency, low-resource settings characterising some of these outbreaks. In this presentation, I will outline some of these challenges, and a range of solutions
developed by the R Epidemics Consortium (RECON), an NGO dedicated to developing free analytics resources for health crises. In particular, we will discuss
different aspects relating to the deployment of robust, reliable reporting infrastructures in the 2019 Ebola outbreak in North Kivu, DRC. We will showcase
features of new R packages dedicated to data standardisation and cleaning (linelist), automated reporting (reportfactory), and offline alternatives to
online repositories such as CRAN and github for deploying R-based analysis environments (RECON deployer). I will conclude on how R can help strengthening
outbreak response capacities, and more generally humanitarian work, in low and middle income countries.";"oral presentation"
"Borms Samuel";"The R Package sentometrics to Compute, Aggregate and Predict with Textual Sentiment";"We provide a hands-on introduction to optimized textual sentiment indexation using the R package sentometrics. Driven by the need to unlock the potential of textual data, sentiment analysis is increasingly used to capture its information value. The sentometrics package implements an intuitive framework to efficiently compute sentiment scores of numerous texts, to aggregate the scores into multiple time series, and to use these time series to predict other variables. The workflow of the package is illustrated with a built-in corpus of news articles from The Wall Street Journal and The Washington Post to create a selection of interesting aggregated text-based indices, and use these to forecast expected stock market volatility.";"lightning talk"
"Rossi Marcelo";"Fuzzy-based rules R packages use in an Equine Infectious Anemia epidemical ODE model.";"Background: The Equine Infectious Anemia (EIA) is worldwide incurable disease affecting Equid family by contaminated blood transmissions, and it have been a major impact on the economy of the Brazilian Pantanal region. A fuzzy- and knowledge-based model to represent Equine Infectious Anemia virus infections by sharing of blood-contaminated needles was developed from literature and researcher's knowledge to study this type of virus infection. Methods: Fuzzy set and propositional fuzzy rules were employed to represent this ?contaminated needles force of viral infection? due the difficulty of values determination by experiments. To develop the system based on fuzzy rules, it used the minimum t-norm, Mandani's inferring t-conorms method, and centroid method to defuzzyfication operations. It applying fuzzy sets and nine propositional rules, this force of blood-contaminated needles infection was calculated. After, we assessed the simulation runs from an ODE epidemiological model employed to represent the viral infections and fuzzy rules constructed through fbrs, deSolve and pracma R packages. Conclusions: The results suggest, for the clinical concepts represented by fuzzy rules, the same EIA prevalence observed in the field over the past 40 years. So, this study showed two possible infection sources in Pantanal horse troops: one, from insect-vector, and another, iatrogenic.";"poster"
"Adin Aritz";"A shiny web application for disease mapping. Making easy the fit of spatio-temporal models.";"Spatial and spatio-temporal analyses of count data are crucial in epidemiology and other fields to provide accurate estimates of mortality and/or incidence risks, and unveil the underlying spatial and spatio-temporal patterns. However, fitting spatial and spatio-temporal models is not easy for non-expert users. Here, we present the interactive web application SSTCDapp for the analysis of spatial and spatio-temporal mortality (or incidence) data, which is addressed at https://emi-sstcdapp.unavarra.es/. The web application is designed to perform descriptive analyses in space and time of mortality risks or rates, and to fit an extensive range of fairly complex spatio-temporal models commonly used in disease mapping. The application is built with the R package shiny and relies on the well founded integrated nested Laplace (INLA) approximation technique for model fitting and inference. Unlike other software used in disease mapping, SSTCDapp provides an user-friendly interface that facilitates the fit of complex statistical models to non-experts users without the need of installing any software in their own computers, since all the analyses and computations are made in a powerful remote server. In addition, a desktop version is also available to run the application locally if needed, which avoids uploading the data to the online application fully guaranteeing data confidentiality.";"lightning talk"
"Onkelinx Thierry";"git2rdata: storing dataframes in a plain text format suitable for version control";"Base R has write.table() and read.table() to work with data in plain text files. Factors are stored as their labels, making them indistinguishable from characters and losing information on the levels. The git2rdata packages provides write_vc() which stores a dataframe as two plain text files: a tab separated file with the data and a metadata file in YAML format. The metadata stores the class of each variable and all other relevant metadata, e.g. the factor levels and their order. read_vc() reads the raw data using read.table() and restores the variables based on their metadata. Storing the metadata also allows to optimize the file storage, e.g. by storing factor indices rather than factor labels in the data file. This optimization can be turned of in case human readable data is preferred over smaller files. Git stores changes as row wise diffs. Swapping the order of two variables results in changing every single row of the plain text file. write_vc() avoids this be reordering the variables based on the existing metadata. Sorting the observations along user defined variables result in a stable row order. Metadata can be overridden if needed to accommodate a change in variables. git2rdata is useful to store and retrieve data in a reproducible workflow. Installation instructions, documentation and vignettes are available at https://inbo.github.io/git2rdata/";"oral presentation"
"Bodner Christoph";"Serverless Computing for R";"R is a great language for rapid prototyping and experimentation, but putting an R model in production is still more complex and time-consuming than it needs to be. With the growing popularity of serverless computing frameworks that offer Functions-as-a-Service (like AWS Lambda, Azure Functions) or Container-as-a-Service (ECS and ACI) we see a a huge chance to allow R developers to more easily deploy their code into production. We want to show you how you can use serverless computing to easily put models in production. We will discuss the pros and cons of various approaches and how we implemented a completely serverless data science platform for R in Microsoft Azure that you can arbitrarily scale (as long as your credit card allows:) up and down. While we come from an Azure background, porting the ideas over to AWS or Google Cloud should be straight forward.";"oral presentation"
"Rousseau David";"Ordinal clustering of seed populations with data extracted from RGB imaging and X-ray tomography";"Population of seeds with different genotypes are imaged in X-ray tomography before imbibition and with RGB imaging during imbibition. Morphological traits characterizing the shape and volume of suborgans are computed from the 3D X-ray images. Time of germination and germination speed is determined after processing RGB images. From these data a clustering based on K-mean and generalized mixture models is produced after dimension reduction with a principal component analysis. The analyzed genotypes are ordered from empirical knowledge of their agronomical values. The possibility to recover this ordinal relationship, without supervision, with the data extracted from the images is investigated. The purity of this ordinal clustering is analyzed with new simple metrics (distance class, genotype distance and associated standard deviation) made available under R functions. Fusion of data from RGB and X-ray tomography demonstrates good performances of prediction of the agronomical value. This is obtained on 7 genotypes of sugar beet with more than 10000 seeds analyzed.  This work received support from the French Government supervised by the ?Agence Nationale de la Recherche? in the framework of the program Investissements d'Avenir under Reference ANR-11-BTBR-0007 (AKER program).  ";"poster"
"Tryputsen Volha";"Streamlining complex analyses of in-vivo data with INVIVOLDA shiny application";"In vivo studies are crucial to the development of novel therapies. In vivo data is important for proof-of-concept validation, FDA applications and clinical trials. Appropriate data analysis and interpretation are essential in providing the knowledge about the drug efficacy and safety within a living organism. With drug discovery science moving forward at an ever-accelerating rate analyses software not always capable to offer appropriate analysis suit. In vivo scientists at Janssen R&D needed comprehensive analysis tool to conduct appropriate and efficient analyses of in vivo data to insure quality and speed of decision-making. INVIVOLDA shiny application was developed to fulfill the gap. INVIVOLDA offers: powerful Linear Mixed Effect modeling for evaluating differences between treatments over time; Buckley-James method is used to infer about mean treatment differences when response is non-linear in the presence of censoring; survival analysis enables evaluation of time-to-event data. Furthermore, interactive and animated graphics allow users to conduct independent and thorough data explorations. INVIVOLDA allows streamlining complex statistical analyses of in-vivo longitudinal data by utilizing modern graphics, appropriate modelling techniques and report generation and insures efficient, traceable and reproducible in vivo research and data-driven decision making.";"lightning talk"
"Lopez-Lopera Andrés";"lineqGPR: An R Package for Gaussian Process Regression Modelling with Linear Inequality Constraints";"Introducing inequality constraints in Gaussian processes (GP) models can lead to more accurate uncertainty quantification in a great variety of applications. Based on the framework detailed in López-Lopera et al. [SIAM/ASA JUQ, 6(03): 1224-1255, 2018], lineqGPR is an R package for GP regression modelling with inequality constraints. It can be used to create GP models under boundedness, monotonicity, convexity constraints or a combination of these. Furthermore, it also allows users to design their own sets of linear inequalities. lineqGPR contains all the typical features of classic GP libraries, e.g. the parameter estimation via maximum likelihood, support for noisy observations. Finally, recent developments allow the implementation of additive GP models under linear inequality constraints leading to models that can be used to the case of thousands of observations and to high dimensions.

lineqGPR is available on CRAN as an open-source software licensed under GNU GPL-3. It is based on previous R developments produced by the Deep Inside Computer Experiments (Dice) and ReDice Consortiums (e.g. DiceKriging, DiceDesign, kergp), but incorporating structures of classic GP libraries from other platforms (e.g. the GPmat toolbox from MATLAB, and the GPy library from Python). This work was conducted within the frame of the Chair in Applied Mathematics OQUAIDO.";"poster"
"Zeileis Achim";"colorspace: A Toolbox for Manipulating and Assessing Color Palettes";"The R package ""colorspace"" (http://colorspace.R-Forge.R-project.org/) provides a flexible toolbox for selecting individual colors or color palettes, manipulating these colors, and employing them in statistical graphics and data visualizations. In particular, the package provides a broad range of color palettes based on the HCL (Hue-Chroma-Luminance) color space. The three HCL dimensions have been shown to match those of the human visual system very well, thus facilitating intuitive selection of color palettes through trajectories in this space.

Namely, general strategies for three types of palettes are provided: (1) Qualitative for coding categorical information, i.e., where no particular ordering of categories is available and every color should receive the same perceptual weight. (2) Sequential for coding ordered/numeric information, i.e., going from high to low (or vice versa). (3) Diverging for coding ordered/numeric information around a central neutral value, i.e., where colors diverge from neutral to two extremes.

To aid selection and application of these palettes the package provides scales for use with ggplot2; shiny (and tcltk) apps for interactive exploration (see also http://hclwizard.org/); visualizations of palette properties; accompanying manipulation utilities (like desaturation and lighten/darken), and emulation of color vision deficiencies.

";"oral presentation"
"Liquet Benoit";"PLS for Big Data: A Unified Parallel Algorithm for Regularized Group PLS";"Partial Least Squares (PLS) methods have been heavily exploited to analyze the association between two blocks of data. These powerful approaches can be applied to data sets where the number of variables is greater than the number of observations and in the presence of high collinearity between variables.
Different sparse versions of PLS have been developed to integrate multiple data sets while simultaneously selecting the contributing variables. Sparse modeling is a key factor in obtaining better estimators and identifying associations between multiple data sets. The cornerstone of the sparse PLS methods is the link between the singular value decomposition (SVD) of a matrix (constructed from deflated versions of the original data) and least squares minimization in linear regression. We review four popular PLS methods for two blocks of data. A unified algorithm is proposed to perform all four types of PLS including their regularised versions. We present various approaches to decrease the computation time and show how the whole procedure can be scalable to big data sets. The bigsgPLS R package implements our unified algorithm and is available at https://github.com/matt-sutton/bigsgPLS";"oral presentation"
"Lee Gi-Seop";"Integrated Operational Modeling of the Harmful Algal Blooms(HABs) Using R";" Harmful algal blooms(HABs) are critical issues on the coastal ocean. Government agencies and ocean scientists are struggling to reduce the damage of coastal aquaculture by HABs. As a part of the effort, ocean scientists are conducting researches on how to cope with HABs and how to predict it before HABs occur. In this study, a model to predict the route and range of HABs was constructed to prevent damage caused by HABs. As with most ecological models, the major parts of the HABs model including physical, chemical, and biological factors are fundamental, but technical processes for efficient operation of the model are also important. The open source language, R, has a great advantage in performing comprehensive modeling research because a wide range of libraries which are well-verified can be available. Whole modeling processes including data acquisition using API(Application Programming Interface) from government, localized physical model data input, rasterizing and computing spatial data and visualization of prediction results were integrated into the R and implemented with satisfaction. The integrated R code calculates the 72-hour forecast results everyday through the batch process.";"poster"
"Laa Ursula";"Visualising high-dimensional data: new developments of the tourr package using Shiny and plotly";"The tour is a tool for the visualisation of multi-dimensional structures by means of dynamic projections, implemented the R package tourr (Wickham et al., 2011). Availability in R means that we can readily extend it with features from other packages. In this talk I will show how we can use Shiny and plotly to create a graphical interface, enhancing usability for non-experts and allowing for interactive features like linked brushing in the tour display, stopping and restarting with new settings, or hover text information on data points. In addition I will show how index functions scoring the ?interestingness? of 2D projections, available in various R packages, can be combined with the guided tour, steering projections towards more interesting views of the data.";"oral presentation"
"Allen Brandon";"A Significant Difference: How the Institutional Research Team at the University of Chicago Booth School of Business uses R to Optimize and Institutionalize Analytics";"I propose to present how the Institutional Research team at the University of Chicago Booth School of Business has leveraged R to change the landscape of institutional decision making. The poster focuses on four areas: statistics, optimization, institutionalization, and future use. How we have used R: Statistics  Significance testing Statistical modeling:    Multiple linear regression Logistic regression Elastic net regression  Machine learning:  k-means clustering k-NN Random forests   Optimization  Codified processes that are easy to review and revise Cleaned data/engineered features Saved significant amounts of time Automated API interaction  Institutionalization  Founded the University of Chicago Staff R User Group Delivered cross-departmental and executive recommendations derived from R Published interactive dashboards that are reliant on R Engineered and led institution-wide data analysis trainings Advocated for R by collaborating with Chicago-area user groups  How we plan to use R in the future:  Continue to share production-quality code across the university Encourage and sustain further use of Shiny Further develop internal Natural Language Processing applications ";"poster"
"Dolphin Alex";"trivago: Data analysis in big business";"Introduction - trivago: Data analysis in big business
Internet business brings huge amount of data and daily challenges. This presentation is about 2 challenges we meet daily in trivago. Anomaly classification and reporting in a user-friendly format across departments for various topics. Peter Brejcak - Anomaly detection in trivago:
For us it is very important to correctly detect nonhuman traffic or detect anomalies. We are going to share our real example of an obvious anomaly with business impact:
How was the anomaly detected?
What caused the anomaly? Are we able to answer this confidently?
What action should we take?
How can, and can't R help us? Alex Dolphin - Muffin Tin: Our template in R for beautiful, lightning-fast analyses and reports:
We maintain high-quality templates for performing analyses in R and producing reports in RMarkdown using Cookiecutter (based in Python). 
We no longer spend time setting up Hadoop connections in R or creating RMarkdown from scratch. We can focus on selecting the right data, analysing, and producing great visualisations.
As a result, our analyses are readable, reproducible, and extendable, and can be run using a single command!";"lightning talk"
"Gehrke Matthias";"Teaching Statistics for Data Literacy by the means of R mosaic and R Markdown";"Data and data analysis has never been as ubiquitous as it is today. However, such data abundance has triggered a surge in the complexity of data analyses. Therefore, statistical education can and should address both the current challenges and the opportunities of data analysis more thoroughly than it is currently the case.

Educators should teach all students ""data literacy"" which has been defined as ""the ability to collect, manage, evaluate, and apply data in a critical manner"" (Ridsdale et al. 2015). But as Gould (2017) already phrased it: data literacy ""is Statistical Literacy"".

The didactical concept we present here builds on three aspects - wherever possible with real-life applications. First, we employ concepts of modeling (Stigler & Son, 2018) including simulation based inference (Chance et al. 2016) using the R package mosaic (Pruim et al. 2017). It provides a simple and consistent coding interface for graphical and numerical modeling. Second, we show our students how to render an analysis reproducibly by using R Markdown (Baumer et al. 2014). Third, we demonstrate core theoretical concepts in interactive shiny apps (Doi et al. 2016). Following Wild et al. (2018), we hope to help our students ""to be able to learn about how our world operates using data, all the while acknowledging sources and levels of uncertainty"".

In this talk we will present our experiences and a review of the lessons learned.";"poster"
"Chan Rosa";"Comparing Mechanistic and Pre-clinical Predictions of Volume of Distribution on a Large Set of Drugs";"Volume of distribution at steady state (Vdss) is a fundamental pharmacokinetic (PK) parameter and is believed to be predominantly driven by passive processes and physicochemical properties of the compound. For human prediction, Vdss can be estimated using in silico mechanistic methods or scaled from Vdss values obtained from preclinical species. A systematic comparison of these methods was performed to assess the relative predictivity with a sufficiently large data set, which is lacking in the current literature. The objective of this study is to compare preclinical and human Vdss with predictions resulting from a mechanistic in silico model based on experimental in vitro properties for a large dataset including over 150 marketed drugs. This assessment indicated that overall the in silico mechanistic model presented by Rodgers and Rowland is comparably accurate or superior to Vdss predictions based on direct proportionality between different species. While anions, neutral compounds and strong bases can be predicted with satisfactory accuracy, the data suggest that predictions for weak bases are generally poor. These results illustrate the great potential of mechanistic in silico models to predict Vdss, which do not rely on in vivo data and are, consequently, significantly time and resource saving.  ";"poster"
"Tamir Noa";"The truth about satRdays";"9 satRday events were organised since UseR! 2018. Join us for a panel discussoin with a diverse group of organisers from across the world. You will learn about why they chose to volunteer their time, how they approached satRday's mission, what they have learnt from organising the event, how satRday impacted their local R community, and more!";"oral presentation"
"Pantera Laurent";"Creation of a generic IHM embedding data processing analysis for physics experimenters developed with R, rjava, shiny, htmlidgets and plotly tools";"An important aspect in data processing is the capability to use R for an interactive data visualization. The place where interactions are the most frequent is in a web page. And Javascript is powerfull for adding behavior to a web page. To use it, all you need to do is to put the code in a HTML document and tell the browser that it is Javascript. The principal bridge between these two tools is the htmlwidgets package. In JAVA programming, JavaFX framework provides us a WebView component that can be used as an embedded web browser in a JavaFX application. This component handles most of the work of web browsing, for example in own concern, rendering the HTML content. Another advantage of embedding a web browser in the JAVA HMI is to offer the possibility to insert shiny applications and hence gaining a more complete control on the interactivity. The poster describes the structure of the computer application developped to facilitate the sharing of analysis tools of the experimentalists working on the CABRI nuclear experimental pulse reactor at the CEA/Cadarache. The software development has been realised in order to enable each experimentalist to integrate their own contributions without concerted efforts in term of HMI and OOP programming. The experimentalists only have to tweak the R scripts outside the application and edit parameters files for their HMI incorporation.";"poster"
"Acion Laura";"Insights from the recent R community development and growth in Latin America";"This talk summarizes some of the recent work that put Latin America (LatAm) in the international R user map. In January 2017, R-Ladies Buenos Aires (BA) was founded. R-Ladies BA encouraged the foundation of other R-Ladies groups in the region (e.g., Santa Rosa, Santiago). In two years, LatAm went from one to 29 R-Ladies active groups. English-only materials can be a barrier to R. Hence, LatAm R-Ladies led efforts to translate into Spanish: R for Data Science (es.r4ds.hadley.nz), R-Ladies code of conduct and policies (bit.ly/2UwP79x), and R Consortium 2017 (bit.ly/2Tpt8RR) and RStudio 2018 surveys. The creation of new regional conferences was another R-Ladies-led effort. Thanks to the R-Ladies network, LatinR (latin-r.com) got rapidly started and LatAm had its first SatRDay. Among other achievements, these events impulsed the regional community in the form of new and diverse R user groups (e.g., Rosario, Montevideo). In a nutshell, in the last two years, the LatAm R user community has seen fast growth and regrouping, mainly spearheaded by R-Ladies. This presentation details key aspects of this process and seeks to inspire other R regional communities worldwide.";"oral presentation"
"Martin Kieran";"R in Pharma: A tailored approach to converting programmers to R in an industry resistant to change";"The pharmaceutical industry has historically, overwhelmingly made use of one software package, SAS. Almost every submission made to regulatory agencies such as the FDA and the EMA have been made using SAS code, and SAS data formats. For those of us who are enthusiastic about promoting usage of R, this can lead to an uphill struggle, but things are changing. R has become more and more popular, and appetite for R usage is growing. In this talk I will discuss some of the efforts to promote R within Roche, with a focus on two particular methods: - Targeted training, focused on problems programmers have to solve every day - Tidyverse training, and a focus on functional programming - In house data and as close to real examples as possible, to show direct applications for work in R - New R packages. Multiple different packages have been developed to support different activities. In this talk I will focus primarily on diffdf, a package that is available on CRAN, which used allows detailed dataset comparison, meeting an unmet need";"lightning talk"
"Legaie Roxane";"Using advanced R packages for the visualisation of clinical data in a cancer hospital setting";"NGS-based assays for cancer testing have been increasingly adopted by clinical laboratories, especially targeted gene panels which are now commonly used to guide diagnostic and therapeutic decisions. While there are many databases for the annotation and curation of individual variants, there is a lack of off-the-shelf software for the mining of those databases.   The Pathology Department at Peter MacCallum Cancer Centre conducts variant curation using in-house software, PathOS. This database contains variant information from thousands of clinical tumour samples, from a variety of NGS assays (mostly gene panels), and a range of tumour and tissue types. Visualising variations across all clinical contexts can be used to validate important findings, improve patient treatments and refine tumour classifications.    In this talk I will show how, by using advanced R packages, such as maftools and oncoplots, we are able to interrogate our database and make sense of variants from thousands of patients across multiple cancer types. I will discuss the need for implementing such solutions into our routine curation QC process, and will describe the challenges around working with data stored in various file formats and implementing solutions into a curation system in production.  ";"lightning talk"
"Windecker Saras";"Open-access software for research: beyond data analysis";"Amidst growing recognition of irreproducibility in the scientific literature, we face concerns about the credibility of research and the reliability of decisions they inform. Open science practices, such as writing open-access software, encourage research practices that are both transparent and repeatable. Although open-source R packages for data analysis are increasingly common, proprietary software and black-box approaches are still common for many data collection and processing procedures. In this talk I will briefly present ?mixchar', a R package alternative to financially restricted ?point-and-click' methods to estimate carbon in plant litter, and discuss software development for open science in research more generally.";"lightning talk"
"Aaron Ping";"Using R to model the drivers of Mascara performance and preference";"L'Oreal regularly conducts research to evaluate how consumers feel about various cosmetic products. With this poster, we show how questionnaire-based research on mascara from 40,000 people across four markets - Japan, China, USA, and France - was used to build models of product performance in R. Two models of mascara performance were built 1) An SEM-based model built using the lavaan package, for understanding the relative impact to percieved performance from each product attribute, and 2) a BNN-based model using the bnlearn package, for simulating the potential impact to performance from upgrades affecting percieved mascara product attributes.";"poster"
"Lecoutre Eric";"Machine Learning with R: do it with a framework";"There is no doubt that R is the swiss knife for modeling activities.
Variety of families of models and implementations in numberous packages talks for itself.
Interested R users will read the Machine Learning task view and discover (some of) those packages.
This talk is not about models themselves but about frameworks. Some meta-packages indeed do not at all contain modeling functions but act as wrappers around existing assets and provide high level functionalities (cross-validation, hyperparameters grid search, staking...).
Objective is to have consistent syntax with a limited set of 'verbs' as well as providing a way to implement a modeling process.
We will introduce the reasons why such packages are so interesting for Data Scientists and present 3 solutions: caret, mlr and SuperLearner.
In addition, as modeling pipe also requires data preparation, we will also talk about vtreat, recipes (+embed), mlCPO, sl3 and their possible integration with those frameworks.
We will present some modeling flows as example and also introduce a tidy approach for modeling .";"oral presentation"
"Moradi Mohammad Mehdi";"Classes, methods and data analysis for trajectories";"Object tracking has been recently considered drastically to get
insight into the behavior of moving objects within different contexts
such as eye tracking, animal tracking, traffic analysis, city
management, sports, etc. This, however, results in a huge amount of
irregular data which are usually not well structured, depending on the
tracking methods and the aim of the study. In this work, we review
different classes and methods from R package trajectories with the aim
of structuring, handling and summarizing movement data. Simulation
techniques and model fitting are also studied. We further proceed with
some statistical characteristics based on spatial point processes such
as first- and second-order summary statistics to analyze the behavior
of objects over time in terms of distribution and pairwise
interaction. We finally demonstrate our findings of a taxi movement
data from Beijing, China.";"oral presentation"
"De Troyer Ewoud";"Applied Biclustering using the BiclustGUI R Package";"Big and high dimensional data with complex structures are emerging rapidly over the last few years. A relative new analysis method that aims to discover meaningful patterns in such data is biclustering, an approach that applies simultaneous clustering on the 2 dimensions of a data set, i.e. identifying a local pattern in the data. The BiclustGUI, a R Commander plug-in, is currently, to our best knowledge, the most complete software available for biclustering analysis and has the potential to keep growing in the future. The BiclustGUI is an envelope package that interconnects 10 biclustering analysis and visualisation packages on a single platform. The current software targets multiple audiences: the R Commander plug-in is a point-and-click solution to explore different biclustering algorithms, but will also output the relevant R code. This is ideal for both experienced and beginning R users who want to switch to or learn about the underlying R code itself. For non-R users, a shiny application was developed as well. The final target audience are developers of new biclustering methods who are given the possibility to include new methods in the GUI with minimum support from the BiclustGUI's maintainer through easy template scripts. This software development approach is based on the knowledge that the development of R software is a community-based effort and is done by many independent researchers.";"oral presentation"
"Rundel Colin";"ghclass: an R package for managing classes with GitHub";"In this talk we will present the details of the ghclass package which we have developed as a tool for managing statistical and data science courses with a computation component. The package is designed to automate the process of distributing and collecting assignments via git repositories hosted on GitHub. As part of this talk will discuss best practices for how these tools can be deployed in a variety of classroom settings from first year introductory courses through graduate level courses. Finally we will discuss how these tools fit into the larger context of modern statistical / data science pedagogy with a particular emphasis on the role of reproducibility in training new researchers.";"oral presentation"
"Chapman Chris";"choicetools: a package for conjoint analysis and best-worst surveys";"We present choicetools, a new package to work with data from conjoint analysis and best-worst (aka MaxDiff) stated choice surveys. Users may supply their own choice data, or import data from two popular survey platforms, Qualtrics (best-worst surveys) and Sawtooth Software Lighthouse Studio (conjoint and best-worst). choicetools estimates classical and hierarchical Bayesian models, performs preference share market simulation, and plots aggregate and individual-level estimates. It is also useful to teach conjoint analysis; it provides functions to create and field a survey in CSV format to be completed in a spreadsheet application in a classroom setting, and to estimate preferences from the responses. In our UseR! talk, we demonstrate a vignette style choice-based conjoint project using simulated responses, including survey specification, spreadsheet-style presentation, estimation, and plotting. We briefly discuss experimental features for attribute importance, and answer audience questions. This talk will be of interest to research practitioners in marketing, transportation, and other fields, and to academics who teach discrete choice and conjoint analysis methods.";"oral presentation"
"Friendly Michael";"Visualizing multivariate linear models in R";"This talk reviews our recent work in the development of visualization methods in R
for understanding and interpreting multivariate linear models (MLMs).

We begin with a description and examples of the HE plots framework, utilizing the
heplots package, wherein multivariate tests can be visualized via ellipsoids in 2D, 3D. HE
plots provide visual tests of significance: a term is significant by Roy's test if and only if its H
ellipsoid projects somewhere outside the E ellipsoid. 

These graphical methods apply equally to all multivariate designs: MMRA, MANOVA, and
MANCOVA, and the test of any linear hypothesis can also be displayed in an HE plot. 

When the rank of the hypothesis matrix for a term is 2 or more, these effects can also be
visualized in a reduced-rank canonical space via the candisc package, which also provides new
data plots for canonical correlation problems. These plots quite often provide a very simple
description of differences in means among groups in a MANOVA setting, by displaying the
projections of response variables as vectors in canonical space. We also describe some recent extensions of these ideas:

* We extend the visualization methods to robust MLMs.
* Influence measures and diagnostic plots for MLMs have now been implemented in the mvinfluence package.
* We develop visual tests of equality of covariance matrices for MANOVA.";"oral presentation"
"Taylor Angus";"Deploying machine learning models at scale";"Data scientists face multiple challenges when deploying machine learning models at the scale required for many commercial applications. In this talk, I will discuss best practices for deploying R models in production, including: how to operationalize predictive workflows with event- or frequency-based scheduling; how to scale model training and scoring operations with cloud-based clusters of virtual machines; how to maintain parity between development and production environments using Docker containers; the use of continuous integration and continuous delivery (CI/CD) tools to build and test production code. I will present a demo from a retail product forecasting context, in which R models are deployed in an end-to-end predictive workflow on cloud-based architecture.";"oral presentation"
"Obriant Kelly";"Advanced Git Integrations for Automating the Delivery of Reproducible Data Products in R";"We know that adopting git or code version control mechanisms are important for creating a culture of reproducibility in data science. But once you've established the basic best practices in your daily workflow, what comes next? This talk will be an introduction to continuous integration and continuous delivery tools (CI/CD). I'll cover reasons why CI/CD tools can enhance reproducibility for R and data science, showcase practical examples like automated testing and push-based application deployment, and point to simple resources for getting started with these tools in a git and GitHub based environment.  The target user base for advanced Git and GitHub integration tooling remains focused on software engineers and IT professionals. As data scientists lead the scientific community as a whole toward better, reproducible research practices, we need to be aware of the vast ecosystem of technology solutions that can benefit this mission. The specific tools I'll aim to cover in this short presentation are: GitHub webhooks, Jenkins, and Travis, all framed in terms of their use with R code and data products.  ";"oral presentation"
"Lawrence Michael";"Interfacing R/Bioconductor with Hail, a Spark-based platform for genomics";"Hail is a Spark-based framework for genomic computing at scale. We
have explored the application of deferred evaluation to the
construction of an interface between R and Hail. The interface
implements standard base R and Bioconductor APIs on top of Hail by
constructing expressions in Hail's interface language and evaluating
them using sparklyr. Users require no special knowledge of Hail or
Spark. We will describe the design of the interface and demonstrate
the manipulation of a Hail-backed SummarizedExperiment object, the
core abstraction for genomic data in Bioconductor.";"oral presentation"
"Jones Wayne";"GroundWater Spatiotemporal Data Analysis Tool (GWSDAT)";"Groundwater is located beneath the Earth's surface in soil pore spaces and in the fractures of rock formations. Environmental monitoring of this water is routinely conducted to protect human health and the environment.  The GroundWater Spatiotemporal Data Analysis Tool (GWSDAT) is a Shiny web-app developed jointly by the University of Glasgow and Shell for the analysis and visualisation of trends in environmental (groundwater) monitoring data. The app is made publicly available here: www.gwsdat.net, together with supporting information, case studies, tutorials and user manuals for engineers, regulators and practitioners active in the field of groundwater monitoring. In this presentation we will discuss the underlying software architecture and demonstrate GWSDAT functionality. In addition, we shall discuss the challenges in developing and maintaining such a large Shiny based application.";"poster"
"Ahmed Mahmoud";"colocr: an R package for conducting co-localization analysis on fluorescence microscopy images";"Background The co-localization analysis of microscopy images is a widely used technique in biology. Often used to determine the co-distribution of two proteins in the cell. The limiting step in conducting microscopy image analysis in a graphical interface tool is selecting the regions of interest. Implementation This package provides a simple straight forward workflow for loading images, choosing regions of interest and calculating co-localization statistics. Included in the package, is a shiny app that can be invoked locally to select the regions of interest interactively. Availability colocr is available on the comprehensive R archive network, and the source code is available on GitHub as part of the ROpenSci on-boarding collection, https://github.com/ropensci/colocr.";"poster"
"Irorere Dennis";"R for Data Science Online Community";"The R for Data Science (R4DS) Online Learning Community was started by Jesse Mostipak in August of 2017, with the goal of creating a supportive and responsive online space for learners and mentors to come together to provide support and guidance in learning R in a beginner-friendly, safe and supportive environment. However, the main aim of the R for Data Science community was to move through the *R for Data Science* text by Garrett Grolemund and Hadley Wickham, which walks readers through the major features of the tidyverse in the context of data science. We also aim to help users learn R and expand their R knowledge. Over time R4DS science community has been developing projects intended to help connect mentors and learners. One of the first projects born out of this collaboration is #TidyTuesday, a weekly social data project focused on using tidyverse packages to clean, wrangle, tidy, and plot a new dataset every Tuesday. Over the years, the community have been experienced a steep growth with over 2000 members from various countries in the world in our slack channel. At the R4DS community, we encourage diversity and contribution from everyone. We believe that no question is silly and no one is an island of knowledge. In this talk, we will be sharing the positive changes made since the transfer of leadership, our challenges, the lesson we have learnt and provide room for suggestion and opinions.";"oral presentation"
"Turcan Alexandra";"Data for all: Empowering teams with scalable Shiny applications";"Shiny, alongside packages like dplyr and ggplot2, offers an unparalleled developer experience for creating self-service analytics dashboards that empower teams to make data-driven decisions. However, out of the box, Shiny is not well-suited to deployment in a multi-user environment. As part of our mission to establish a data culture in a game development studio, we wanted to deploy a suite of Shiny dashboards such that exploring player behaviour became part of every team's workflow. In this talk, we will discuss the architecture of the supporting cloud infrastructure, including packaging, service orchestration, and authentication. Also, we will show how we've adapted Shiny to a multi-user environment using its new support for promises in combination with the future package. Integrating Shiny into this production-grade architecture allows for a streamlined data science workflow that enables data scientists to focus on creating dashboard content with a built-in code review process, and also to deploy changes to production in a button click. We hope to demonstrate how any data-driven organisation can augment their team-wide workflow by leveraging this end-to-end Shiny pipeline.";"oral presentation"
"Bossek Jakob";"Evolutionary Computation in R with the ecr Package";"Evolutionary Algorithms (EAs) are stochastic optimizers which mimick principles from Darwinian evolution theory in bits and bytes. A set of solution candidates, termed the population, is usually evolved in a loop of random recombination, mutation and selection operators. This broad class of algorithms has proven its effectiveness in many fields of research and applications with highly encouraging outcomes. EAs are particularly successful in tackling (combinatorial) problems with large (disrete) search spaces, little knowledge about the problem domain and/or multiple conflicting objectives (multi-objective optimization). Corresponding applications in statistical data analysis incorporates, e.~g., feature/variable selection in machine learning, model selection, cluster analysis to name a few. Basically all EAs are composed of a sequence of operators and building blocks which are evaluated repeatedly within a loop. The R package ecr provides a framework for building custom evolutionary algorithms in R. Many predefined building blocks, e.g., operators, logging procedures, facilitate the implementation of algorithmic ideas. The package has been conceived as a white-box framework which encourages to user to write the evolutionary loop by hand for maximal flexibility. This poster gives a brief overview of ecr' functionality by its lead developer.";"poster"
"Mierzwa-Sulima Olga";"Best practices for building Shiny enterprise applications";"Shiny is conquering the enterprise world. Shiny apps are now built for hundreds of users who need reliability and functionality. Having built and scaled a number of Shiny applications in production, we've learned what's important for a Shiny app to become a stunning success. We'd like to share our experiences with you. This presentation will give you a deep dive into the best practices for building Shiny production apps, including: * how to design app architecture for optimal maintainability and development velocity; * approaches for avoiding errors in production; * test pyramid: why it pays off to have automated end-to-end tests and unit tests, and how to minimize manual testing; * how to scale Shiny apps to hundreds of users and set up automated performance testing; * deployment strategies and the path from deployment, involving many manual steps, to full automation. Shiny allows you to rapidly iterate on functionality and build impressive dashboards, but it's not easy to do a great job getting that through to production. In this talk, we will discuss what specific steps you can take to achieve reliability without sacrificing speed of development.";"oral presentation"
"Aguirre V.";"BAYESDEF an R Based Graphical Interface to Analyze Definitive Screening Designs";"Definitive Screening Designs (DSD) are a class of experimental designs that have the possibility to estimate linear, quadratic and interaction effects with relatively little experimental effort. The linear or main effects are completely independent of two factor interactions and quadratic effects. The two factor interactions are not completely confounded with other two factor interactions, and quadratic effects are estimable. The number of experimental runs is twice the number of factors of interest plus one. In this work we present an R based graphical interface that performs a sequential Bayesian analysis of DSD's. In the first step only main effects are considered, then in the next steps quadratic an interaction effects are added. The decision rules consider an effect as significant if the posterior odds that the effect is active is grater that some predetermined value. The procedure is calibrated so that the overall experiment wise error rate is kept at 10%. The programming of the interface makes use of the gWidgets package.";"poster"
"Hayes Alex";"Taking the guesswork out of prediction: the tidymodels approach";"After you fit a model in R, you will likely want to predict outcomes for new data. Since every package handles prediction differently, this is often a painful process. In this talk I'll present the new tidymodels API specification for prediction and show how it will keep you from tearing your hair out. We have implemented this API in the extensively tested safepredict package, and will demonstrate workflows that emphasize uncertainty calculations and integration with other tidymodels packages.";"lightning talk"
"Allard Tania";"Everything star wars has taught me about data science";"Every day we interact with data, which is collected in a number of ways and feeds thousands of algorithms (or machine learning models). Never has machine learning or data science had this much potential for good, improvement of services and processes or marginalisation of specific populations or groups. At the same time, research now mostly depends on software and automated (or semi-automated) data analyses to generate knowledge. This places a huge responsibility on the researchers and data scientist to ensure that their work is transparent, robust an reproducible. In this talk, we will dive into techniques to ensure your data work complies with these standards, whether it is at an R&D stage or in a production environment, serving customers and consumers. I will use inspiration from movies and characters from Star Wars to present the concepts of this talk to make this more memorable The outline looks something like this: - Introduction (me and the topic) - Shared understanding: what do reproducibility and transparency even mean?  - Early stages of a project: what to consider for a reproducibility first approach - Development stages: keeping your code and data quality as they evolve - Data lineage and provenance: what are these and why do they matter? - My model is ready to ship/be published, what now? - Standing the weight of time: is my project sustainable? robust? is it decaying? - Summarise the main points ";"lightning talk"
"Nowosad Jakub";"How to win friends and write an open-source book";"Over the last few years, a quiet revolution has been brewing in the R book publishing industry. Since the publication of early open source books, such as Advanced R (published in 2014), many authors have switched to a hybrid system, in which books are published online and in print. This approach has several advantages, including (1) people can choose how to read the book, online or in print, and make an informed decision before buying it; (2) the book retains the process of reviews and professional copy-editing provided by publishers; (3) the wider community can contribute, leading to many improvements in the code and text. Several developments have enabled this revolution, including the bookdown package, online version control services (e.g., GitHub), continuous integration services (e.g., Travis CI), and many more. This talk will share our experience of writing the open source book Geocomputation with R (https://geocompr.github.io/). We will show how to start writing an open-source book, which tools are useful, and how to collaborate on a large project while being a continent apart. We will also point out a few issues in the process, and how we navigated them. Overall, we advocate writing an open source book and hope the talk will be useful to others thinking about starting such a project.";"oral presentation"
"Harner Jim";"xstatR: an Environment for Running R and XLISP-STAT in Docker Containers";"R is the lingua franca of statistical computing, but it lacks a strong API for interactive, dynamic graphics. Shiny and ggvis are excellent, but they do not support dynamic actions (brushing, geodesic rotations, etc.). XLISP-STAT and R were the dominant computing and graphics platforms in the nineties and early 2000s, but it became clear that only a single open source platform was viable and the community chose R. However, it is now common to use multiple platforms, e.g., R and Spark or R and Python. xstatR in an environment that combines R and XLISP-STAT within a single Docker container (https://github.com/jharner/xstatR). R and XLISP-STAT run as separate Linux processes in a container with a bridge between them, which allows saved R objects to be translated into Lisp objects (or vice versa). Models typically are built in R and exploratory and diagnostic dynamic plots are created in XLISP-STAT. Since XLISP-STAT also supports windows, menus, etc., a graphical interface has been developed which obviates the need for users to learn Lisp. Prototype XLISP-STAT packages have been built for model-based interactive diagnostic plots, multivariate visualizations, and GGobi-type dynamic graphs. The xstatR container can be deployed locally or to any cloud service, e.g., AWS. We are refactoring xstatR to run XLISP-STAT as a subprocess of R, which will allow XLISP-STAT to more-directly access R objects.";"oral presentation"
"Alfons Andreas";"Robust mediation analysis using the R package robmed";"Mediation analysis is one of the most widely used statistical techniques in the social and behavioral sciences. In its simplest form, a mediation model allows to study how an independent variable (X) affects a dependent variable (Y) through an intervening variable that is called a mediator (M). Such an analysis is often carried out via a pair of regression models, in which case the indirect effect of X on Y through M can be computed as a product of coefficients from those regression models. The standard test for this indirect effect is a bootstrap test based on ordinary least squares (OLS) regressions. However, this test is very sensitive, e.g., to outliers or heavy tails, which poses a serious threat to empirical testing of theory about mediation mechanisms. The R package robmed implements a robust test for mediation analysis based on the fast and robust bootstrap methodology for robust regression estimators. This procedure yields reliable results for estimating the effect size and assessing its significance, even when the data deviate from the usual normality assumptions. In addition to simple mediation models, the package also provides functionality for mediation models with multiple mediators as well as control variables. Furthermore, the standard bootstrap test and other proposals are included in the package. We will demonstrate the use of package robmed in an example from management research.";"oral presentation"
"Berel Dror";"Bioc2mlr: R package to bridge between Bioconductor's S4 complex genomic data container, to mlr, a meta machine learning aggregator package.";"Bioconductor's S4 data containers for genomic assays are popular, well established data structures. Their data architecture facilitates the application of common analytical procedures and well established statistical methodologies to large assay data. They are extensible to encompass new emerging technologies and analytical methods.

However, the S4 system enforces strict constraints on the data and these constraints raise barriers for interoperability and integration with software and packages outside of Bioconductor's repository.

Mlr is a comprehensive package for machine learning. It aggregates hundreds of supervised and unsupervised models and facilitates analytics such as resampling, benchmarking, tuning, and ensemble. The mlrCPO package extends mlr's pre-processing and feature engineering functionality via composable Preprocessing Operators (CPO) 'pipelines'.

Bioc2mlr is a compact utility package designed to bridge between these approaches. It deploys transformations of SummarizedExperiment and MultiAssayExperiment S4 data structures into mlr's expected format. It also implements Bioconductor's popular feature selection (filtering) methods used by limma and others, as a CPO. The vignettes present comparisons to the MLInterfaces package, which aims to achieve similar goals, and presents workflows for popular public datasets such as curatedTCGAData.";"oral presentation"
"Spinielli Enrico";"R in the Air";"Aircraft trajectories are becoming more available both publicly via ADS-B data crowd-sourced by aviation enthusiasts and non-profit organisations such as OpensSky Network and ADSBexchange, and also commercially via platforms such as FlightRadar24 and Flightaware.
The trrrj R package supports import, export and analysis of aircraft trajectories (4D: 3D plus time) in the various phases of a commercial flight.
The package also provides support for spatial analysis and plotting horizontal and vertical profiles.
We present a real use case of trrrj application for arrival flights to several European airports by dealing with the assessment of inefficiencies (time, distance flown, fuel/CO2 emission) related to holdings.";"lightning talk"
"Lovelace Robin";"R for Transport Planning";"Since the first release of R on CRAN, in 1997, its use in many fields has grown rapidly. Lai et al. (2019), for example, suggest that more than 50% of research articles published in Ecology use R in some way. Much like many ecological datasets, transport data tend to be large, diverse and have spatial and temporal attributes. Unlike Ecology, Transport Planning has been a slow adopter of R, with a much lower percentage of papers using the language. This raises the question: why? After exploring this question, in relation to dominant transport planning software products, the talk will sketch of what an open source transport planning 'ecosystem' could look like. Based on my own experience, of developing the stplanr package and teaching practitioners, the talk will discuss the importance of building ?communities of practice', for transport planners making the switch to R. These observations relate to others promoting R in new environments, and link to the wider question of how to advocate for open source software in wider society. Lai, J., Lortie, C.J., Muenchen, R.A., Yang, J., Ma, K., 2019. Evaluating the popularity of R in ecology. Ecosphere 10.

";"oral presentation"
"Oladokun Joseph";"R support for Public Health and Bioinformatics";"Data is essential to reliable and valid public health research. However, data from studies and surveys will only be useful if used, analyzed, and applied in a timely manner. Data can be used to evaluate program impact, to determine appropriate public health interventions, to monitor progress, to determine populations to target for an intervention, to determine barriers to care, and to influence public policy. Some of the packages that support public health and bionformatics include but not limited to Bioconductor, genetics, gap, and pheatmap.  ";"poster"
"Rasmussen Luke";"RCocoa and R.NET ? A Consistent Interface Across Development Environments";"This talk will cover the development of RCocoa (https://github.com/StatTag/rcocoa), a library written in Objective-C that provides a bridge to R. Similar to existing libraries such as RInside for C++ and R.NET for C#, RCocoa integrates the R engine into an application written in Objective C. RCocoa enables developers to more easily port Windows applications written with R.NET to macOS. We wrote RCocoa as part of our development of StatTag (http://stattag.org), an open source, user-friendly program to support reproducible research. Due to design considerations, StatTag was developed for Windows in C# using R.NET, and in Objective-C for macOS. RCocoa allowed us to maintain a consistent interface to R across both platforms. We will discuss the development and use of RCocoa, focusing on similarities to R.NET. Using our development of StatTag as an exemplar use case, we will describe how other developers may incorporate RCocoa into their own projects. ";"poster"
"Hogervorst Roel";"network analyses for the masses; tidygraph talks neo4j now";"For the casual network analist / data scientist working with network data is a pain, because the manipulating is so unlike rectangular data. Luckily there is tidygraph to help us. But what if you have networks on larger scales? That's where graph databases come in, in this talk I will show how I extended the 'tidygraph' package to work with neo4j. Neo4j is a graph database in wide use in industry. With tidygraph you can select and filter neo4j graph properties with familiar verbs before bringing them to your computer for further analysis. This work is build on Thomas Lin Pedersen's 'tidygraph' package and Colin mcFay's work on 'neo4r'. ";"oral presentation"
"Wirhaspati Novia Listiyani";"Optimization in R: Black-Box Optimization for Problem with Time Dimension";"There are many examples where we can only evaluate without explicit equation. In such cases, black-box optimization has enormous potentials. We demonstrate its capability in the business world - especially in the case of purchasing item(s) with dynamic pricing. Roughly speaking, we need to adjust the amount of money spent and its allocation within the next few periods. Prophet is employed to be the simple-yet-scalabale black-box function, through which we can predict future cost of purchased item(s) given amount of money spent. Denote that the objective of this problem is to minimize the total of cost per purchased item(s). The black-box function is optimized using SACOBRA package, where the objective comes with business constraints such as minimum purchase level, maximum cost to pay, etc. SACOBRA uses radial basis function (RBF) to interpolate objective function and constraint(s). More importantly, it can repair solution where constraint(s) is violated ? that SACOBRA can perform well against business problem and its numerous constraints. The result in implementing SACOBRA is good. Furthermore, for 3 different items in the next 7 days, it takes around 6 hours to converge. This initial study strongly indicates that SACOBRA can revolutionize decision making process in the business world as it allows us to get the optimal decision within the feasible region by only providing black-box function.";"oral presentation"
"Dai Zhuo Jia";"You don't need Spark for this - larger-than-RAM data manipulation with disk.frame";"R is blessed with the very best data munging tools such as dplyr and data.table. However, R requires data to be loaded to RAM in the form of a data.frame; and a corollary of this is that R cannot deal with larger-than-RAM data easily. This talk introduces the disk.frame package which is designed to manipulate ""medium"" data - those datasets that are too large to fit into RAM but can be manipulated on a single machine with the right tools. In a nutshell, disk.frame makes use of two simple ideas to make larger-than-RAM data manipulation feasible * split up a larger-than-RAM dataset into chunks and store each chunk in a separate file inside a folder and
* provide a convenient API to manipulate these chunks Furthermore, disk.frame optimises on-disk data manipulation * by using state-of-the-art data storage format provided by the fst package to efficiently read and write data to disk
* by parallelizing operations with the excellent future package
* by using the data.table package's fast algorithms for grouping and merging Finally, disk.frame supports many dplyr-verbs to make it accessible to useRs who are already familiar with dplyr.";"oral presentation"
"Zauchner Clemens";"Level up your tables with tableHTML in R";"In data science, sharing results to enable data driven decisions is vital. There are great data visualisation packages like ggplot2 or matplotlib, but there is a need to show data in tables. When writing a report or creating a shiny app, you would need a tool that is intuitive and easy to use while being flexible. On top of that, the results should look good. tableHTML was developed to do just that. It's a tool to create HTML tables from your dataframe or matrix objects in R and style them with CSS. These tables can be exported and used in any application that accepts HTML (e.g. shiny, RMarkdown, MS Office, Emails,...). This talk will introduce tableHTML, how it is used and how it integrates to the data science workflow.";"poster"
"Kowarik Alexander";"unconfUROS and one of its outputs vornoiTreemap";"The annual conference on the use of R in official statistics is since 2018 enriched by a side event similar to the rOpenSci Unconfs. Similar to the idea of the awesome official statistics software list, this should complement an abstract top-down approach such as the common statistical production architecture with fast bottom-up solutions. One of the implemented ideas 2018 was to provide an easy way to generate Voronoi treemaps in R. The motivation came from examples of the ""Price Kaleidocscope"" from the federal statistical office of Germany. The newly released CRAN package voronoiTreemaps provides the functionality to create Voronoi treemaps with the JavaScript library d3-voronoi-treemap and to integrate them into a shiny app.";"lightning talk"
"Peter Balazi";"The transition from conventional tools in banking to R";"Presenter: Peter Balazi Focus: Historical and future usage of R in credit risk modeling and its application Abstract: The highly regulated commercial banking industry has been slow and in general averse to change. To blame is the system legacy, processes but also managerial unwillingness. Historically and to these days, the commercial banking industry, especially risk management area, has been dominated by large software companies, SAS and alike. However this is changing rapidly, mainly led by the inner initiative of software end users. Now also increasingly recognized by management, of a need to change and adapt to the demand of end-users by realizing its potential and align to market place to attract talent. This presentation will briefly discuss the historical, current and future direction of a transition from typical conventional commercial type of statistical software (SAS) to R. Practical industry examples will be provided/shown where the choice of the statistical language is crucial and/or preferred in achieving the desired output giving the existing constrains. Brief view into this industry and best practice will be provided and encouragement given for R user community that better days lay ahead. Welcome for cooperation and sharing of knowledge will be also offered for those interested more details provided during the break in 1:1 session while approach in person.  ";"lightning talk"
"Chan Chung-Hong";"Die Nutella oder Das Nutella? Grammatical Gender Prediction of German Nouns";"One of the big challenges of learning German is determining the grammatical gender of German Nouns (Genus / grammatisches Geschlecht, e.g. der Löffel, die Gabel, das Messer). For light-hearted, the rules seem to be pretty arbitrary. In this talk, I am going to show how I created a quite (or not quite) accurate model to predict the grammatical gender of German nouns using the R package keras. During the development process, I have encountered some interesting problems about the German Language and machine learning in general.";"lightning talk"
"Sachs Michael";"Machine learning for censored event history data using pseudo-observations";"There are many powerful implementations of machine learning procedures in R, yet many of them are focused on binary or continuous outcomes. Censored event history data is common in biomedical research, and pseudo-observations is a powerful approach to modeling possibly censored event history data with or without competing risks. Pseudo observations replace incompletely observed event history processes with jackknifed nonparametric estimates of marginal estimands so that standard regression methods can be used to estimate conditional quantities. The calculation of pseudo observations is a data pre-processing step that allows any and all machine learning procedures to be used for censored time to event outcomes with or without competing risks. We develop a suite of loss functions that are useful targets in the context of prediction of time to event outcomes, such as the area under the time varying ROC curve, and illustrate how they can be used with current R packages for machine learning. We also show how to evaluate the performance and operating characteristics of the estimated models also using the pseudo observations to provide unbiased estimates of quantities such as the cumulative cause specific incidence and restricted mean survival. ";"poster"
"Matthias Kaeding";"RcppGreedySetCover: Scalable Set Cover";"The set cover problem is of fundamental importance in the field of approximation algorithms: Given a collection of sets, find the smallest sub-collection, so that all elements from a universe are covered. A diverse range of real world problems can be represented as set cover instance such as location-allocation, shift planning or virus detection. An optimal solution to the problem via linear programming is available. Due to the computational costs involved, this is not a feasible solution for large problems. A quick approximation is given by the greedy algorithm. This talk introduces RcppGreeySetCover, an implementation of the greedy set cover algorithm using Rcpp. The implementation is fast due to the reliance on efficient data structures made available by the Boost headers and the C++ 11 standard library. Preprocessing of input data is done efficiently via the data.table package. Input and output data is a tidy two column data.frame.  As a case study, we apply the package on a large scale (>= 100 million rows) hospital placement problem, which initially motivated the creation of the package.";"lightning talk"
"Rey Jean-François";"R package development using GitLab CI/CD pipeline";"Nowadays computer project management tools are widely widespread, many of them, provide a specific part of project life cycle management or are third-parties solutions.
Here we introduce an open source and free solution to manage R packages developments and deliveries. This poster presents the GitLab Community Edition self-hosted, a web-based Git-repository and projet life cycle management. We focus in it continuous integration and delivery pipeline part, using Docker and VirtualBox, for R packaging.
This local solution uses in our laboratory allows us to develop R codes in a collaborative mode and to automate R packages checking and building (archives and binaries).
This process allows us to share private and/or in development R packages on multiples OS and to accelerate CRAN submission by decreasing checking error. Many things can be improve to scale all the processes, that is the main purpose for the perspectives in the poster.";"poster"
