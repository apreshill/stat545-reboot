# Using an API to get data {#use-api}

In the simplest case, the data that you want is already available on the internet in a familiar format such as a csv and you can use `readr::read_csv()` to load it directly into R.

Often times, however, things are a bit more complicated. You might need to go through an [**Application Programming Interface**](https://en.wikipedia.org/wiki/Application_programming_interface#Web_APIs) (API) to get the data that you want.

An API allows you (the client) to programmatically get resources (e.g. a csv file, html file, etc.) from a web server via a [**Hypertext Transfer Protocol**](https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol) (HTTP) request. It is essentially facilitating a conversation: you send the web server a request and then the server sends you back a response. This response can either be the data you asked for or an error message.

![Image Source: [What exactly IS an API?](https://medium.com/@perrysetgo/what-exactly-is-an-api-69f36968a41f)  by Perry Eising](img/api-diagram.png)

If an API is popular, it might have an API wrapper package that will handle the HTTP part for you. If not, then you will need to handle the HTTP part yourself. This isn't as bad as it may seem! The [`httr`][httr-web] package (on [CRAN][httr-cran], on [GitHub][httr-github]) can help make this easier.

If you are interested in learning more about APIs and HTTP check out Chapter \@ref(api-resources) for some good places to start.


## Install-and-play {#api-wrappers}

Many popular APIs have helper R packages that "wrap" around the original API. These packages generally will have functions that assist with formatting a query, sending the query to the web server, and parsing the response sent back.

The first thing that you should do if you're planning on using an API to get data is to see if there is already an API wrapper package that exists. Try searching the API name on CRAN and GitHub. If there is an API wrapper package available, you should definitely try to use it. 

Here are a few popular API wrapper packages for APIs that you may have heard of (see Chapter \@ref(api-resources) for more):

* [`rtweet`](https://rtweet.info) for the Twitter API (on [CRAN](https://CRAN.R-project.org/package=rtweet), on [GitHub](https://github.com/mkearney/rtweet))
* [`gutenbergr`](https://docs.ropensci.org/gutenbergr/) for Project Gutenberg's API (on [CRAN](https://CRAN.R-project.org/package=gutenbergr), on [GitHub](https://github.com/ropensci/gutenbergr))
* [`gistr`](https://docs.ropensci.org/gistr/) for GitHub gists (on [CRAN](https://CRAN.R-project.org/package=gistr), on [GitHub](https://github.com/ropensci/gistr))

### Exploring Density Data on Dryad Digital Repository with `rdryad` {#dryad-wood}

<!--There is a note attached to this dataset asking that Lopez-Gonzalez be notified if the dataset is used in publication--> 

The [Dryad Digital Repository](https://datadryad.org) contains datasets from scientific publications that are available to download and explore. From searching "Dryad Digital Repository R package" on google I found the [`rdryad`][rdryad-web] package (on [CRAN][rdryad-cran], on [GitHub][rdryad-github]) that is a wrapper for the Dryad Digital Repository API.

The dataset that we will explore is the [Global wood density database](https://datadryad.org/resource/doi:10.5061/dryad.234/1), which is data from a [paper](https://onlinelibrary.wiley.com/doi/full/10.1111/j.1461-0248.2009.01285.x) that was published in 2009:

> Chave J, Coomes DA, Jansen S, Lewis SL, Swenson NG, Zanne AE (2009) Towards a worldwide wood economics spectrum. Ecology Letters 12(4): 351-366. https://doi.org/10.1111/j.1461-0248.2009.01285.x

The dataset contains the wood density (g/cm^3) values for over 16,000 samples of trees from various regions around the world. Let's pull down the dataset into R using the `rdryad` package and check out the distribution of the wood density values for trees in Australia, Europe, India, Madagascar, and Mexico.

The first step is to install/load the `rdryad` package.
```{r}
#install.packages("rdyrad")
library(rdryad)
```

We will use two functions from `rdryad`: [`dryad_files()`](https://docs.ropensci.org/rdryad/reference/dryad_files.html) and [`dryad_fetch()`](https://docs.ropensci.org/rdryad/reference/dryad_fetch.html). 

`dryad_files()` requires one argument, `doi`, the DOI ([Digital Object Identifier](https://en.wikipedia.org/wiki/Digital_object_identifier)) for a Dryad dataset as a string. The `doi` needs to have the form "10.5061/dryad.xxx". It returns a url that can be used to download the dataset.

`dryad_fetch()` requires one argument, `url`, which is the output of `dryad_files()`. When called it will download the dataset located at the url and saves it as a temporary file. It returns a named list where the name corresponds to the url and the value corresponds to the temporary file location.

We can find the DOI for the for the dataset we're interested on the [webpage for the record](https://datadryad.org/resource/doi:10.5061/dryad.234/1). Next to DOI is "https://doi.org/10.5061/dryad.234/1". This is not quite the format that `dryad_files()` requires so let's change this to "10.5061/dryad.234". Pass it to `dryad_files()` to get the download url for the dataset.

```{r}
wood_url <- dryad_files(doi = "10.5061/dryad.234")
wood_url
```

Great! Now let's use `dryad_fetch()` to download the dataset into a temporary file.

```{r}
fetch_output <- dryad_fetch(url = wood_url)
fetch_output
```

I'm going to pull out the file path and save it as `file_path` just to make things cleaner.
```{r}
file_path <- fetch_output[[1]]
file_path
```

All that's left now is to read in the temporary file into R. The [webpage for the record](https://datadryad.org/resource/doi:10.5061/dryad.234/1) tells us that the file is an xls file. We can use the `read_xls()` function from the [`readxl`][readxl-web] package (on [CRAN][readxl-cran], on [GitHub][readxl-github]) to load the file into R. Before that, however, let's use the `excel_sheets()` function from the same package to find out how many sheets there are. 

```{r}
library(read_xls)

excel_sheets(file_path)
```

There are three sheets in the file. Let's load in the sheet called "Data" by using `sheet = "Data"` when calling `read_xls`.
```{r}
wood_data <- read_xls(file_path, sheet = "Data")
```

Now let's check out what we have!
```{r message = FALSE}
library(dplyr)
glimpse(wood_data)
```

Oh wow, those columns names are pretty messy. I'm going to use the [`clean_names()`](https://www.rdocumentation.org/packages/janitor/versions/1.2.0/topics/clean_names) function from the [`janitor`](janitor-web) package (on [CRAN][janitor-cran], on [GitHub][janitor-github]) to help tidy these up. `clean_names()` takes a data frame, `dat`, and returns it back with the column names cleaned up. The column names are changed to [Snake case][wiki-snake-case] by default, but you can change with the `case` argument. Check out the [`clean_names()`](https://www.rdocumentation.org/packages/janitor/versions/1.2.0/topics/clean_names) documentation to see all the possible values for `case`. For now, let's just stick with snake case.

```{r}
#install.packages("janitor")
library(janitor)

wood_data <- wood_data %>% 
  clean_names()

glimpse(wood_data)
```

Looks good! Next let's filter the data so we only keep observations that are in Australia, Europe, India, Madagascar, and Mexico. We can do this by using `filter()` from `dplyr` on the `region` column. 

Matching for so many values would require a lot of repetitions of `filter(region == __)`. We can do the same thing by using the `%in%` operator. To use it here let's first save all of the `region` values that we want to keep as a vector called `region_values`. Now we can filter for rows where `region` matches any one of the values in `region_values` with `region %in% region_values`. You can pull up the documentation for `%in%` by calling `?`%in%`` in your console. 

Here is the `%in%` operator in action:
```{r}
region_list <- c("Australia","Europe", "India", "Madagascar", "Mexico")

wood_filtered <- wood_data %>% 
  filter(region %in% region_list)

glimpse(wood_filtered)
```

Now let's visualize the distribution of wood density values of by using the `geom_boxplot()` layer in `ggplot2`.

```{r}
library(ggplot2)

ggplot(wood_filtered, aes(region, wood_density_g_cm_3_oven_dry_mass_fresh_volume)) +
  geom_boxplot() +
  labs(x = "Region", y = "Wood Density (g/cm^3)", title = "Distributions of Wood Density Values for Trees in 5 Countries",
       caption = "Data Source: https://doi.org/10.5061/dryad.234")
```

Interesting. The trees in Australia had the highest mean wood density while the trees in Europe had the lowest mean wood density. If you want to keep going try visualizing the distribution of wood density values for other regions included in the dataset.


### References

Chave J, Coomes DA, Jansen S, Lewis SL, Swenson NG, Zanne AE (2009) Towards a worldwide wood economics spectrum. Ecology Letters 12(4): 351-366. https://doi.org/10.1111/j.1461-0248.2009.01285.x

Zanne AE, Lopez-Gonzalez G, Coomes DA, Ilic J, Jansen S, Lewis SL, Miller RB, Swenson NG, Wiemann MC, Chave J (2009) Data from: Towards a worldwide wood economics spectrum. Dryad Digital Repository. https://doi.org/10.5061/dryad.234



## Interact with an API {#api-query}

In Chapter \@ref(api-wrappers) we experimented with several packages that "wrapped" APIs. API wrapper packages handle the creation of the request and the parsing of the output. What do you do if you want to use an API in R that doesn't already have a wrapper package written for it? One option is using the [`httr`][httr-web] package (on [CRAN][httr-cran], on [GitHub][httr-github]) to help out with the HTTP requests and responses. 

The [Getting started with `httr`](https://httr.r-lib.org/articles/quickstart.html) vignette is a good place to start if you haven't used the `httr` package before. 

The main functions in `httr` that we will be using are:

* [`GET()`](https://httr.r-lib.org/reference/GET.html) - sends a request to a `url` with `config` settings that you specify and returns a `response` object
* [`content()`](https://httr.r-lib.org//reference/content.html) - returns the body of a response object (more on this later)

A `response` object contains all of the information associated with a request. You can pull up the documentation for `response` by calling `?response` in your console. Some of the fields that a `response` object will have include:

* `url` - where the request was sent (i.e. the url passed to `GET()`)
* `status_code` - the HTTP status code request/response
* `header` - a named list of the headers in the response 

We will talk more about the specifics of all of this when we walk through an example of using `httr` to query the [Open Movie Database](http://www.omdbapi.com) (OMBD) API in the following section.

Before that, however, there is one more layer to querying an API that we need to address: the format of the response sent back by the web server. A good API wrapper package will spare you the pain of parsing the response and hand you the data all neat and cleaned up. We won't be as lucky this time around. 

The two most common formats that you will encounter are [JSON](https://www.json.org) and [XML](https://en.wikipedia.org/wiki/XML), which stand for JavaScript Object Notation and Extensible Markup Language respectively. JSON is a ["lightweight data-interchange format"](https://www.json.org) and XML is a data format and also a markup language. Both are used to store and transfer data on the web. There is the [`jsonlite`][jsonlite-cran] package for handling JSON in R and the [`xml2`][xml2-web] package for handling XML in R.

We will be focusing mainly on XML in this chapter but I recommend starting with [JSON vs XML](https://restfulapi.net/json-vs-xml/) and [json.org](https://www.json.org) to learn more about both. 

Next up: using `httr` to query the [Open Movie Database](http://www.omdbapi.com) (OMBD) API and `xml2` to parse the response!

### Getting movie data with the OMDb API

The [Open Movie Database](http://www.omdbapi.com) (OMDb) is an API that you can use to get data about movies and movie posters. <!--CC BY-NC 4.0-->We won't be using an API wrapper package to get data via this API so we will need to read the OMDb documentation to find out how to use it. 

Looking through [omdbapi.com](http://www.omdbapi.com) we find out that in order to interact with this API we will need to request an API key. An API key is used to track how an API is being used. It is a secret, unique identifier that is associated with a request(s). They are often used to limit the number of requests that someone is sending. For example, the OMDb API has a 1,000 daily request limit. You can read more about API keys in the `httr` vignette [Managing secrets](https://httr.r-lib.org/articles/secrets.html).

Request a free API key for the OMDb API here: <http://www.omdbapi.com/apikey.aspx> 

Check your email and follow the instructions to activate your key. This email should include your key, which will be a mix of letters and digits.

Now that you have an API key, the next step is to put it somewhere that you (or an API wrapper package, etc.) can access. Your API key should be kept secret so we need make sure we put it somewhere safe. One option is to stash it in your .Renviron file. Be sure to save it as something memorable, like `OMDB_API_KEY`. You can use the `edit_r_environ()` function from the `usethis` package to add it to your `.Renviron` file.

```r
library(usethis)

usethis::edit_r_environ() # edit your user level.Renviron file.
```
This will open up your .Renviron file in a new tab. Add `OMDB_API_KEY=<your-secret-api-key>` on a new line (be sure to add a new line after this).

Save the file and restart your R session (Session -> Restart R). You can now access your key whenever you want by calling `Sys.getenv("OMDB_API_KEY")`. __Note!__ If you are using GitHub you may want to add `.Renviron` to your `.gitignore` file so you don't accidentally push the key that you just took the time to hide.





* brief review of available functions
* pull down Harry Potter and the Prisoner of Azkaban the movie poster
* pull down Harry Potter and the Prisoner of Azkaban metadata
  + returns an xml doc
* main xml2 functions
* parse xml response


```{r links, child="links.md"}
```



