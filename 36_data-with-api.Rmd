# Using an API to get data {#use-api}

<!--[Link to original content](https://stat545.com/webdata02_activity.html)-->
<!--[Link to original content](https://stat545.com/webdata03_activity.html)-->

## Data on the web

Many times, the data that you want is not already organized into one or a few tables that you can read directly into R. More frequently, you find this data is given in the form of an API. 

**A**pplication **P**rogramming **I**nterfaces (APIs) are descriptions of the kind of requests that can be made of a certain piece of software, and descriptions of the kind of answers that are returned. 

Many sources of data -- databases, websites, services -- have made all (or part) of their data available via APIs over the internet. Computer programs ("clients") can make requests of the server, and the server will respond by sending data (or an error message). This client can be many kinds of other programs or websites, including R running from your laptop.


## Click-and-Download

In the simplest case, the data you need is already on the internet in a tabular format. There are a couple of strategies here:

* use `read.csv` or `readr::read_csv` to read the data straight into R.

* use the command line program `curl` to do that work, and place it in a Makefile or shell script.

The second case is most useful when the data you want has been provided in a format that needs cleanup. For example, if the data you are interested in is available as Excel sheets, the safest option in this case is to download the `.xls` file and then read it into R with `readxl::read_excel()` or something similar. An exception to this is data provided as Google Spreadsheets, which can be read straight into R using the `googlesheets` package (on [CRAN][googlesheets-cran], on [GitHub][googlesheets-github]).


### Grizzly Bear Deaths in British Columbia {#grizzly-bears-1}

The [British Columbia Data Catalogue](https://catalogue.data.gov.bc.ca/dataset) has a dataset of [human-caused grizzly bear deaths](https://catalogue.data.gov.bc.ca/dataset/history-of-grizzly-bear-mortalities) recorded in British Columbia between 1976 and 2017. The dataset is licensed under [Open Government License - British Columbia](https://www2.gov.bc.ca/gov/content/data/open-data/open-government-licence-bc). You can download the csv file by going [here](https://catalogue.data.gov.bc.ca/dataset/history-of-grizzly-bear-mortalities/resource/c5fc42c7-67d3-4669-b281-61dc50fdef22) and clicking the Access/Download button. Save this file somewhere in your current directory. I'm going to save it in `data/`. 


We can use the `read_csv()` function from the `readr` package to read in the file.  
```{r}
library(readr)

bears <- read_csv("data/grizzlybearmortalityhistory_1976_2017.csv")
```

Let's check out what we have.
```{r message = FALSE}
library(dplyr)

glimpse(bears)
```

<!--After looking at some of the other grizzly bear data that BC Data Catalogue has available I was able to find out that `GBPU` stands for "grizzly bear population unit" and `MU` stands for management unit (used to separate different areas of land).-->

The notes online mention that there is some inconsistency for the records with `KILL_CODE` values of "Pick Up", "Rail Kill", and "Road Kill", so let's focus only on those classified as "Hunter Kill", "Animal Control", and "Illegal". We can filter to retain only those records by using `dplyr::filter()` and then count how many observations we have for each unique year and cause of death using `dplyr::count()`.
```{r}
bears_proc <- bears %>% 
  filter(KILL_CODE %in% c("Hunter Kill", "Animal Control", "Illegal")) %>% 
  count(HUNT_YEAR, KILL_CODE)

bears_proc %>% 
  head()
```

We can use these counts to plot the number of recorded grizzly bear deaths for each `KILL_CODE` category over time.
```{r fig.width = 10, fig.height = 9}
library(ggplot2)

bears_plot <- ggplot(bears_proc, aes(HUNT_YEAR, n, color = KILL_CODE)) +
  geom_point() +
  geom_line() +
  theme_minimal() +
  labs(x = "Year", y = "Grizzly Bears Killed", 
       title = "Grizzly Bear Deaths Caused by Humans", 
       subtitle = "British Columbia, 1976-2017",
       caption = "Data Source: https://catalogue.data.gov.bc.ca") +
  scale_color_discrete(name = "Cause of Death")

bears_plot
```


We can take this plot a step further by adding in some contextual information that is included in the notes for this dataset in the BC Data Catalogue. 
```{r fig.width = 10, fig.height = 9}
bears_plot2 <- bears_plot +
  geom_vline(xintercept = 1996, color = "black", 
             linetype = "dashed", alpha = 0.5) +
  annotate(geom = "text", x = 1996.25, y = 395, 
           label = "1996:\nLimited Entry\nHunt Instituted", 
           size = 3, hjust = 0) +
  geom_vline(xintercept = 2001, color = "black", 
             linetype = "dashed", alpha = 0.5) +
  annotate(geom = "text", x = 2001.25, y = 365, 
           label = "2001:\nGrizzly Bear\nHunting\nMoratorium", 
           size = 3, hjust = 0)
  
bears_plot2
```

## Install-and-play {#api-wrappers}

Many common web services and APIs have been "wrapped", i.e. R functions have been written around them which send your query to the server and format the response.

Why do we want this?

* provenance
* reproducible
* updating
* ease
* scaling


### Revisiting Grizzly Bear Mortality Data using `bcdata` {#grizzly-bear-2}

The `bcdata` package is a wrapper for the British Columbia Data Catalogue. We can use this package to get the same Grizzly Bear mortality data that we manually downloaded in earlier in Chapter \@ref(grizzly-bears-1).

First, we need to install the package using `remotes::install_github()`.

Install the `bcdata` package on GitHub
```{r message = FALSE}
library(remotes)

# remotes::install_github("bcgov/bcdata") 
library(bcdata)
```

We can use the `bcdc_search()` function to find all records that contain the words "grizzly" and "mortality".
```{r warning = FALSE}
bcdc_search("grizzly", "mortality")
```

The first record listed is the same csv file that we downloaded earlier. Let's try to read the file into our session without manually downloading it. We can do this using the `bcdc_get_data()` function. We need to pass it the ID of the record we're interested in, which we already know from our `bcdc_search()`.
```{r}
bears_2 <- bcdc_get_data("4bc13aa2-80c9-441b-8f46-0b9574109b93")
```

Let's see if `bears_2` is the same as our previous `bears`
```{r}
library(dplyr)

glimpse(bears_2)
```

It looks the same but is it *identical*?
```{r}
dplyr::all_equal(bears, bears_2)
```

Looks good!

### Exploring US census data using `tidycensus`

The [`tidycensus`][tidycensus-web] package (on [CRAN][tidycensus-cran], on [GitHub][tidycensus-github]) is a wrapper package for two US Census Bureau API's:

* [Decennial Census (1990, 2000, 2010) API](https://www.census.gov/data/developers/data-sets/decennial-census.html)
* [American Community Survey 5-Year Data (2009-2017) API](https://www.census.gov/data/developers/data-sets/acs-5year.html)

The authors have written a great vignette to help get you started with `tidycensus` [here](https://walkerke.github.io/tidycensus/articles/basic-usage.html).

First, let's install/load the package.
```{r message = FALSE}
#install.packages("tidycensus")
library(tidycensus)
```

To access the US Census Bureau API's first we need to request a new API key here: <https://api.census.gov/data/key_signup.html>. Check your email and follow the instructions to activate your key.

Now that you have a personal API key, the next step is to add it to your .Renviron file. The `tidycensus` package has a helpful function called `census_api_key()` that will take care of this step for you but we can also do it ourselves with the `usethis` package.

```r
library(usethis)

usethis::edit_r_environ() # edit your user level.Renviron file.
```

This will open up your .Renviron file in a new tab. Add `CENSUS_API_KEY=<your-secret-api-key>` on a new line. Be sure to add a new line after this! <!--insert photo of RStudio default setting here--> 

Save the file and restart your R session. You can now access your key with `Sys.getenv("CENSUS_API_KEY")`. If you are using GitHub be sure to add `.Renviron` to your `.gitignore` file so you don't accidentally push the key that you just took the time to hide.

Let's try pulling in some data from the [1990 Decennial Census](https://www.census.gov/data/developers/data-sets/decennial-census.1990.html). We can check out the different variables for one of the datasets through the US Census Bureau [website](https://api.census.gov/data/1990/sf3/variables.html) or with `tidycensus::load_variables()`.

```{r}
library(dplyr)

# what are the variables for the sf3 dataset for 1990?
variables_1990 <- load_variables(1990, "sf3")
glimpse(variables_1990)
```


Let's explore variables related to income. We can filter for labels that contain the string "income" by combining `stringr::str_detect()` with `dplyr::filter()`.
```{r}
library(stringr)

variables_1990 %>% 
  mutate(label = str_to_lower(label)) %>%  # eliminate case sensitivity probs
  filter(str_detect(label, "income"))
```

What was the median household income in 1990 by state? Let's use `tidycensus::get_decennial()` to pull down the relevant data.
```{r}
median_income <- get_decennial(geography = "state", variables = "P080A001", 
                               year = 1990) %>% 
  rename(state = NAME)

glimpse(median_income)
```

Hmm, there are 51 observations instead of 50. Maybe this is because they separated DC?
```{r}
median_income %>% 
  mutate(state = str_to_lower(state)) %>% # eliminate case sensitivity probs
  filter(state %in% c("dc", "d.c.", "district of columbia")) # check a few variations
```
It looks like they did!

Let's try plotting.
```{r fig.width = 9, fig.height = 9}
library(ggplot2)

ggplot(median_income, aes(state, value)) +
  geom_point(color = "cornflowerblue", alpha = 0.8) +
  labs(x = "State", y = "Median Household Income", 
       title = "1990 US Decennial Census Data",
       subtitle = "Median Household Income by State",
       caption = "Data Source: https://www.census.gov") +
  scale_y_continuous(breaks = c(20000, 25000, 30000, 35000, 40000, 45000),
                     labels = c("$20,000", "$25,000", "$30,000", "$35,000",
                                "$40,000", "$45,000")) +
  coord_flip() +
  theme_minimal()
```

Whoops. Let's use `forcats::fct_reorder` to reorder the factor levels of `state` to make this plot a little bit easier to read.
```{r}
library(forcats)

median_income_ordered <- median_income %>% 
  mutate(state = fct_reorder(state, value))
```

```{r fig.width = 9, fig.height = 9}
ggplot(median_income_ordered, aes(state, value)) +
  geom_point(color = "cornflowerblue", alpha = 0.8) +
  labs(x = "State", y = "Median Household Income", 
       title = "1990 US Decennial Census Data",
       subtitle = "Median Household Income by State",
       caption = "Data Source: United States Census Bureau") +
  scale_y_continuous(breaks = c(20000, 25000, 30000, 35000, 40000, 45000),
                     labels = c("$20,000", "$25,000", "$30,000", "$35,000",
                                "$40,000", "$45,000")) +
  coord_flip() +
  theme_minimal()
```

The states with the highest median household income in 1990 were Connecticut, Alaska, and New Jersey while those with the lowest were Arkansas, West Virginia, and Mississippi. Is it what you were expecting to see?


## Interact with an API

Earlier we experimented with several packages that "wrapped" APIs. That is, they handled the creation of the request and the formatting of the output. Now we're going to look at (part of) what these functions were doing.

First we're going to examine the structure of API requests via the [Open Movie Database](https://www.omdbapi.com/) (OMDb). OMDb is very similar to IMDB, except it has a nice, simple API. We can go to the website, input some search parameters, and obtain both the XML query and the response from it. 

### Exploring movies using the OMDb API

stat545 examples no longer work :( this section needs to be updated


```{r links, child="links.md"}
```



